{
    "paper_00026": {
        "abstract": " A common way to learn is by studying written step-by-step tutorials such as worked examples. However, tutorials for computer programming can be tedious to create since a static text-based format cannot convey what happens as code executes. We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization. Using Codepourri, we developed a novel crowdsourcing workflow where learners who are visiting an educational website (www.pythontutor.com) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations. Since there are far more learners than experts, using learners as a crowd is a potentially more scalable way of creating tutorials. Our experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd's annotations to be accurate, informative, and containing some insights that even experts missed.",
        "keywords": [
            "rogram visualization",
            " worked examples",
            " crowd-sourcing",
            " tutorial creation",
            " CS education"
        ],
        "id": "00026",
        "papertitle": "",
        "_id": "00026"
    },
    "paper_00438": {
        "abstract": " As the conventional power systems turn towards smart grids (SGs) on a fast pace, this transition may create new and significant challenges to the existing electrical network security. Along with many important features of the SGs cyber security has emerged to be a critical issue due to the interconnection of several loads, generators, and renewable resources through the communication network. Cyber-physical attacks (CPAs) are classified as the major threatening of SGs security because it may lead to severe consequences such as large blackout and destruction of infrastructures. Cyber switching attacks (CSAs) (as a part CPAs) start to attract the attention due to its severity and speed in destabilizing the SGs, we present in this paper Thyristor-Controlled Braking Resistor (TCBR) as a solution to mitigate this type of attack. TCBR can enable us to stabilize the target generator in a relatively short time.",
        "keywords": [
            "Cyber switching attack",
            " SG security and SG stability",
            " thyristor-controlled braking resistor"
        ],
        "id": "00438",
        "papertitle": "",
        "_id": "00438"
    },
    "paper_00519": {
        "abstract": " Background and objective: According to the World Health Organization (WHO) epilepsy affects approximately 45-50 million people. Electroencephalogram (EEG) records the neurological activity in the brain and it is used to identify epilepsy. Visual inspection of EEG signals is a time-consuming process and it may lead to human error. Feature extraction and classification are two main steps that are required to build an automated epilepsy detection framework. Feature extraction reduces the dimensions of the input signal by retaining informative features and the classifier assigns a proper class label to the extracted feature vector. Our aim is to present effective feature extraction techniques for automated epileptic EEG signal classification. Methods: In this study, two effective feature extraction techniques (Local. Neighbor Descriptive Pattern [LNDP] and One-dimensional Local Gradient Pattern [1D-LGP]) have been introduced to classify epileptic EEG signals. The classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers. The benchmark epilepsy EEG dataset provided by the University of Bonn is used in this research. The classification performance is evaluated using 10 -fold cross validation. The classifiers used are the Nearest Neighbor (NN), Support Vector Machine (SVM), Decision Tree (DT) and Artificial Neural Network (ANN). The experiments have been repeated for 50 times. Results: LNDP and 1D-LGP feature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82% and 99.80%, respectively, for the classification between normal and epileptic EEG signals. Eight different experimental cases were tested. The classification results were better than those of some existing methods. Conclusions: This study suggests that LNDP and 1D-LGP could be effective feature extraction techniques for the classification of epileptic EEG signals. (C) 2017 Elsevier Ltd. All rights reserved.",
        "keywords": [
            "Electroencephalogram (EEG) signals",
            " Local Neighbor Descriptive Pattern (LNDP)",
            " One-dimensional Local Gradient Pattern",
            " (1D-LGP)",
            " Feature extraction",
            " Classification"
        ],
        "id": "00519",
        "papertitle": "",
        "_id": "00519"
    },
    "paper_00836": {
        "abstract": " Computer networks consist of several assets such as hardware, software, and data sources. These assets have often some vulnerabilities which can be exploited by attackers that violate security policies in the network. Considering the limited budget, the network administrator should analyze and prioritize these vulnerabilities to be able to efficiently protect a network by mitigating the most risky ones. So far, several security parameters are offered to analyze security risks from the network security administrator's perspective. The major drawback of these methods is that they do not consider attacker's motivation. Depending on the motivation of potential attackers, different attack path may be selected for network security compromise. So, attacker's motivation is a key factor in predicting the attacker's behavior. In this paper, the attacker's motivation is considered in the process of security risk analysis, so network administrators are able to analyze security risks more accurately. The proposed method is applied on a network and the results are compared with novel works in this area. The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently.",
        "keywords": [
            "risk analysis",
            " network security",
            " vulnerability",
            " Bayesian Network",
            " benefit-cost analysis",
            " behavioral parameters"
        ],
        "id": "00836",
        "papertitle": "",
        "_id": "00836"
    },
    "paper_01046": {
        "abstract": " Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients. In this paper, the characteristics of medical data are studied; two mainstream technologies of data storage for medical information are compared, and three strategies of medical documents storage are described with detailed advantages and disadvantages. Semi-structured storage technology is easier to deploy and much more promising to promote in a wider range than all-structured methods. The combination of central and distributed data storage is more practical for regional data sharing. This analysis suggests that semi-structural data storage technology and the combination of central and distributed data storage are efficient and fit well the current situation in China.",
        "keywords": [
            "data storage",
            " data sharing",
            " data center",
            " medical information"
        ],
        "id": "01046",
        "papertitle": "",
        "_id": "01046"
    },
    "paper_01049": {
        "abstract": " Data Structures and Algorithms are a central part of Computer Science. Due to their abstract and dynamic nature, they are a difficult topic to learn for many students. To alleviate these learning difficulties, instructors have turned to algorithm visualizations (AV) and AV systems. Research has shown that especially engaging AVs can have an impact on student learning of DSA topics. Until recently, most AV systems were Java-based systems. But, the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online. In this paper, we present JSAV: the JavaScript AV development library. JSAV goes beyond traditional AV library support for displaying standard data structures components, to provide functionality to simplify creation of AVs on many engagement levels including interactive exercises. We describe the growing body of content created with JSAV and summarize our three years of experience and research results from using JSAV to build content that supports CS education.",
        "keywords": [
            "Data structure and algorithm visualizations",
            " algorithm animation",
            " interactive courseware",
            " HTML5",
            " active electronic textbooks",
            " hypertextbook",
            " JSAV"
        ],
        "id": "01049",
        "papertitle": "",
        "_id": "01049"
    },
    "paper_01370": {
        "abstract": " Extracting OWL ontologies from relational databases is extremely helpful for realising the Semantic Web vision. However, most of the approaches in this context often drop many of the expressive features of OWL. This is because highly expressive axioms can not be detected from database schema alone, but instead require a combined analysis of the database schema and data. In this paper, we present an approach that transforms a relational schema to a basic OWL schema, and then enhances it with rich OWL 2 constructs using schema and data analysis techniques. We then rely on the user for the verification of these features. Furthermore, we apply machine learning algorithms to help in ranking the resulting features based on user supplied relevance scores. Testing our tool on a number of databases demonstrates that our proposed approach is feasible and effective.",
        "keywords": [
            "Ontology Learning",
            " OWL 2 Ontologies",
            " Data Analysis",
            " Machine Learning"
        ],
        "id": "01370",
        "papertitle": "",
        "_id": "01370"
    },
    "paper_01466": {
        "abstract": " Formal language theory plays, in computer science, a fundamental role that allows, among other things, the development of one of the cornerstones of information technology: programming languages. They define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines. Despite its significance, formal language theory is often taken for granted, even by software developers, who regularly follow the rules of their programming domain. Unless the developer is creating its own programming language, data structure, or describing the formal background of an existing one, he will not need to dive deep into formal languages, grammar or automaton theory. Those who do need to develop their own rules will first have to understand the theory of formal languages, and the limitations they impose. This paper will do an introduction to the fundamentals of language theory, their classification, restrictions and representation. Once this ground rules are set, we will use a worldwide known data structure format such as the JavaScript Object Notation (JSON), to formally define its grammar rules and automaton. All of this formal background will allow us to transform theory into bits, by developing an algorithm that will analyze streams of texts, accepting or rejecting them as they comply or not with the predefined rules. Finally, we will analyze the outcomes of this implementation, its benefits, limitations, and alternatives that could have been followed.",
        "keywords": [
            "Grammar",
            " Formal",
            " Languages",
            " Pushdown",
            " Automata",
            " Context",
            " Data",
            " Structure",
            " JSON"
        ],
        "id": "01466",
        "papertitle": "",
        "_id": "01466"
    },
    "paper_01925": {
        "abstract": " In the last two decades, we have seen an amazing development of image processing techniques targeted for medical applications. We propose multi-GPU-based parallel real-time algorithms for segmentation and shape-based object detection, aiming at accelerating two medical image processing methods: automated blood detection in wireless capsule endoscopy (WCE) images and automated bright lesion detection in retinal fundus images. In the former method we identified segmentation and object detection as being responsible for consuming most of the global processing time. While in the latter, as segmentation was not used, shape-based object detection was the compute-intensive task identified. Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second. By applying the multi-GPU framework for bright lesion detection in fundus images we are able to process 62 frames per second with a speedup average 667 times faster than the equivalent CPU version.",
        "keywords": [
            "Segmentation",
            " Shape-based object detection",
            " Wireless capsule endoscopy",
            " Fundus images",
            " Automated diagnosis",
            " Parallel image processing",
            " Multi-GPU Systems"
        ],
        "id": "01925",
        "papertitle": "",
        "_id": "01925"
    },
    "paper_02026": {
        "abstract": " In this paper we focus on gender classification from face images. Despite advances in equipment as well as methods, automatic face image processing for recognition or even just for the extraction of demographics, is still a challenging task in unrestricted scenarios. Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones. When directly using features, we first apply different operators to extract the corresponding feature vectors, and then stack such vectors. These are classified by a SVM-based approach. When using scores, the different operators are applied in a completely separate way, so that each of them produces the corresponding scores. Answers are then either fed to a SVM, or compared pairwise to exploit Likelihood Ratio. The testbeds used for experiments are EGA database, which presents a good balance with respect to demographic features of stored face images, and GROPUS, an increasingly popular benchmark for massive experiments. The obtained performances confirm that feature level fusion achieves an often better classification accuracy. However, it is computationally expensive. We contribute to the research on this topic in three ways: 1) we show that the proposed score level fusion approaches, though less demanding, can achieve results that are comparable to feature level fusion, or even slightly better given that we fuse a particular set of experts; the main advantage over the feature-based approach relying on chained vectors, is that it is not required to evaluate a complex multi-feature distribution and the training process: thanks to the individual training of experts the overall process is more efficient and flexible, since experts can be easily added or discarded from the final architecture; 2) we evaluate the number of uncertain/ambiguous cases, i.e., those that might cause classification errors depending on the classification thresholds used, and show that with our score level fusion these significantly decreases; despite the final rate of correct classifications, this results in a more robust system; 3) we achieve very good results with operators that are not computationally expensive. (C) 2016 Elsevier Inc. All rights reserved.",
        "keywords": [
            "Automatic gender classification",
            " Face images",
            " Multi-feature classification",
            " Feature level vs. score level fusion"
        ],
        "id": "02026",
        "papertitle": "",
        "_id": "02026"
    },
    "paper_02316": {
        "abstract": " Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs. However, they rarely provide a semantic model to describe their contents. Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data. Such models are the key ingredients to automatically publish the data into knowledge graphs. Manually modeling the semantics of data sources requires significant effort and expertise, and although desirable, building these models automatically is a challenging problem. Most of the related work focuses on semantic annotation of the data fields (source attributes). However, constructing a semantic model that explicitly describes the relationships between the attributes in addition to their semantic types is critical. We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source. This model represents the semantics of the new source in terms of the concepts and relationships defined by the domain ontology. Given some sample data from the new source, we leverage the knowledge in the domain ontology and the known semantic models to construct a weighted graph that represents the space of plausible semantic models for the new source. Then, we compute the top k candidate semantic models and suggest to the user a ranked list of the semantic models for the new source. The approach takes into account user corrections to learn more accurate semantic models on future data sources. Our evaluation shows that our method generates expressive semantic models for data sources and services with minimal user input. These precise models make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition. They also make it possible to automatically publish semantic data into knowledge graphs. (C) 2015 Elsevier B.V. All rights reserved.",
        "keywords": [
            "Knowledge graph",
            " Semantic model",
            " Semantic labeling",
            " Semantic web",
            " Ontology",
            " Linked data"
        ],
        "id": "02316",
        "papertitle": "",
        "_id": "02316"
    },
    "paper_02317": {
        "abstract": " Information technology has been contributing to various areas of knowledge; in particular, the field of education stands out. In what concerns the teaching of computer programming, literature contains important efforts that aim to assist in the learning process. Teaching algorithms and programming concepts for first year students has always been a great challenge for universities, new Computer Science students usually have difficulties in understanding and abstracting the problem logics. An alternative that has contributed to the teaching-learning process is the use of Learning Objects (LO), which contribute towards mediating and enhancing the teaching-learning process. One of the great difficulties of learning during the initial semesters of Engineering and Computer Science courses is related to the contents of computer programming, which increases the students' failure level and also the dropout rate of such courses. In order to decrease those rates, we have developed a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses, and the results were very positive. This paper presents the qualitative and quantitative results of the experiment we conducted with the development and application of learning objects to help teaching students of Computer Science. The project was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process.",
        "keywords": [
            "computer programming",
            " computer science education",
            " educational process",
            " software algorithms",
            " learning objects"
        ],
        "id": "02317",
        "papertitle": "",
        "_id": "02317"
    }
}