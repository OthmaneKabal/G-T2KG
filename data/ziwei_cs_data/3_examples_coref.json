{
  "paper_00836": {
    "abstract": " Computer networks consist of several assets such as hardware, software, and data sources. several assets such as hardware, software, and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks . Considering the limited budget, the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones. So far, several security parameters are offered to analyze security risks from the network security administrator's perspective. The major drawback of these methods is that these methods do not consider attacker's motivation. Depending on the motivation of potential attackers, different attack path may be selected for network security compromise. So, attacker's motivation is a key factor in predicting the attacker's behavior. In this paper, the attacker's motivation is considered in the process of security risk analysis, so network administrators are able to analyze security risks more accurately. The proposed method is applied on a network and the results are compared with novel works in this area. The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently.",
    "keywords": [
      "risk analysis",
      " network security",
      " vulnerability",
      " Bayesian Network",
      " benefit-cost analysis",
      " behavioral parameters"
    ],
    "id": "00836",
    "papertitle": "",
    "_id": "00836"
  },
  "paper_00519": {
    "abstract": " Background and objective: According to the World Health Organization (WHO) epilepsy affects approximately 45-50 million people. Electroencephalogram (EEG) records the neurological activity in the brain and Electroencephalogram (EEG) is used to identify epilepsy. Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error. Feature extraction and classification are two main steps that are required to build an automated epilepsy detection framework. Feature extraction reduces the dimensions of the input signal by retaining informative features and the classifier assigns a proper class label to the extracted feature vector. Our aim is to present effective feature extraction techniques for automated epileptic EEG signal classification. Methods: In this study, two effective feature extraction techniques (Local. Neighbor Descriptive Pattern [LNDP] and One-dimensional Local Gradient Pattern [1D-LGP]) have been introduced to classify epileptic EEG signals. The classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers. The benchmark epilepsy EEG dataset provided by the University of Bonn is used in this study . The classification performance is evaluated using 10 -fold cross validation. The classifiers used are the Nearest Neighbor (NN), Support Vector Machine (SVM), Decision Tree (DT) and Artificial Neural Network (ANN). The experiments have been repeated for 50 times. Results: Neighbor Descriptive Pattern [LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82% and 99.80%, respectively, for the classification between normal and epileptic EEG signals. Eight different experimental cases were tested. The classification results were better than those of some existing methods. Conclusions: this study suggests that Neighbor Descriptive Pattern [LNDP and 1D-LGPcould be effective feature extraction techniques for the classification of epileptic EEG signals. (C) 2017 Elsevier Ltd. All rights reserved.",
    "keywords": [
      "Electroencephalogram (EEG) signals",
      " Local Neighbor Descriptive Pattern (LNDP)",
      " One-dimensional Local Gradient Pattern",
      " (1D-LGP)",
      " Feature extraction",
      " Classification"
    ],
    "id": "00519",
    "papertitle": "",
    "_id": "00519"
  },
  "paper_01466": {
    "abstract": " Formal language theory plays, in computer science, a fundamental role that allows, among other things, the development of one of the cornerstones of information technology: programming languages.  Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines. Despite  Formal language theory significance, formal language theory is often taken for granted, even by software developers, who regularly follow the rules of their programming domain. Unless the developer is creating the developer own programming language, data structure, or describing the formal background of an existing one, the developer will not need to dive deep into formal languages, grammar or automaton theory. Those who do need to develop their own rules will first have to understand the theory of formal languages, and the limitations formal languages impose. This paper will do an introduction to the fundamentals of language theory, the fundamentals of language theory classification, restrictions and representation. Once the fundamentals of language theory are set, we will use a worldwide known data structure format such as the JavaScript Object Notation (JSON), to formally define its grammar rules and automaton. All of this formal background will allow us to transform theory into bits, by developing an algorithm that will analyze streams of texts, accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules. Finally, we will analyze the outcomes of this implementation, this implementation benefits, limitations, and alternatives that could have been followed.",
    "keywords": [
      "Grammar",
      " Formal",
      " Languages",
      " Pushdown",
      " Automata",
      " Context",
      " Data",
      " Structure",
      " JSON"
    ],
    "id": "01466",
    "papertitle": "",
    "_id": "01466"
  }
}