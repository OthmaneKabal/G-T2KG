{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6155690c-31b7-43a1-a238-10b4052f31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, input_path, output_path , extended=False):\n",
    "\n",
    "        self.results = []\n",
    "        self.sentences = self.get_sentences(input_path)\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.__adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure',\n",
    "            'a', 'the', \"treeb\", \"rbek\", \"en\", \"that\", \"those\", \"some\"\n",
    "        ]\n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        # the first Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "                'last'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?for example (, )?'\n",
    "                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n",
    "                    '(\\\\. )?\\\\))',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "                    'for instance)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "                    '?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "        self.__spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def get_sentences(self, file_name):\n",
    "        \n",
    "        try:\n",
    "            with open(file_name, 'r', encoding= 'utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                lines = [l.strip() for l in lines]\n",
    "                return lines\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Le fichier '{file_name}' n'a pas été trouvé.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "    \n",
    "    def chunk(self, rawtext):\n",
    "        doc = self.__spacy_nlp(rawtext)\n",
    "        chunks = []\n",
    "        for sentence in doc.sents:\n",
    "            sentence_text = sentence.lemma_\n",
    "            for chunk in sentence.noun_chunks:\n",
    "                if chunk.lemma_.lower() == \"example\":\n",
    "                    start = chunk.start\n",
    "                    pre_token = sentence[start - 1].lemma_.lower()\n",
    "                    post_token = sentence[start + 1].lemma_.lower()\n",
    "                    if start > 0 and\\\n",
    "                            (pre_token == \"for\" or post_token == \"of\"):\n",
    "                        continue\n",
    "                if chunk.lemma_.lower() == \"type\":\n",
    "                    continue\n",
    "                chunk_arr = []\n",
    "                replace_arr = []\n",
    "                # print(\"chunk:\", chunk)\n",
    "                for token in chunk:\n",
    "                    if token.lemma_ in self.__adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                        continue\n",
    "                    chunk_arr.append(token.lemma_)\n",
    "                    # Remove punctuation and stopword adjectives\n",
    "                    # (generally quantifiers of plurals)\n",
    "                    if token.lemma_.isalnum():\n",
    "                        replace_arr.append(token.lemma_)\n",
    "                    else:\n",
    "                        replace_arr.append(''.join(\n",
    "                            char for char in token.lemma_ if char.isalnum()\n",
    "                        ))\n",
    "                if len(chunk_arr) == 0:\n",
    "                    chunk_arr.append(chunk[-1].lemma_)\n",
    "                chunk_lemma = ' '.join(chunk_arr)\n",
    "                # print(chunk_lemma)\n",
    "                replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "                if chunk_lemma:\n",
    "                    sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                           r'%s' % replacement_value,\n",
    "                                           sentence_text)\n",
    "            chunks.append(sentence_text)\n",
    "        return chunks\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list\n",
    "        of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged\n",
    "            # into a single NP, it's a chunk error\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "\n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        pair = (\n",
    "                            self.clean_hyponym_term(specifics[i]),\n",
    "                            self.clean_hyponym_term(general)\n",
    "                        )\n",
    "                        # reduce duplicates\n",
    "                        if pair not in hyponyms:\n",
    "                            hyponyms.append(pair)\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    def get_triples(self):\n",
    "        for sentence in self.sentences:\n",
    "            hyponyms = self.find_hyponyms(sentence)\n",
    "            if hyponyms:\n",
    "                for hyp in hyponyms:\n",
    "                    self.results.append(\n",
    "                        {\n",
    "                                'sentence': sentence,\n",
    "                                'subject': hyp[0],\n",
    "                                'predicate': \"is-a\",\n",
    "                                'object': hyp[1],\n",
    "                                'confidence': \"-\"\n",
    "                                \n",
    "                            }\n",
    "                \n",
    "            )\n",
    "    \n",
    "\n",
    "    \n",
    "    def write_to_json(self):\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(self.results, jsonfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c677d68-e427-4831-a998-ecc73547af36",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/admin-user/Desktop/my_phd/implementations_KG/data/Music_Bench/Bench_music_oie.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/admin-user/Desktop/my_phd/implementations_KG/src/informations_extraction/hyponyms_outputs/hyp_Music-Bench.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m HP \u001b[38;5;241m=\u001b[39m HearstPatterns(input_path, output_path)\n\u001b[0;32m     10\u001b[0m HP\u001b[38;5;241m.\u001b[39mget_triples()\n\u001b[0;32m     11\u001b[0m HP\u001b[38;5;241m.\u001b[39mwrite_to_json()\n",
      "Cell \u001b[1;32mIn[5], line 240\u001b[0m, in \u001b[0;36mHearstPatterns.__init__\u001b[1;34m(self, input_path, output_path, extended)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extended:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__hearst_patterns\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     56\u001b[0m         (\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m((NP_\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mw+ ?(, )?)+(and |or )?any other NP_\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mw+)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m         ),\n\u001b[0;32m    238\u001b[0m     ])\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__spacy_nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# ## Computer_Science\n",
    "# input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/ziwei_cs_data/CS_bench_oie.txt\"\n",
    "# output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/informations_extraction/hyponyms_outputs/hyp_Cs-Bench.json\"\n",
    "\n",
    "### Music\n",
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/Music_Bench/Bench_music_oie.txt\"\n",
    "output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/informations_extraction/hyponyms_outputs/hyp_Music-Bench.json\"\n",
    "\n",
    "HP = HearstPatterns(input_path, output_path)\n",
    "HP.get_triples()\n",
    "HP.write_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd97cd0-3aac-4bf8-9678-199f295dd6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
