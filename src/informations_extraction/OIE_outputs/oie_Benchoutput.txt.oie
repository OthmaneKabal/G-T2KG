A common way to learn is by studying written step-by-step tutorials such as worked examples .
1.00: (A common way to learn; is; by studying written step-by-step tutorials such as worked examples)

However , tutorials for computer programming can be tedious to create since a static text-based format can not convey what happens as code executes .
0.99: (tutorials for computer programming; can be; tedious to create)
0.91: (a static text-based format; can not convey; what happens as code executes)
0.98: (code; executes; )

We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization .
0.99: (We; created; a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization)
0.99: (a system called Codepourri; enables; people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization)
0.03: (people; to easily create; visual coding tutorials by annotating steps in an automatically-generated program visualization)
0.84: (people; visual coding tutorials; )

Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .
0.76: (a system called Codepourri; enables; in an automatically-generated program)
0.22: (people learners who are visiting an educational website ( www.pythontutor.com; collectively create; to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization a tutorial annotating execution)
0.56: (people; to easily create; visual coding tutorials)
0.76: (We; developed; a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code)
0.03: (learners who are visiting an educational website ( www.pythontutor.com; collectively create a tutorial; )
0.92: (a system called Codepourri; enables; in)
0.40: (We; developed; a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by then voting on the best annotations)
0.01: (where learners who are visiting an educational website ( www.pythontutor.com ); collectively create a tutorial by then voting; on the best annotations)

Since there are far more learners than experts , using learners as a crowd is a potentially more scalable way of creating tutorials .
0.99: (using learners as a crowd; is; a potentially more scalable way of creating tutorials)

We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .
0.96: (4 expert judges; adding; 145 raw annotations to two pieces of textbook Python code)
0.04: (We; experiments; with 4 expert judges the learner crowd 's annotations to be accurate)
0.48: (Python show the learner crowd 's annotations; to be; accurate)
0.89: (the learner crowd 's annotations; show to be; informative)
0.25: (We code the learner crowd 's annotations; show; 4 expert judges to be informative)
0.89: (4 expert judges; adding; 145 raw annotations to two pieces of textbook code)
0.93: (some insights; missed; that even experts)
0.09: (We the learner crowd 's annotations; experiments with show to be containing; 4 expert judges some insights that even experts missed)
0.44: (the learner crowd 's annotations; to be containing; some insights that even experts missed)
0.99: (101 learners; adding; 145 raw annotations to two pieces of textbook code)
0.85: (Python; show; the learner crowd 's annotations to be accurate)
0.03: (We; experiments; with 101 learners the learner crowd 's annotations to be accurate)
0.83: (Python the learner crowd 's annotations; show to be; informative)
0.34: (We Python show the learner crowd 's annotations; experiments to be; 101 learners informative)
0.06: (We experiments with 101 learners Python code; show; the learner crowd 's annotations to be containing some insights)
0.81: (We; experiments; with 101 learners)
0.84: (the learner crowd 's annotations; show to be containing; some insights that even experts missed)

As the conventional power systems turn towards smart grids ( SGs ) on a fast pace , turn may create new and significant challenges to the existing electrical network security .
0.99: (the conventional power systems; turn; towards smart grids ( SGs)
0.00: (turn; may create; new challenges to the existing electrical network security)
0.01: (turn; may create; significant challenges to the existing electrical network security)

Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .
1.00: (SGs cyber security; has emerged; Along with many important features of smart grids)
0.98: (SGs cyber security; to be; a critical issue due to the interconnection of several loads through the communication network)
0.97: (SGs cyber security; to be; a critical issue due to the interconnection of several generators through the communication network)
0.98: (SGs cyber security; to be; a critical issue due to the interconnection of several renewable resources through the communication network)

Cyber-physical attacks ( CPAs ) are classified as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as large blackout and destruction of infrastructures .
1.00: (Cyber-physical attacks; are classified; as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as large blackout)
1.00: (Cyber-physical attacks; may lead; to severe consequences such as large blackout)
1.00: (Cyber-physical attacks; are classified; as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as destruction of infrastructures)
1.00: (Cyber-physical attacks; may lead; to severe consequences such as destruction of infrastructures)

Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .
0.66: (Cyber switching attacks ( CSAs ) ( as a part CPAs; start; the attention due to Cyber switching attacks)
0.81: (Cyber switching attacks ( CSAs ) ( as a part CPAs; to attract; the attention due to Cyber switching attacks)
0.93: (we; present; in this paper Thyristor-Controlled Braking Resistor)
0.26: (this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution; to mitigate; this type of attack)
0.72: (Cyber switching attacks ( CSAs ) ( as a part CPAs; start to attract; the attention due to Cyber switching attacks)
0.82: (we a; present; in this paper Thyristor-Controlled Braking Resistor)
0.07: (Cyber switching attacks ( CSAs ) ( as a part CPAs a part CPAs as solution; start mitigate; this type of attack)

Thyristor-Controlled Braking Resistor ( TCBR ) can enable us to stabilize the target generator in a relatively short time .
1.00: (Thyristor-Controlled Braking Resistor; can enable; us to stabilize the target generator in a relatively short time)
1.00: (us; to stabilize; the target generator in a relatively short time)

Background and objective : According to the World Health Organization ( WHO ) epilepsy affects approximately 45-50 million people .
1.00: (epilepsy; affects; approximately 45-50 million people)
0.98: (epilepsy; affects; approximately 45-50 million people World)

Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .
1.00: (Electroencephalogram; records; the neurological activity in the brain)
1.00: (Electroencephalogram; is used; to identify epilepsy)
1.00: (Electroencephalogram; to identify; epilepsy)

Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error .
1.00: (Visual inspection of EEG signals; is; a time-consuming process)
1.00: (Visual inspection of EEG signals; may lead; to human error)

Feature extraction and classification are two main steps that are required to build an automated epilepsy detection framework .
1.00: (Feature extraction; are; two main steps that are required to build an automated epilepsy detection framework)
1.00: (two main steps; are required; to build an automated epilepsy detection framework)
1.00: (two main steps; to build; an automated epilepsy detection framework)
0.00: (classification; are; two main steps that are required to build an automated epilepsy detection framework)

Feature extraction reduces the dimensions of the input signal by retaining informative features and the classifier assigns a proper class label to the extracted feature vector .
1.00: (Feature extraction; reduces; the dimensions of the input signal by retaining informative features)
1.00: (the classifier; assigns; a proper class label to the extracted feature vector)

Our aim is to present effective feature extraction techniques for automated epileptic EEG signal classification .
1.00: (Our aim; is; to present effective feature extraction techniques for automated epileptic EEG signal classification)

Methods : In this study , two effective feature extraction techniques ( Local .

Neighbor Descriptive Pattern [ LNDP ] and One-dimensional Local Gradient Pattern [ 1D-LGP ] ) have been introduced to classify epileptic EEG signals .
1.00: (Neighbor Descriptive Pattern; have been introduced; to classify epileptic EEG signals)
0.98: (Neighbor Descriptive Pattern; to classify; epileptic EEG signals)
1.00: (One-dimensional Local Gradient Pattern; have been introduced; to classify epileptic EEG signals)
0.91: (One-dimensional Local Gradient Pattern; to classify; epileptic EEG signals)

The classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers .
0.90: (The classification between epileptic seizure and non -seizure signals; is performed; using different machine learning)

The benchmark epilepsy EEG dataset provided by the University of Bonn is used in this study .
0.99: (The benchmark epilepsy EEG dataset; provided; by the University of Bonn)
1.00: (The benchmark epilepsy EEG dataset; is used; in this study)

The classification performance is evaluated using 10 -fold cross validation .
1.00: (The classification performance; is evaluated; )
1.00: (The classification performance; using; 10 -fold cross validation)

The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .
1.00: (The classifiers; used; )
0.97: (The classifiers used ( NN ); are; the Nearest Neighbor)
0.96: (The classifiers used ); are; Support Vector Machine)
1.00: (The classifiers used; are; Decision Tree)
0.94: (The classifiers used ); are; Artificial Neural Network)

The experiments have been repeated for 50 times .
1.00: (The experiments; have been repeated; for 50 times)

Results : Neighbor Descriptive Pattern [ LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82 % and 99.80 % , respectively , for the classification between normal and epileptic EEG signals .
0.60: (Neighbor Descriptive Pattern [ LNDP extraction techniques with ANN classifier; achieved respectively; the average classification accuracy of 99.82 % and 99.80 % , , for the classification between normal and epileptic EEG signals)
0.61: (with ANN classifier; achieved respectively; the average classification accuracy of 99.82 % and 99.80 % , , for classification)

Eight different experimental cases were tested .
1.00: (Eight different experimental cases; were tested; )

The classification results were better than those of some existing methods .
1.00: (The classification results; were; better than those of some existing methods)

Conclusions : this study suggests that Neighbor Descriptive Pattern [ LNDP and 1D-LGPcould be effective feature extraction techniques for the classification of epileptic EEG signals .
1.00: (this study; suggests; that Neighbor Descriptive Pattern [ LNDP be effective feature extraction techniques for the classification of epileptic EEG signals)
0.90: (Neighbor Descriptive Pattern LNDP; be; effective feature extraction techniques for the classification of epileptic EEG signals)
1.00: (this study; suggests; that Neighbor Descriptive Pattern [ 1D-LGPcould be effective feature extraction techniques for the classification of epileptic EEG signals)
0.98: (Neighbor Descriptive Pattern 1D-LGPcould; be; effective feature extraction techniques for the classification of epileptic EEG signals)

Computer networks consist of several assets such as hardware , software , and data sources .
1.00: (Computer networks; consist; of several assets such as hardware)
1.00: (Computer networks; consist; of several assets such as software)
1.00: (Computer networks; consist; of several assets such as data sources)

several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .
1.00: (several assets such as hardware; have; often some vulnerabilities which can be exploited by attackers)
0.97: (some vulnerabilities; can be exploited; by attackers often)
1.00: (attackers; violate; security policies in Computer networks)
1.00: (several assets such as software; have; often some vulnerabilities which can be exploited by attackers)
1.00: (several assets such as data sources; have; often some vulnerabilities which can be exploited by attackers)

Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .
1.00: (the network administrator; should analyze; some vulnerabilities which can be exploited by attackers)
1.00: (some vulnerabilities; can be exploited; by attackers)
0.70: (attackers; violate; security policies in the network network)
0.16: (attackers in the network; security policies to be; able to efficiently protect a network by mitigating the most risky ones)
0.82: (attackers; to efficiently protect; a network)
1.00: (the network administrator; should prioritize; some vulnerabilities which can be exploited by attackers)
0.96: (attackers; violate; security policies in the network)
0.80: (attackers; security policies to be; able to efficiently protect a network by mitigating the most risky ones)

So far , several security parameters are offered to analyze security risks from the network security administrator 's perspective .
0.96: (several security parameters; are offered; to analyze security risks from the network security administrator 's perspective So far)
1.00: (several security parameters; to analyze; security risks)

The major drawback of these methods is that these methods do not consider attacker 's motivation .
0.00: (The major drawback of these methods; is; that these methods do not consider attacker 's motivation)
1.00: (these methods; do not consider; attacker 's motivation)

Depending on the motivation of potential attackers , different attack path may be selected for network security compromise .
1.00: (different attack path; may be selected; for network security compromise)

So , attacker 's motivation is a key factor in predicting the attacker 's behavior .
1.00: (attacker 's motivation; is; a key factor in predicting the attacker 's behavior)

In this paper , the attacker 's motivation is considered in the process of security risk analysis , so network administrators are able to analyze security risks more accurately .
0.50: (the attacker 's motivation network administrators; is more; in the process of security risk analysis able to analyze security risks accurately)
0.40: (so network administrators; are to analyze more accurately; able security risks)
0.39: (the attacker 's motivation; is considered; In this paper in the process of security risk analysis)

The proposed method is applied on a network and the results are compared with novel works in this area .
1.00: (The proposed method; is applied; on a network)

The proposed method is applied on a network and the results are compared with novel works in this area .
1.00: (the results; are compared; with novel works in this area)

The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .
1.00: (network administrator; to precisely predict; the behavior of attackers)
1.00: (The experimental results; show; that network administrator will be able to precisely predict the behavior of attackers)
1.00: (network administrator; will be; able to precisely predict the behavior of attackers)
0.99: (network administrator; to apply more efficiently; countermeasures)
1.00: (The experimental results; show; that network administrator will be able to apply countermeasures more efficiently)
0.99: (network administrator; will be; able to apply countermeasures more efficiently)

Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .
1.00: (Data sharing among medical institutions; is; a requirement for convenient and effective data availability for both healthcare professionals)
1.00: (Data sharing among medical institutions; is; a requirement for convenient and effective data availability for both patients)
1.00: (information exchange among medical institutions; is; a requirement for convenient and effective data availability for both healthcare professionals)
1.00: (information exchange among medical institutions; is; a requirement for convenient and effective data availability for both patients)

In this paper , the characteristics of medical data are studied ; two mainstream technologies of data storage for medical information are compared , and three strategies of medical documents storage are described with detailed advantages and disadvantages .
1.00: (the characteristics of medical data; are studied; In this paper)
1.00: (two mainstream technologies of data storage for medical information; are compared; In this paper)
0.97: (three strategies of medical documents storage; are described; In this paper with)
1.00: (three strategies of medical documents storage; are described; with detailed disadvantages In this paper)

Semi-structured storage technology is easier to deploy and much more promising to promote in a wider range than all-structured methods .
1.00: (Semi-structured storage technology; is; easier to deploy than all-structured methods)
1.00: (Semi-structured storage technology; is; much more promising to promote in a wider range than all-structured methods)
1.00: (Semi-structured storage technology; to promote; in a wider range than all-structured methods)

The combination of central and distributed data storage is more practical for regional data sharing .
1.00: (The combination of central data storage; is; more practical for regional data sharing)
1.00: (The combination of distributed data storage; is; more practical for regional data sharing)

This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .
1.00: (This analysis; suggests; that semi-structural data storage technology are efficient)
1.00: (semi-structural data storage technology; are; efficient)
1.00: (This analysis; suggests; that semi-structural data storage technology fit well the current situation in China)
0.99: (semi-structural data storage technology; fit well; the current situation in China)
1.00: (This analysis; suggests; that The combination of central data storage are efficient)
1.00: (The combination of central data storage; are; efficient)
1.00: (This analysis; suggests; that The combination of distributed data storage are efficient)
1.00: (The combination of distributed data storage; are; efficient)
1.00: (This analysis; suggests; that The combination of central data storage fit well the current situation in China)
1.00: (This analysis; suggests; that The combination of distributed data storage fit well the current situation in China)

Data Structures and Algorithms are a central part of Computer Science .
1.00: (Data Structures; are; a central part of Computer Science)
1.00: (Algorithms; are; a central part of Computer Science)

Due to Data Structures and Algorithms abstract and dynamic nature , Data Structures and Algorithms are a difficult topic to learn for many students .
1.00: (Data Structures; are; a difficult topic to learn for many students)
1.00: (Algorithms; are; a difficult topic to learn for many students)

To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .
1.00: (instructors; have turned; to algorithm visualizations)

To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .
1.00: (instructors; have turned; to AV systems)

Research has shown that especially engaging AVs can have an impact on student learning of DSA topics .
1.00: (Research; has shown; that especially engaging AVs can have an impact on student learning of DSA topics)
1.00: (especially engaging AVs; can have; an impact on student learning of DSA topics)

Until recently , most AV systems were Java-based systems .
1.00: (most AV systems; were; Java-based systems Until recently)

But , the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online .
1.00: (the popularity of Java; has declined; )
1.00: (the popularity of Java; is being supplanted; by HTML5 content online)
1.00: (the popularity of Java; is being supplanted; by JavaScript content online)

In this paper , we present JSAV : the JavaScript AV development library .
1.00: (we; present; JSAV In this paper)

JSAV : the JavaScript AV development library goes beyond traditional AV library support for displaying standard data structures components , to provide functionality to simplify creation of AVs on many engagement levels including interactive exercises .
0.99: (the JavaScript AV development library; goes; beyond traditional AV library support for displaying standard data structures components)
0.37: (JSAV; to provide; functionality creation of AVs including)

we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .
1.00: (we; describe; the growing body of content created with JSAV : the JavaScript AV development library)
1.00: (the growing body of content; created; with JSAV)
0.82: (the JavaScript AV development library; to build; content)
0.59: (we content; summarize; we three years of experience from using JSAV that supports CS education)
0.35: (we content; supports; we three years of experience from using JSAV : the JavaScript AV development library to build CS education)
0.77: (the JavaScript AV development library; to build; : content)
0.72: (we content; summarize; we three years of research results from using JSAV supports CS education)
0.40: (JavaScript content; supports; three years of research results from using JSAV CS education)

Extracting OWL ontologies from relational databases is extremely helpful for realising the Semantic Web vision .
1.00: (Extracting OWL ontologies from relational databases; is; extremely helpful for realising the Semantic Web vision)

However , most of the approaches in this context often drop many of the expressive features of OWL .
1.00: (most of the approaches in this context; drop; many of the expressive features of OWL often)

drop is because highly expressive axioms can not be detected from database schema alone , but instead require a combined analysis of the database schema and data .
0.00: (drop; is; because highly expressive axioms can not be detected from database schema alone)
1.00: (highly expressive axioms; can not be detected; from database schema alone)
1.00: (drop; is; because highly expressive axioms require a combined analysis of the database schema)
1.00: (highly expressive axioms; require; a combined analysis of the database schema)
1.00: (drop; is; because highly expressive axioms require a combined analysis of data)
1.00: (highly expressive axioms; require; a combined analysis of data)

In this paper , we present an approach that transforms a relational schema to a basic OWL schema , and then enhances a basic OWL schema with rich OWL 2 constructs using schema and data analysis techniques .
1.00: (we; present; an approach that transforms a relational schema to a basic OWL schema In this paper)
1.00: (an approach; transforms; a relational schema to a basic OWL schema)
1.00: (we; present; an approach that enhances a basic OWL schema with rich OWL 2 constructs In this paper)
1.00: (an approach; enhances; a basic OWL schema with rich OWL 2 constructs)
1.00: (rich OWL 2 constructs; using; schema techniques)
1.00: (rich OWL 2 constructs; using; data analysis techniques)

we then rely on the user for the verification of these features .
1.00: (we; rely; on the user for the verification of these features then)

Furthermore , we apply machine learning algorithms to help in ranking the resulting features based on user supplied relevance scores .
0.80: (the resulting features; based; on user supplied relevance scores)
0.02: (we; apply supplied; machine learning algorithms to help in ranking the resulting features relevance scores)
0.63: (we; apply machine learning algorithms to help; in ranking the resulting features based on user)
0.64: (we; in ranking; based on supplied relevance scores)

Testing we tool on a number of databases demonstrates that an approach that transforms a relational schema to a basic OWL schema , and then enhances it with rich OWL 2 constructs using schema and data analysis techniques is feasible and effective .
1.00: (we; tool; on a number of databases)
0.90: (an approach; transforms; that a relational schema to a basic OWL schema)
0.99: (Testing we tool on a number of databases; demonstrates; that an approach that transforms a relational schema to a basic OWL schema is feasible)
1.00: (an approach that transforms a relational schema to a basic OWL schema; is; feasible)
0.99: (Testing we tool on a number of databases; demonstrates; that an approach that transforms a relational schema to a basic OWL schema is effective)
1.00: (an approach that transforms a relational schema to a basic OWL schema; is; effective)
0.85: (an approach rich OWL 2 constructs; enhances; it with schema techniques)
0.64: (rich OWL 2 constructs; using; that enhances it with schema techniques)
0.94: (Testing we tool on a number of databases; demonstrates; that an approach that enhances it with rich OWL 2 constructs using schema techniques is feasible)
0.79: (an approach that enhances it with rich OWL 2 constructs techniques; is; feasible)
0.79: (an approach rich OWL constructs; enhances; it with 2 data analysis techniques)
0.74: (rich OWL 2 constructs; using; that it data analysis techniques)
0.91: (Testing we tool on a number of databases; demonstrates; that an approach that enhances it with rich OWL 2 constructs using data analysis techniques is feasible)
0.74: (an approach that enhances it with rich OWL 2 constructs data analysis techniques; is; feasible)
0.91: (Testing we tool on a number of databases; demonstrates; that an approach that enhances it with rich OWL 2 constructs using schema techniques is effective)
0.77: (an approach that enhances it with rich OWL 2 constructs techniques; is; effective)
0.75: (an approach that rich OWL constructs; enhances; it with 2 data analysis techniques)
0.77: (it rich OWL 2 constructs; using; data analysis techniques)
0.92: (Testing we tool on a number of databases; demonstrates; that an approach that enhances it with rich OWL 2 constructs using data analysis techniques is effective)
0.71: (an approach that enhances it with rich OWL 2 constructs data analysis techniques; is; effective)

Formal language theory plays , in computer science , a fundamental role that allows , among other things , the development of one of the cornerstones of information technology : programming languages .
0.98: (Formal language theory; plays; in computer science)
0.89: (a fundamental role; allows; among other things)

Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines .
0.99: (Formal language theory; define; the mandatory grammatical rules that programmers need to follow to create the tools)
0.91: (the tools; enable; humans to interact with machines)
0.61: (the grammatical programmers humans; need to interact; to follow to create with machines)
0.83: (the mandatory grammatical rules programmers; need follow; to to create the tools)
0.84: (the mandatory grammatical rules programmers; need to follow to create; the tools that enable humans to interact with machines)

Despite Formal language theory significance , formal language theory is often taken for granted , even by software developers , who regularly follow the rules of their programming domain .
1.00: (formal language theory; is taken; for granted often)
1.00: (software developers; regularly follow; the rules of their programming domain)

Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .
0.99: (the developer; is creating; the developer own programming language , data structure)
1.00: (the developer; will not need; to dive deep into formal languages)
0.91: (the developer; will need to dive deep; into formal languages)
1.00: (the developer; will not need; to dive deep into grammar)
0.94: (the developer; need to dive deep; into grammar)

Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .
0.99: (the developer; is creating; the developer own programming language , data structure)
1.00: (the developer; will not need; to dive deep into automaton theory)
0.30: (the developer; will not need to dive; deep into automaton theory)
1.00: (the developer; is describing; the formal background of an existing one)
1.00: (the developer; will not need; to dive deep into formal languages)
0.41: (the developer; will not need to dive; deep into formal languages)
1.00: (the developer; will not need; to dive deep into grammar)
0.43: (the developer; will not need to dive; deep into grammar)

Those who do need to develop their own rules will first have to understand the theory of formal languages , and the limitations formal languages impose .
0.95: (Those who do need to develop their own rules; to understand; the theory of formal languages)
1.00: (Those; do need; to develop their own rules)
0.99: (Those; do need to develop; their own rules)
0.93: (Those who to develop their own rules; to understand; the limitations formal languages impose)
0.87: (Those the limitations languages; impose; formal)

This paper will do an introduction to the fundamentals of language theory , the fundamentals of language theory classification , restrictions and representation .
1.00: (This paper; will do; an introduction to the fundamentals of language theory , the fundamentals of language theory classification)
1.00: (This paper; will do; an introduction to the fundamentals of language theory , the fundamentals of language theory restrictions)
1.00: (This paper; will do; an introduction to the fundamentals of language theory , the fundamentals of language theory representation)

Once the fundamentals of language theory are set , we will use a worldwide known data structure format such as the JavaScript Object Notation ( JSON ) , to formally define its grammar rules and automaton .
1.00: (the fundamentals of language theory; are set; )
0.98: (we; will use; a worldwide known data structure format such as the JavaScript Object Notation Once the fundamentals of language theory are set)
0.80: (we; will use a worldwide known data structure format such as to formally define; its grammar rules)
0.79: (we; will use a worldwide known data structure format such as to formally define; its automaton)

All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .
0.98: (All of this formal background; will allow; us to transform theory into bits , by developing an algorithm)
0.84: (us; to transform; theory into bits)
0.47: (algorithm that will analyze streams of texts , streams texts as streams of texts; comply; the predefined rules)
0.21: (an algorithm that streams of texts ,; accepting; streams of texts as streams of texts)
0.26: (an algorithm will analyze streams of texts; accepting; streams of texts as streams of texts not with the predefined rules)
0.72: (that will analyze texts , streams of texts; comply; as the predefined rules)
0.27: (an algorithm that streams of texts ,; rejecting; streams of texts as streams of texts comply the predefined rules)
0.37: (an algorithm that will analyze streams of texts; rejecting; streams of texts as streams of texts not with the predefined rules)

Finally , we will analyze the outcomes of this implementation , this implementation benefits , limitations , and alternatives that could have been followed .
1.00: (we; will analyze; the outcomes of this implementation Finally)
1.00: (this implementation benefits; could have been followed; )
0.92: (we; will analyze; the outcomes of this implementation this implementation limitations Finally)
1.00: (this implementation limitations; could have been followed; )
1.00: (this implementation alternatives; could have been followed; )

In the last two decades , we have seen an amazing development of image processing techniques targeted for medical applications .
1.00: (we; have seen; an amazing development of image processing techniques In the last two decades)
1.00: (image processing techniques; targeted; for medical applications)

we propose multi-GPU-based parallel real-time algorithms for segmentation and shape-based object detection , aiming at accelerating two medical image processing methods : automated blood detection in wireless capsule endoscopy ( WCE ) images and automated bright lesion detection in retinal fundus images .
1.00: (we; propose; multi-GPU-based parallel real-time algorithms for segmentation)
0.90: (we; propose multi-GPU-based parallel real-time algorithms for segmentation aiming; at accelerating two medical image processing methods)
1.00: (we; propose; multi-GPU-based parallel real-time algorithms for shape-based object detection)
0.74: (we; multi-GPU-based parallel real-time algorithms for shape-based object detection aiming; at accelerating two medical image processing methods)
0.70: (we object; multi-GPU-based parallel real-time algorithms for shape-based detection aiming; at accelerating two medical image processing methods)

In automated blood detection in wireless capsule endoscopy ( WCE ) images and automated bright lesion detection in retinal fundus images we identified segmentation and object detection as being responsible for consuming most of the global processing time .
0.99: (we; identified; segmentation as being responsible for consuming most of the global processing time In automated blood detection in wireless capsule endoscopy ( WCE ) images)
0.99: (we; identified; object detection as being responsible for consuming most of the global processing time In automated blood detection in wireless capsule endoscopy ( WCE ) images)
1.00: (we; identified; segmentation as being responsible for consuming most of the global processing time In automated bright lesion detection in retinal fundus images)
1.00: (we; identified; object detection as being responsible for consuming most of the global processing time In automated bright lesion detection in retinal fundus images)

While in automated bright lesion detection in retinal fundus images , as segmentation was not used , shape-based object detection was the compute-intensive task identified .
1.00: (segmentation; was not used; )
0.01: (shape-based object detection; was; the compute-intensive task identified While in automated bright lesion detection in retinal fundus images , as segmentation was not used)
0.99: (the compute-intensive task; identified; )

Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .
0.97: (the accelerated method; running; on multi-GPU systems for blood detection in WCE images)
1.00: (Experimental results; show; that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version)
0.97: (the accelerated method running on multi-GPU systems for blood detection in WCE images; is; on average 265 times faster than the original CPU version)
0.72: (Experimental results; show to process; that the accelerated method running on multi-GPU systems for blood detection in WCE images is able 344 frames per second)
0.47: (Experimental results; show; that the accelerated method running on multi-GPU systems for blood detection in WCE images is able to process 344 frames per second)
0.91: (the accelerated method running on multi-GPU systems for blood detection in WCE images; is; able to process 344 frames per second)

By applying multi-GPU-based parallel real-time algorithms for segmentation and shape-based object detection for bright lesion detection in fundus images we are able to process 62 frames per second with a speedup average 667 times faster than the original CPU version .
0.98: (we; are; able to process 62 frames per second with a speedup average 667 times faster than the original CPU version images)
0.52: (we; to process faster; 62 frames per second times)

In this paper we focus on gender classification from face images .
0.99: (we; focus; on gender classification from face images In this paper)

Despite advances in equipment as well as methods , automatic face image processing for recognition or even just for the extraction of demographics , is still a challenging task in unrestricted scenarios .
0.91: (Despite equipment as well automatic face image processing for recognition; is; still a challenging task in unrestricted scenarios)
1.00: (automatic face image processing just for the extraction of demographics; is; still a challenging task in unrestricted scenarios)

Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones .
0.99: (Our tests; are aimed; at carrying out an extensive comparison of a feature based approach with two score based ones)
0.95: (Our tests; at carrying out; an extensive comparison of a feature based approach with two score based ones)
0.87: (two score; based; ones)

When directly using features , Our first apply different operators to extract the corresponding feature vectors , and then stack the corresponding feature vectors .
0.96: (Our first; apply; different operators to extract the corresponding feature vectors directly)
0.99: (Our; stack; the corresponding feature vectors When directly using features)

the corresponding feature vectors are classified by a SVM-based approach .
1.00: (the corresponding feature vectors; are classified; by a SVM-based approach)

When using scores , different operators are applied in a completely separate way , so that each of different operators produces the corresponding scores .
0.99: (different operators; are applied; When using scores)
1.00: (each of different operators; produces; the corresponding scores)

Answers are then either fed to a SVM , or compared pairwise to exploit Likelihood Ratio .
0.95: (Answers; are; either fed to a SVM then)
0.88: (Answers; fed; to a SVM)
0.98: (Answers; are; then either compared pairwise to exploit Likelihood Ratio)
0.80: (Answers; compared pairwise; to exploit Likelihood Ratio)

The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .
1.00: (The testbeds; used; for experiments)
1.00: (The testbeds used for experiments; are; EGA database)
1.00: (EGA database; presents; a good balance with respect to demographic features of stored face images)
1.00: (The testbeds used for experiments; are; GROPUS)

The obtained performances confirm that feature level fusion achieves an often better classification accuracy .
1.00: (The obtained performances; confirm; that feature level fusion achieves an often better classification accuracy)
1.00: (feature level fusion; achieves; an often better classification accuracy)

However , feature level fusion is computationally expensive .
1.00: (feature level fusion; is; computationally expensive)

Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .
1.00: (Information sources such as relational databases; contain; a tremendous amount of structured data)
0.99: (structured data; can be leveraged; to build and augment knowledge graphs)
1.00: (Information sources such as spreadsheets; contain; a tremendous amount of structured data)
1.00: (Information sources such as XML; contain; a tremendous amount of structured data)
0.68: (that; augment; )
1.00: (Information sources such as JSON; contain; a tremendous amount of structured data)
1.00: (Information sources such as Web APIs; contain; a tremendous amount of structured data)

However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .
1.00: (Information sources such as relational databases; provide; a semantic model to describe Information sources such as relational databases rarely)
1.00: (Information sources such as relational databases; provide; a semantic model to describe Information sources such as spreadsheets rarely)
0.90: (semantic; to describe; Information sources such as spreadsheets)
1.00: (Information sources such as relational databases; provide; a semantic model to describe Information sources such as XML rarely)
1.00: (Information sources such as relational databases; provide; a semantic model to describe Information sources such as JSON rarely)
1.00: (Information sources such as relational databases; provide; a semantic model to describe Information sources such as Web APIs contents rarely)
1.00: (Information sources such as spreadsheets; provide; a semantic model to describe Information sources such as relational databases rarely)
1.00: (Information sources such as spreadsheets; provide; a semantic model to describe Information sources such as spreadsheets rarely)
1.00: (Information sources such as spreadsheets; provide; a semantic model to describe Information sources such as XML rarely)
0.81: (a semantic model; to describe; Information sources such as XML)
1.00: (Information sources such as spreadsheets; provide; a semantic model to describe Information sources such as JSON rarely)
0.82: (a semantic; to describe; Information sources such as JSON)
1.00: (Information sources such as spreadsheets; provide; a semantic model to describe Information sources such as Web APIs contents rarely)
0.82: (a semantic model; to describe; Information sources such as Web APIs contents)
1.00: (Information sources such as XML; provide; a semantic model to describe Information sources such as relational databases rarely)
0.87: (a semantic model; to describe; Information sources such as relational databases)
1.00: (Information sources such as XML; provide; a semantic model to describe Information sources such as spreadsheets rarely)
0.91: (a semantic model; to describe; Information sources such as spreadsheets)
1.00: (Information sources such as XML; provide; a semantic model to describe Information sources such as XML rarely)
0.85: (a semantic; to describe; Information sources such as XML)

However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .
1.00: (Information sources such as XML; provide; a semantic model to describe Information sources such as JSON rarely)
0.90: (a semantic model; to describe; Information sources such as JSON)
1.00: (Information sources such as XML; provide; a semantic model to describe Information sources such as Web APIs contents rarely)
0.89: (a semantic model; to describe; Information sources such as Web APIs contents)
1.00: (Information sources such as JSON; provide; a semantic model to describe Information sources such as relational databases rarely)
0.91: (JSON; to describe; Information sources such as relational databases)
1.00: (Information sources such as JSON; provide; a semantic model to describe Information sources such as spreadsheets rarely)
0.71: (Information sources as JSON a semantic; to describe; Information sources such as spreadsheets)
1.00: (Information sources such as JSON; provide; a semantic model to describe Information sources such as XML rarely)
0.87: (a semantic model; to describe; Information sources such as XML)
1.00: (Information sources such as JSON; provide; a semantic model to describe Information sources such as JSON rarely)
1.00: (Information sources such as JSON; provide; a semantic model to describe Information sources such as Web APIs contents rarely)
0.70: (Information sources such as JSON a semantic model; to describe; Information sources such as Web APIs contents)
1.00: (Information sources such as Web APIs; provide; a semantic model to describe Information sources such as relational databases rarely)
1.00: (Information sources such as Web APIs; provide; a semantic model to describe Information sources such as spreadsheets rarely)
0.88: (a semantic; to describe; Information sources such as spreadsheets)
1.00: (Information sources such as Web APIs; provide; a semantic model to describe Information sources such as XML rarely)
1.00: (Information sources such as Web APIs; provide; a semantic model to describe Information sources such as JSON rarely)
0.90: (semantic; to describe; Information sources such as JSON)
1.00: (Information sources such as Web APIs; provide; a semantic model to describe Information sources such as Web APIs contents rarely)

Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .
1.00: (Semantic models of data sources; represent; the implicit meaning of the data)
0.71: (Semantic models of data sources; by specifying; the concepts)
0.62: (Semantic models data sources; by specifying; the relationships within the data)

Such models are the key ingredients to automatically publish the data into knowledge graphs .
1.00: (Such models; are; the key ingredients to automatically publish the data into knowledge graphs)
0.89: (the key ingredients; to automatically publish; the data into knowledge graphs)

Manually modeling the semantics of data sources requires significant effort and expertise , and although desirable , building Such models automatically is a challenging problem .
0.97: (desirable; building; Such models)
0.98: (although desirable , building Such models; automatically is; a challenging problem)
1.00: (Manually modeling the semantics of data sources; requires; significant effort)
1.00: (Manually modeling the semantics of data sources; requires; significant expertise)

Most of the related work focuses on semantic annotation of the data fields ( source attributes ) .
1.00: (Most of the related work; focuses; on semantic annotation of the data fields)

However , constructing a semantic model that explicitly describes the relationships between the attributes in addition to the attributes semantic types is critical .
0.98: (a semantic model; explicitly describes; the relationships between the attributes in addition to the attributes semantic types)
0.80: (, constructing a semantic model that explicitly describes the relationships between the attributes in addition to the attributes semantic types; is; critical)

We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source .
0.88: (We; present; a novel approach that exploits the knowledge from a domain ontology automatically learn a rich semantic model)
0.78: (a novel approach; exploits; the knowledge from a domain ontology to automatically a rich semantic model for a new source)
0.69: (a novel approach that exploits the knowledge from a domain ontology; to automatically learn; a rich semantic model for a new source)
1.00: (We; present; a novel approach that exploits the semantic models of previously modeled sources)
1.00: (a novel approach; exploits; the semantic models of previously modeled sources)
0.17: (a novel approach the semantic models of previously modeled sources; to automatically learn; a rich semantic model for a new source)

a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .
0.94: (a rich semantic model for a new source; represents; the semantics of a new source in terms of the concepts)
1.00: (the concepts; defined; by a domain ontology)
0.99: (a rich semantic model for a new source; represents; the semantics of a new source in terms of the relationships)
1.00: (the relationships; defined; by a domain ontology)

Given some sample data from a new source , We leverage the knowledge in a domain ontology and the known semantic models to construct a weighted graph that represents the space of plausible semantic models for a new source .
1.00: (a weighted graph; represents; the space of plausible semantic models for a new source)
1.00: (We; leverage; the knowledge in a domain ontology)
0.81: (We; leverage the knowledge in a domain ontology to construct; a weighted graph that represents the space of plausible semantic models for a new source)
1.00: (We; leverage; the known semantic models to construct a weighted graph)
0.50: (We; leverage the known semantic models to construct; a weighted graph that represents the space of plausible semantic models for a new source)

Then , We compute the top k candidate semantic models and suggest to the user a ranked list of the semantic models for a new source .
0.98: (We; compute; the top k candidate semantic models Then)
1.00: (We; suggest; to the user a ranked list of the semantic models for a new source Then)

The approach takes into account user corrections to learn more accurate semantic models on future data sources .
1.00: (The approach; takes; into account user corrections to learn more accurate semantic models on future data sources)

We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .
0.80: (We evaluation; shows; that a novel approach that exploits the knowledge from a domain ontology to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources with minimal user input)
0.40: (a novel approach that exploits the knowledge from a domain ontology model for; to automatically learn generates; a rich semantic a new source expressive semantic models for data sources with minimal user input)
0.65: (a novel approach that; exploits; the knowledge from a domain ontology)
0.04: (a novel approach that; the knowledge from a domain ontology to automatically learn; a rich semantic model for a new source)
0.80: (We evaluation; shows; that a novel approach that exploits the knowledge from a domain ontology to automatically learn a rich semantic model for a new source generates expressive semantic models for services with minimal user input)
0.39: (a novel approach that exploits the knowledge from a domain ontology to a model for new; automatically learn generates; rich semantic a source expressive semantic models for services with minimal user input)
0.83: (We evaluation; shows; that a novel approach that exploits the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources with minimal user input)
0.36: (a novel approach that exploits the semantic models of previously modeled sources model; automatically learn generates; to a rich semantic for a new source expressive semantic models for data sources with minimal user input)
0.53: (a novel approach that; exploits; the semantic models of previously modeled sources)
0.02: (a novel approach; modeled automatically learn; the semantic models of previously sources a rich semantic model for a new source)

We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .
0.67: (We evaluation; shows; that a novel approach that exploits the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for services with minimal user input)
0.21: (a novel approach that exploits the semantic models of previously modeled sources learn model for; automatically generates; to a rich semantic a new source expressive semantic models for services with minimal user input)
0.45: (a novel approach that; exploits; the semantic models of previously modeled sources)
0.03: (a novel approach the semantic of previously modeled; automatically learn; models sources a rich semantic model for a new source)

expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .
1.00: (expressive semantic models for data sources; make; it possible to automatically integrate the data across sources)
1.00: (expressive semantic models for data services; make; it possible to automatically integrate the data across sources)
1.00: (expressive semantic models for data sources; make; it possible to provide rich support for source discovery)
1.00: (expressive semantic models for data sources; make; it possible to provide rich support for service composition)
1.00: (expressive semantic models for data services; make; it possible to provide rich support for source discovery)
1.00: (expressive semantic models for data services; make; it possible to provide rich support for service composition)

expressive semantic models for data sources and services also make it possible to automatically publish semantic data into knowledge graphs .
0.98: (expressive semantic models for data sources; also make; it possible to automatically publish semantic data into knowledge graphs)
0.98: (expressive semantic models for data services; also make; it possible to automatically publish semantic data into knowledge graphs)

Information technology has been contributing to various areas of knowledge ; in particular , the field of education stands out .
1.00: (the field of education; stands out; )
1.00: (Information technology; has been contributing; to various areas of knowledge)

In what concerns the teaching of computer programming , literature contains important efforts that aim to assist in the learning process .
1.00: (literature; contains; important efforts that aim to assist in the learning process In what concerns the teaching of computer programming)
1.00: (important efforts; aim; to assist in the learning process)
0.99: (important efforts; aim to assist; in the learning process)

Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .
1.00: (new Computer Science students; have; difficulties in understanding the problem logics usually)
1.00: (Teaching algorithms for first year students; has been; a great challenge for universities always)
0.63: (new Computer Science students; have in understanding; the problem logics)
1.00: (new Computer Science students; have; difficulties in abstracting the problem logics usually)
0.62: (new Computer Science students; have in abstracting; the problem logics)
1.00: (Teaching programming concepts for first year students; has been; a great challenge for universities always)

An alternative that has contributed to the teaching-learning process is the use of Learning Objects ( LO ) , which contribute towards mediating and enhancing the teaching-learning process .
1.00: (An alternative; has contributed; to the teaching-learning process)
1.00: (An alternative that has contributed to the teaching-learning process; is; the use of Learning Objects)
0.83: (the use of Learning Objects; contribute; towards mediating the teaching-learning process)
0.88: (the use of Learning Objects; contribute; towards enhancing the teaching-learning process)

One of the great difficulties of learning during the initial semesters of Engineering and Computer Science courses is related to the contents of computer programming , which increases the students ' failure level and also the dropout rate of such courses .
1.00: (One of the great difficulties of learning during the initial semesters of Engineering courses; is related; to the contents of computer programming)
1.00: (computer programming; increases; the students ' failure level)
1.00: (One of the great difficulties of learning during the initial semesters of Computer Science courses; is related; to the contents of computer programming)
0.91: (the of computer programming; increases; the students ' failure level)

In order to decrease those rates , we have developed a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses , and the results were very positive .
0.79: (we; have developed; a project to create various learning objects help concepts)
0.15: (we; to create; a project various learning objects to help teach concepts students of Science courses)
0.11: (we a project; have developed various learning objects to help; teach concepts that are considered difficult to understand by students of Science courses)
0.12: (we to create; teach; concepts that are considered difficult to understand by students of Science courses)
1.00: (the results; were; very positive)

This paper presents the qualitative and quantitative results of the experiment we conducted with the development and application of learning objects to help teaching students of Computer Science .
1.00: (This paper; presents; the qualitative results of the experiment)
0.93: (the experiment; conducted; we with the development of learning objects)
0.93: (the experiment; conducted; we with the application of learning objects)
1.00: (This paper; presents; the quantitative results of the experiment)
1.00: (the experiment; conducted; with the development of learning objects)
0.87: (the experiment we; conducted; with the application of learning objects to)

a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process .
1.00: (outcome data; showed; that the use of learning objects contributes significantly to the teaching-learning process)
1.00: (the use of learning objects; contributes significantly; to the teaching-learning process)
0.86: (a project; to create; various learning objects to help teach concepts)
0.06: (a project to create various learning objects concepts; was conducted; considered difficult to understand by students of Science courses in 2013)
0.25: (a project various learning objects to help; to create; teach concepts that are)
0.02: (various learning objects; to help teach; concepts that are considered difficult to understand by students of Science courses)
0.07: (a project to create various learning objects concepts; was conducted; considered difficult to understand by students of Science courses in 2014)