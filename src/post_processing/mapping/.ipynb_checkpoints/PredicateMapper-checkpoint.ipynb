{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "623ea00f-68b2-4b1b-87f3-cd8d6c94e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## read data -----> utilities\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents as a Python dictionary.\n",
    "\n",
    "    :param file_path: The path to the JSON file.\n",
    "    :type file_path: str\n",
    "    :return: A dictionary representing the JSON data.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "        \n",
    "def loadVerbMap(verb_map_path):\n",
    "    verb_info = pd.read_csv(verb_map_path, sep=',')\n",
    "    verb_map = {}\n",
    "    for i,r in verb_info.iterrows():\n",
    "        for j in range(34):\n",
    "            verb = r['v' + str(j)]\n",
    "            if str(verb) != 'nan':\n",
    "                verb_map[verb] = r['predicate']\n",
    "    return verb_map\n",
    "\n",
    "def encode_and_store(sentences, model, file_path):\n",
    "    # Encode sentences\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Create a dictionary with sentences as keys and their embeddings as values\n",
    "    embeddings_dict = {sentence: embedding.tolist() for sentence, embedding in zip(sentences, embeddings)}\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(embeddings_dict, file)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    # Load the embeddings from the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        embeddings_dict = json.load(file)\n",
    "\n",
    "    # Convert the embeddings from list to numpy array\n",
    "    embeddings_dict = {sentence: [float(value) for value in embedding] for sentence, embedding in embeddings_dict.items()}\n",
    "    return embeddings_dict\n",
    "\n",
    "# Example sentences\n",
    "sentences = list(verb_map.keys())\n",
    "\n",
    "# Model initialization\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# File path to store and load embeddings\n",
    "\n",
    "embd_path = 'C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/predicate_embeddings.json'\n",
    "\n",
    "# Encode sentences, store in a file, and then load them\n",
    "encode_and_store(sentences, model, embd_path)\n",
    "loaded_embeddings = load_embeddings(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f556ce73-1606-4501-a0e0-9aa9b6929566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PredicateMapper_utilities\n",
    "\n",
    "class PredicateMapper:\n",
    "    def __init__(self, verbs_path, triples, embd_path):\n",
    "        self.predicate_embd = PredicateMapper_utilities.load_embeddings(embd_path)\n",
    "        self.verb_map = PredicateMapper_utilities.loadVerbMap(verbs_path)\n",
    "        self.input_triples = triples\n",
    "        self.mapped_predicate = {}\n",
    "        \n",
    "    ##this function will be used in predicate mapping return the max of sim\n",
    "    # def similarity_mapping(self, predicate):\n",
    "    #     print(predicate)\n",
    "    #     model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    #     embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "    #     embd_predicate = np.array(embd_predicate).reshape(1, -1)\n",
    "    #     results = []\n",
    "    #     for k,v in self.predicate_embd.items():\n",
    "    #         embd_verb = np.array(v).reshape(1, -1)\n",
    "    #         results.append(\n",
    "    #             {\n",
    "    #                 k: cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "    #             }\n",
    "    #         )\n",
    "    #     max_element = max(results, key=lambda x: list(x.values())[0])\n",
    "    #     print(max_element)\n",
    "    #     return max_element\n",
    "\n",
    "\n",
    "    def similarity_mapping(self, predicate):\n",
    "       \n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "        embd_predicate = np.array(embd_predicate).reshape(1, -1)\n",
    "        \n",
    "        max_similarity = float('-inf')\n",
    "        max_element = None\n",
    "    \n",
    "        for k, v in self.predicate_embd.items():\n",
    "            embd_verb = np.array(v).reshape(1, -1)\n",
    "            similarity = cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "    \n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_element = {k: similarity}\n",
    "    \n",
    "        # print(max_element)\n",
    "        return max_element\n",
    "    \n",
    "\n",
    "    def get_last_pp(self, predicate):\n",
    "        ## return a tuple the predicate with out last pp and the pp\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def check_last_word_preposition(self, predicate):\n",
    "    # Liste des prépositions en anglais (à compléter si nécessaire)\n",
    "        prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"around\", \n",
    "                        \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"by\", \n",
    "                        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \n",
    "                        \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\", \"on\", \"onto\", \"out\", \"outside\", \"over\", \n",
    "                        \"past\", \"regarding\", \"round\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"under\", \n",
    "                        \"underneath\", \"until\", \"unto\", \"up\", \"upon\", \"with\", \"within\", \"without\"]\n",
    "    \n",
    "        words = predicate.split()\n",
    "        last_word = words[-1].lower()  # Convertir en minuscules pour la comparaison en anglais\n",
    "    \n",
    "        if last_word in prepositions:\n",
    "            # Le dernier mot est une préposition\n",
    "            sentence_without_preposition = ' '.join(words[:-1])\n",
    "            return (sentence_without_preposition, last_word)\n",
    "        else:\n",
    "            # Le dernier mot n'est pas une préposition\n",
    "            return (predicate,None)\n",
    "    \n",
    "    ## return the dic of mapped predicate\n",
    "        # threshold: the predicate mapping threshold\n",
    "        # option: keep or  delete for unmpapped predicate\n",
    "            ## keep: keep the as them (not mention them in the dict mapping)\n",
    "            ## delete: in the mapping dict add a message that indicate thist triple should be deleted (\"invalid triple\")\n",
    "    def predicate_mapping(self, threshold, option = \"keep\"):\n",
    "        for triple in tqdm(self.input_triples, desc=\"predicate mapping\"):\n",
    "            predicate = triple[\"predicate\"]\n",
    "            predicate_ = self.check_last_word_preposition(predicate)\n",
    "            if predicate_:\n",
    "                if predicate_[0] in self.verb_map.keys(): \n",
    "                    pp = \"\"\n",
    "                    if predicate_[1]:\n",
    "                        pp = \" \"+ predicate_[1]\n",
    "                    self.mapped_predicate[predicate] = self.verb_map[predicate_[0]] + pp\n",
    "                    # print(\"direct mapping:  \", predicate,\"-->\" ,self.verb_map[predicate_[0]] + pp )\n",
    "                else:\n",
    "                   \n",
    "                    max_sim_verb = self.similarity_mapping(predicate_[0])\n",
    "                    # print(max_sim_verb)\n",
    "                    if list(max_sim_verb.values())[0] >= threshold:\n",
    "                        pp = \"\"\n",
    "                        if predicate_[1]:\n",
    "                            pp = \" \"+ predicate_[1]\n",
    "                        self.mapped_predicate[predicate] = self.verb_map[list(max_sim_verb.keys())[0]] + pp\n",
    "                        # print(\"sim:\",max_sim_verb,\"\\n\")\n",
    "                        # print(\"similarity mapping:  \", predicate,\"-->\", self.verb_map[list(max_sim_verb.keys())[0]] + pp )\n",
    "                    else:\n",
    "                        ### keep or delete ! \n",
    "                        if option == \"delete\":\n",
    "                            self.mapped_predicate[predicate] = \"invalid triple\"\n",
    "                        if option == \"keep\":\n",
    "                            pass\n",
    "                        else:\n",
    "                            print(\"invalid option !\")\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1c72e605-4393-419b-886c-139504fa3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/syntactic_cleaning/cleaned_triples.json\"\n",
    "verbs_map_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/CSKG_VerbNet_verb_map.csv\"\n",
    "embd_path = 'C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/predicate_embeddings.json'\n",
    "triples = PredicateMapper_utilities.read_json_file(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "bc418f0d-1b57-471a-843d-b415ab1ca56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predicate mapping: 100%|██████████| 57/57 [01:59<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "pm = PredicateMapper(verbs_map_path, triples,embd_path)\n",
    "pm.predicate_mapping(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f8c79119-d8b3-4f2e-812f-e1cc939e6282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consist of': 'skos:broader/is/hyponym-of of',\n",
       " 'have': 'includes',\n",
       " 'can be exploited by': 'uses by',\n",
       " 'are offered to analyze': 'analyzes',\n",
       " 'to analyze': 'analyzes',\n",
       " 'may be selected for': 'acquires for',\n",
       " 'is': 'skos:broader/is/hyponym-of',\n",
       " 'are to analyze more accurately': 'analyzes',\n",
       " 'is considered in': 'includes in',\n",
       " 'is applied on': 'uses on',\n",
       " 'are compared with': 'matches with',\n",
       " 'to precisely predict': 'predicts',\n",
       " 'affects': 'causes',\n",
       " 'records': 'acquires',\n",
       " 'is used to identify': 'identifies',\n",
       " 'to identify': 'identifies',\n",
       " 'may lead to': 'guides to',\n",
       " 'are required to build': 'based-on',\n",
       " 'to build': 'based-on',\n",
       " 'assigns': 'provides',\n",
       " 'is to present': 'provides',\n",
       " 'have been introduced to classify': 'classifies',\n",
       " 'provided by': 'provides by',\n",
       " 'is used in': 'uses in',\n",
       " 'using': 'uses',\n",
       " 'are': 'skos:broader/is/hyponym-of',\n",
       " 'achieved respectively': 'acquires',\n",
       " 'suggests': 'proposes',\n",
       " 'be': 'skos:broader/is/hyponym-of',\n",
       " 'plays in': 'executes in',\n",
       " 'allows': 'supports',\n",
       " 'define': 'predicts',\n",
       " 'need to follow to create': 'produces',\n",
       " 'is creating': 'produces',\n",
       " 'do need to develop': 'produces',\n",
       " 'to develop to understand': 'produces',\n",
       " 'will use': 'uses',\n",
       " 'comply': 'adapts',\n",
       " 'will analyze': 'analyzes'}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.mapped_predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15fb1e-4d15-4277-a556-5c30e1f48bba",
   "metadata": {},
   "source": [
    "#### similarity based mapping (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c196e17e-7457-4ce0-96d0-ecf1cf2550d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kill': 0.4583008035607263}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity_mapping(predicate, predicate_embd):\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "    embd_predicate = np.array(embd_predicate).reshape(1, -1)\n",
    "\n",
    "    max_similarity = float('-inf')\n",
    "    max_element = None\n",
    "\n",
    "    for k, v in predicate_embd.items():\n",
    "        embd_verb = np.array(v).reshape(1, -1)\n",
    "        similarity = cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_element = {k: similarity}\n",
    "\n",
    "    # print(max_element)\n",
    "    return max_element\n",
    "# p ='are required to build'\n",
    "# p = 'have been repeated for'\n",
    "# p = 'do need do need to'\n",
    "# p = \"is creating\"\n",
    "# p = \"to dive deep\"\n",
    "# p = 'will use'\n",
    "p = 'violate'\n",
    "predicate_embd = PredicateMapper_utilities.load_embeddings(embd_path)\n",
    "similarity_mapping(p, predicate_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f209f6db-e225-4cd7-b9fd-4b441675d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_map = PredicateMapper_utilities.loadVerbMap(verbs_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "23d2d649-cca9-44d7-be50-eef973571522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affects'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_map['kill']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
