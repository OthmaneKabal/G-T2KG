{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf87c7f-f3fe-42b4-a8e0-abd0e79bcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data -----> utilities\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents as a Python dictionary.\n",
    "\n",
    "    :param file_path: The path to the JSON file.\n",
    "    :type file_path: str\n",
    "    :return: A dictionary representing the JSON data.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d97c1b-d597-4504-946e-98e299c83c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from django.utils.encoding import smart_str\n",
    "# from sentence_transformers import util\n",
    "from multiprocessing import Process\n",
    "from urllib.parse import unquote\n",
    "from random import shuffle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdflib\n",
    "import urllib\n",
    "import random\n",
    "import pickle \n",
    "# import torch\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class EntitiesMapper:\n",
    "    def __init__(self, data_path, output_path):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.input_triples = self.get_input_triples()\n",
    "        self.mapping_result = {}\n",
    "        self.entities = []\n",
    "        self.all_pairs = []\n",
    "        self.get_entities_pairs()\n",
    "        self.e2wikidata = {}\n",
    "        self.e2dbpedia = {}\n",
    "        self.e_transformer = {}\n",
    "        self.e2neighbors = {} ## only for context \n",
    "        self.cskg2wikidata = {}\n",
    "        self.cskg2dbpedia = {}\n",
    "        self.label2cskg_entity = {}\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_input_triples(self):\n",
    "        return read_json_file(self.data_path)\n",
    "    \n",
    "    def get_entities_pairs(self):\n",
    "        print(\"get Entities and pairs\")\n",
    "        for triple in self.input_triples:\n",
    "            if triple[\"subject\"] not in self.entities:\n",
    "                self.entities.append(triple[\"subject\"])\n",
    "            if triple[\"object\"] not in self.entities:\n",
    "                self.entities.append(triple[\"object\"])\n",
    "\n",
    "            if tuple([triple[\"subject\"], triple[\"object\"]]) not in self.all_pairs:\n",
    "                self.all_pairs.append(tuple([triple[\"subject\"], triple[\"object\"]]))\n",
    "                \n",
    "    \n",
    "    def linkThroughWikidata(self):\n",
    "        print('- \\t >> Mapping with wikidata started')\n",
    "        timepoint = time.time()\n",
    "        entities_to_explore = list(set(self.entities) - set(self.e2wikidata.keys()))\n",
    "\n",
    "        if len(entities_to_explore) <= 0:\n",
    "            return\n",
    "\n",
    "        # sorting entities\n",
    "        entities_to_explore = sorted(entities_to_explore, key=lambda x: len(x), reverse=True)\n",
    "        c = 0\n",
    "\n",
    "        while c < len(entities_to_explore):\n",
    "            e = entities_to_explore[c]\n",
    "\n",
    "            query = \"\"\"\n",
    "                    SELECT DISTINCT ?entity ?altLabel\n",
    "                    WHERE{\n",
    "                            {\n",
    "                                ?entity  <http://www.w3.org/2000/01/rdf-schema#label> \\\"\"\"\" + e +\"\"\"\\\"@en .\n",
    "                                {?entity wdt:31+ wd:Q21198 }  UNION \n",
    "                                {?entity wdt:31/wdt:P279* wd:Q21198} UNION\n",
    "                                {?entity wdt:P279+ wd:Q21198} UNION\n",
    "                                {?entity  wdt:P361+ wd:Q21198} UNION\n",
    "                                { ?entity  wdt:P1269+ wd:Q21198} UNION\n",
    "                                {FILTER NOT EXISTS {?entity <http://schema.org/description> \"Wikimedia disambiguation page\"@en}}\n",
    "                                 OPTIONAL {\n",
    "                                    ?entity <http://www.w3.org/2004/02/skos/core#altLabel> ?altLabel .\n",
    "                                    FILTER(LANG(?altLabel) = 'en')\n",
    "                                }\n",
    "                                \n",
    "\n",
    "                            }  UNION {\n",
    "                                ?entity <http://www.w3.org/2004/02/skos/core#altLabel> \\\"\"\"\" + e +\"\"\"\\\"@en .\n",
    "                                {?entity wdt:31+ wd:Q21198 }  UNION \n",
    "                                {?entity wdt:31/wdt:P279* wd:Q21198} UNION\n",
    "                                {?entity wdt:P279+ wd:Q21198} UNION\n",
    "                                {?entity  wdt:P361+ wd:Q21198} UNION\n",
    "                                { ?entity  wdt:P1269+ wd:Q21198} \n",
    "                                 OPTIONAL {\n",
    "                                    ?entity <http://www.w3.org/2004/02/skos/core#altLabel> ?altLabel .\n",
    "                                    FILTER(LANG(?altLabel) = 'en')\n",
    "                                }\n",
    "                                SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "\n",
    "                            }\n",
    "                        }\n",
    "                    \"\"\"\n",
    "\n",
    "            url = 'https://query.wikidata.org/sparql'\n",
    "            data = urllib.parse.urlencode({'query': query}).encode()\n",
    "            headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "\n",
    "            try:\n",
    "                req = urllib.request.Request(url, data=data, headers=headers)\n",
    "                response = urllib.request.urlopen(req)\n",
    "\n",
    "                if response.status == 200:\n",
    "\n",
    "                    result = response.read().decode('ascii', errors='ignore')\n",
    "                    jresponse = json.loads(result)\n",
    "                    variables = jresponse['head']['vars']\n",
    "                    for binding in jresponse['results']['bindings']:\n",
    "\n",
    "                        if 'entity' in binding and e not in self.e2wikidata:\n",
    "                            if 'http://www.wikidata.org/entity/Q' in binding['entity']['value']:\n",
    "                                self.e2wikidata[e] = binding['entity']['value']\n",
    "\n",
    "                        if 'altLabel' in binding:\n",
    "                            if binding['altLabel']['value'].lower() in entities_to_explore and binding['altLabel']['value'].lower() not in self.e2wikidata:\n",
    "                                if 'http://www.wikidata.org/entity/Q' in binding['entity']['value']:\n",
    "                                    self.e2wikidata[binding['altLabel']['value'].lower()] = binding['entity']['value']\n",
    "\n",
    "                    c += 1\n",
    "                    if c % 100 == 0:\n",
    "                        print('\\t >> Wikidata Processed', c, 'entities in {:.2f} secs.'.format(time.time() - timepoint))\n",
    "                        # pickle_out = open(\"../../resources/e2wikidata.pickle\", \"wb\")\n",
    "                        # pickle.dump(self.e2wikidata, pickle_out)\n",
    "                        # pickle_out.flush()\n",
    "                        # pickle_out.close()\n",
    "\n",
    "            except urllib.error.HTTPError as err:\n",
    "                print(err)\n",
    "                print(err.headers)\n",
    "                print('sleeping...')\n",
    "                time.sleep(60)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "        # pickle_out = open(\"../../resources/e2wikidata.pickle\", \"wb\")\n",
    "        # pickle.dump(self.e2wikidata, pickle_out)\n",
    "        # pickle_out.close()\n",
    "        print('> Mapped to Wikidata:', len(self.e2wikidata))\n",
    "        \n",
    "    def findNeiighbors(self):\n",
    "    \n",
    "    \t\tfor s,o in self.all_pairs:\n",
    "    \t\t\tif s not in self.e2neighbors:\n",
    "    \t\t\t\tself.e2neighbors[s] = []\n",
    "    \t\t\tif len(self.e2neighbors[s]) < 20:\n",
    "    \t\t\t\tself.e2neighbors[s] += [o]\n",
    "    \n",
    "    \t\t\tif o not in self.e2neighbors:\n",
    "    \t\t\t\tself.e2neighbors[o] = []\n",
    "    \t\t\tif len(self.e2neighbors[o]) < 20:\n",
    "    \t\t\t\tself.e2neighbors[o] += [s]\n",
    "\n",
    "    def linkThroughDbpedia(self):\n",
    "        print('- \\t >> Mapping with dbpedia started')\n",
    "        \n",
    "        entities_to_explore = set(self.entities) - set(self.e2dbpedia.keys())\n",
    "        if len(entities_to_explore) <= 0:\n",
    "            return\n",
    "\n",
    "        self.findNeiighbors()\n",
    "\n",
    "        c = 0\n",
    "        timepoint = time.time()\n",
    "        for e in entities_to_explore:\n",
    "            if e not in self.e2dbpedia:\n",
    "                neighbors = self.e2neighbors[e]\n",
    "                \n",
    "                #content = [e] + [self.id2e[nid] for nid in neighbors_ids[:20]]\n",
    "                content = [e] + neighbors\n",
    "                shuffle(content)\n",
    "                content = ' '.join(content)\n",
    "                \n",
    "                url = 'https://api.dbpedia-spotlight.org/en/annotate'\n",
    "                data = urllib.parse.urlencode({'text': content})\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "        \n",
    "                try:\n",
    "                    req = urllib.request.Request(url + '?' + data, headers=headers)\n",
    "                    response = urllib.request.urlopen(req)\n",
    "                    if response.status == 200:\n",
    "                    \n",
    "                        result = response.read().decode('ascii', errors='ignore')\n",
    "                        jresponse = json.loads(result)\n",
    "                        \n",
    "                        if 'Resources' in jresponse:\n",
    "                            for resource in jresponse['Resources']:\n",
    "                                if resource['@surfaceForm'] == e and float(resource['@similarityScore']) >= 0.8:\n",
    "                                        self.e2dbpedia[e] = resource['@URI']\n",
    "                                        break\n",
    "\n",
    "                except urllib.error.HTTPError as e:\n",
    "                    print('HTTPError: {}'.format(e.code), 'sleeping...')\n",
    "                    time.sleep(60)\n",
    "                except:\n",
    "                    print('E:', e)\n",
    "                    pass\n",
    "\n",
    "                c += 1\n",
    "                if c % 10000 == 0:\n",
    "                    print('- \\t>> DBpedia Processed', c, 'entities in', (time.time() - timepoint), 'secs')\n",
    "                    # pickle_out = open(\"../../resources/e2dbpedia.pickle\",\"wb\")\n",
    "                    # pickle.dump(self.e2dbpedia, pickle_out)\n",
    "                    # pickle_out.close()\n",
    "\n",
    "\t\t# pickle_out = open(\"../../resources/e2dbpedia.pickle\",\"wb\")\n",
    "\t\t# pickle.dump(self.e2dbpedia, pickle_out)\n",
    "\t\t# pickle_out.close()\n",
    "\t\t# print('- \\t >> Mapped to DBpedia:', len(self.e2dbpedia))\n",
    "\n",
    "    def mergeEntities(self):\n",
    "        wikidata2cskg = {}\n",
    "        dbpedia2cskg = {}\n",
    "\n",
    "        for (s,o) in self.all_pairs:\n",
    "            if s in self.e2dbpedia:\n",
    "                if self.e2dbpedia[s] not in dbpedia2cskg:\n",
    "                    dbpedia2cskg[self.e2dbpedia[s]] = []\n",
    "                dbpedia2cskg[self.e2dbpedia[s]] += [s]\n",
    "            if s in self.e2wikidata:\n",
    "                if self.e2wikidata[s] not in wikidata2cskg:\n",
    "                    wikidata2cskg[self.e2wikidata[s]] = []\n",
    "                wikidata2cskg[self.e2wikidata[s]] += [s]\n",
    "            if o in self.e2dbpedia:\n",
    "                if self.e2dbpedia[o] not in dbpedia2cskg:\n",
    "                    dbpedia2cskg[self.e2dbpedia[o]] = []\n",
    "                dbpedia2cskg[self.e2dbpedia[o]] += [o]\n",
    "            if o in self.e2wikidata:\n",
    "                if self.e2wikidata[o] not in wikidata2cskg:\n",
    "                    wikidata2cskg[self.e2wikidata[o]] = []\n",
    "                wikidata2cskg[self.e2wikidata[o]] += [o]\n",
    "\n",
    "\t\t# merging with dbpedia\n",
    "        for dbe, cskg_entities_labels in dbpedia2cskg.items():\n",
    "\t\t\t\n",
    "\t\t\t# check if there exists an entity\n",
    "            cskg_entity = None\n",
    "            for label in list(set(cskg_entities_labels)):\n",
    "                if label in self.label2cskg_entity:\n",
    "                    cskg_entity = self.label2cskg_entity[label]\n",
    "                    break\n",
    "\n",
    "            if cskg_entity == None:\n",
    "                cskg_entity = max(list(set(cskg_entities_labels)), key=len)\n",
    "\t\t\t\n",
    "            for label in list(set(cskg_entities_labels)):\n",
    "                self.label2cskg_entity[label] = cskg_entity\n",
    "            self.cskg2dbpedia[cskg_entity] = dbe\n",
    "\n",
    "\n",
    "\t\t# merging with wikidata\n",
    "        for wde, cskg_entities_labels in wikidata2cskg.items():\n",
    "\t\t\t\n",
    "            # check if there exists an entity\n",
    "            cskg_entity = None\n",
    "            for label in list(set(cskg_entities_labels)):\n",
    "                if label in self.label2cskg_entity:\n",
    "                    cskg_entity = self.label2cskg_entity[label]\n",
    "                    break\n",
    "\n",
    "            if cskg_entity == None:\n",
    "                cskg_entity = max(list(set(cskg_entities_labels)), key=len)\n",
    "\n",
    "            for label in list(set(cskg_entities_labels)):\n",
    "                self.label2cskg_entity[label] = cskg_entity\n",
    "            self.cskg2wikidata[cskg_entity] = wde\n",
    "\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        p_wikidata = Process(target=self.linkThroughWikidata)\n",
    "        p_dbpedia = Process(target=self.linkThroughDBpediaSpotLight)\n",
    "        p_dbpedia.start()\n",
    "        p_wikidata.start() \n",
    "        try: p_wikidata.join() \n",
    "        except: pass \n",
    "        try: p_dbpedia.join()\n",
    "        except: pass \n",
    "\n",
    "\n",
    "    ##################### Mapping using transformers ##############################\n",
    "# function used by mergeEntitiesEuristic\n",
    "    def mergeEntitiesEmbeddings(self, model, entities):\n",
    "\n",
    "        paraphrases = util.paraphrase_mining(model, entities, query_chunk_size=100, corpus_chunk_size=10000, batch_size=256, top_k=5, show_progress_bar=False)\n",
    "\n",
    "        for paraphrase in paraphrases:\n",
    "            score, i, j = paraphrase\n",
    "            ei = entities[i] # entity\n",
    "            ej = entities[j] # entity \n",
    "\t\t\t# since the results are ordered, the loop is stopped when the similarity is lower than 0.9\n",
    "            if score < 0.9:\n",
    "                break\n",
    "\n",
    "            if ei not in self.label2cskg_entity and ej not in self.label2cskg_entity:\n",
    "                self.label2cskg_entity[ej] = ei\n",
    "                self.label2cskg_entity[ei] = ei\n",
    "                #print(ej, '->', ei, ' : ', score)\n",
    "            elif ei not in self.label2cskg_entity and ej in self.label2cskg_entity:\n",
    "                self.label2cskg_entity[ei] = self.label2cskg_entity[ej]\n",
    "                #print(ei, '->', ej, '->',  self.label2cskg_entity[ej], ' : ', score)\n",
    "            elif ei in self.label2cskg_entity and ej not in self.label2cskg_entity:\n",
    "                self.label2cskg_entity[ej] = self.label2cskg_entity[ei]\n",
    "                #print(ej, '->', ei, '->',  self.label2cskg_entity[ei], ' : ', score)\n",
    "\n",
    "\n",
    "    def mergeEntitiesEuristic(self):\n",
    "\n",
    "\n",
    "        try:\n",
    "            # merge lables with separate embeddings merging previously computed if it exists, otherwise it will be computed in the\n",
    "            # execution flow of the code\n",
    "            f = open('../../resources/only_embeddings_label2cskg_entity.pickle', 'rb')\n",
    "            only_embeddings_label2cskg_entity = pickle.load(f)\n",
    "            f.close()\n",
    "            for (ei, ej) in only_embeddings_label2cskg_entity.items():\t\n",
    "\t\t\t\t\n",
    "                if ei not in self.label2cskg_entity and ej not in self.label2cskg_entity:\n",
    "                    self.label2cskg_entity[ej] = ei\n",
    "                    self.label2cskg_entity[ei] = ei\n",
    "                elif ei not in self.label2cskg_entity and ej in self.label2cskg_entity:\n",
    "                    self.label2cskg_entity[ei] = self.label2cskg_entity[ej]\n",
    "                elif ei in self.label2cskg_entity and ej not in self.label2cskg_entity:\n",
    "                    self.label2cskg_entity[ej] = self.label2cskg_entity[ei]\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            # sentence-transformers/paraphrase-distilroberta-base-v2\n",
    "            model = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\n",
    "            word2entities = {}\n",
    "            \n",
    "            for (s,o) in self.all_pairs:\n",
    "                stokens = word_tokenize(s)\n",
    "                otokens = word_tokenize(o)\n",
    "\n",
    "                for t in stokens:\n",
    "                    if t not in word2entities:\n",
    "                        word2entities[t] = set()\n",
    "                    word2entities[t].add(s)\n",
    "\n",
    "                for t in otokens:\n",
    "                    if t not in word2entities:\n",
    "                        word2entities[t] = set()\n",
    "                    word2entities[t].add(o)\n",
    "\n",
    "            wordcount = len(word2entities)\n",
    "            for word, entities in word2entities.items():\n",
    "                #print(wordcount, word, len(entities))\n",
    "                #wordcount -= 1\n",
    "                if len(entities) > 1:\n",
    "                    self.mergeEntitiesEmbeddings(model, list(entities))\n",
    "                #print('\\t>> tokens to be checked:', wordcount)\n",
    "\n",
    "    def run(self):\n",
    "        print('\\t>> Entities to be mapped:', len(self.entities))\n",
    "        # self.load()\n",
    "        self.linkThroughWikidata()\n",
    "        self.linkThroughDbpedia()\n",
    "        self.mergeEntities()\n",
    "        self.mergeEntitiesEuristic()\n",
    "        ## apply_mapping \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "635244ff-8157-4ede-8991-225dc27921db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get Entities and pairs\n"
     ]
    }
   ],
   "source": [
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/syntactic_cleaning/cleaned_triples.json\"\n",
    "output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/mapping/mapped_triples_555.json\"\n",
    " \n",
    "Mapper = EntitiesMapper(input_path,output_path )\n",
    "Mapper.entities = [\"bright lesion detection\",\"automated bright lesion detection\",\"network security\",\"network security compromise\",\"machine learning approach\", \"machine learning method\", \"machine learning algorithm\", \"deep learning approach\",\"ML method\", \"deep learning method\"]\n",
    "Mapper.all_pairs = [(\"machine learning approach\",\"deep learning method\"),\n",
    "                    (\"machine learning method\",\"machine learning algorithm\"),\n",
    "                    (\"deep learning approach\",\"deep learning method\"),\n",
    "                    (\"deep learning method\",\"ML method\"),\n",
    "                    (\"bright lesion detection\",\"automated bright lesion detection\")\n",
    "                    \n",
    "    \n",
    "]\n",
    "Mapper.mergeEntitiesEuristic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ccb9296-1b05-4706-9aa7-0f15e63f4d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim( deep learning approach , deep learning method ) = 0.9530277252197266 \n",
      "\n",
      "sim( machine learning approach , machine learning method ) = 0.9473652839660645 \n",
      "\n",
      "sim( bright lesion detection , automated bright lesion detection ) = 0.9195941686630249 \n",
      "\n",
      "sim( machine learning method , machine learning algorithm ) = 0.9101974964141846 \n",
      "\n",
      "sim( machine learning approach , machine learning algorithm ) = 0.8752262592315674 \n",
      "\n",
      "sim( network security , network security compromise ) = 0.7709977626800537 \n",
      "\n",
      "sim( machine learning method , deep learning method ) = 0.7298985719680786 \n",
      "\n",
      "sim( machine learning approach , deep learning method ) = 0.70999675989151 \n",
      "\n",
      "sim( machine learning approach , deep learning approach ) = 0.7096518278121948 \n",
      "\n",
      "sim( machine learning algorithm , deep learning method ) = 0.6696242690086365 \n",
      "\n",
      "sim( machine learning method , deep learning approach ) = 0.6425883173942566 \n",
      "\n",
      "sim( machine learning algorithm , deep learning approach ) = 0.6006295680999756 \n",
      "\n",
      "sim( machine learning method , ML method ) = 0.4235364496707916 \n",
      "\n",
      "sim( machine learning approach , ML method ) = 0.35687679052352905 \n",
      "\n",
      "sim( ML method , deep learning method ) = 0.32269972562789917 \n",
      "\n",
      "sim( machine learning algorithm , ML method ) = 0.322380006313324 \n",
      "\n",
      "sim( deep learning approach , ML method ) = 0.24583891034126282 \n",
      "\n",
      "sim( network security , deep learning approach ) = 0.17849472165107727 \n",
      "\n",
      "sim( network security , deep learning method ) = 0.17504242062568665 \n",
      "\n",
      "sim( network security , machine learning algorithm ) = 0.1524932086467743 \n",
      "\n",
      "sim( automated bright lesion detection , deep learning method ) = 0.1522764265537262 \n",
      "\n",
      "sim( automated bright lesion detection , machine learning method ) = 0.14655284583568573 \n",
      "\n",
      "sim( automated bright lesion detection , deep learning approach ) = 0.14523997902870178 \n",
      "\n",
      "sim( bright lesion detection , deep learning method ) = 0.14290976524353027 \n",
      "\n",
      "sim( network security , machine learning approach ) = 0.14073877036571503 \n",
      "\n",
      "sim( automated bright lesion detection , machine learning approach ) = 0.14069199562072754 \n",
      "\n",
      "sim( bright lesion detection , deep learning approach ) = 0.1339699923992157 \n",
      "\n",
      "sim( bright lesion detection , network security ) = 0.11660154163837433 \n",
      "\n",
      "sim( network security compromise , deep learning method ) = 0.09740902483463287 \n",
      "\n",
      "sim( network security compromise , deep learning approach ) = 0.09688057750463486 \n",
      "\n",
      "sim( bright lesion detection , machine learning method ) = 0.09362266957759857 \n",
      "\n",
      "sim( network security compromise , bright lesion detection ) = 0.07860258221626282 \n",
      "\n",
      "sim( network security compromise , machine learning algorithm ) = 0.07784058153629303 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, Mapper.entities, query_chunk_size=100, corpus_chunk_size=10000, batch_size=256, top_k=5, show_progress_bar=False)\n",
    "\n",
    "for paraphrase in paraphrases:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"sim( \"+Mapper.entities[i]+\" , \"+Mapper.entities[j]+\" ) =\", score, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e4feba-9ac7-4951-97e4-9a23dcb4f314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine learning approach': 'machine learning method',\n",
       " 'machine learning method': 'machine learning method',\n",
       " 'machine learning algorithm': 'machine learning method',\n",
       " 'deep learning approach': 'deep learning method',\n",
       " 'deep learning method': 'deep learning method'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mapper.label2cskg_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747f8a16-10dd-47a6-9b48-4a4488ec2c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get Entities and pairs\n",
      "\t>> Entities to be mapped: 75\n",
      "- \t >> Mapping with wikidata started\n",
      "> Mapped to Wikidata: 27\n",
      "- \t >> Mapping with dbpedia started\n"
     ]
    }
   ],
   "source": [
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/syntactic_cleaning/cleaned_triples.json\"\n",
    "output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/mapping/mapped_triples.json\"\n",
    " \n",
    "Mapper = EntitiesMapper(input_path,output_path )\n",
    "Mapper.run()\n",
    "# Mapper.linkThroughWikidata()\n",
    "# Mapper.linkThroughDbpedia()\n",
    "# # Mapper.load()\n",
    "\n",
    "# Mapper.mergeEntities()\n",
    "# Mapper.mergeEntitiesEuristic()Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4129b0-4c14-4118-a1da-fef4d08400c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computer network': 'computer network',\n",
       " 'security policy': 'security policy',\n",
       " 'network administrator': 'network administrator',\n",
       " 'security parameter': 'security parameter',\n",
       " 'epilepsy': 'epilepsy',\n",
       " 'electroencephalogram': 'neurological activity',\n",
       " 'neurological activity': 'neurological activity',\n",
       " 'human error': 'human error',\n",
       " 'classifier': 'classifier',\n",
       " 'formal language theory': 'formal language theory',\n",
       " 'computer science': 'computer science',\n",
       " 'software developer': 'software developer',\n",
       " 'network': 'computer network',\n",
       " 'asset': 'asset',\n",
       " 'vulnerability': 'vulnerability',\n",
       " 'attacker': 'attacker',\n",
       " 'security risk': 'security risk',\n",
       " 'method': 'method',\n",
       " 'paper': 'paper',\n",
       " 'result': 'result',\n",
       " 'aim': 'aim',\n",
       " 'study': 'study',\n",
       " 'experiment': 'experiment',\n",
       " 'thing': 'thing',\n",
       " 'tool': 'tool',\n",
       " '': '',\n",
       " 'introduction': 'introduction',\n",
       " 'we': 'we',\n",
       " 'theory': 'theory'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mapper.label2cskg_entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
