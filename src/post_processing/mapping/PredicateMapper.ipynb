{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623ea00f-68b2-4b1b-87f3-cd8d6c94e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin-user\\anaconda3\\envs\\mapping-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## read data -----> utilities\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents as a Python dictionary.\n",
    "\n",
    "    :param file_path: The path to the JSON file.\n",
    "    :type file_path: str\n",
    "    :return: A dictionary representing the JSON data.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "        \n",
    "def loadVerbMap(verb_map_path):\n",
    "    verb_info = pd.read_csv(verb_map_path, sep=',')\n",
    "    verb_map = {}\n",
    "    for i,r in verb_info.iterrows():\n",
    "        for j in range(34):\n",
    "            verb = r['v' + str(j)]\n",
    "            if str(verb) != 'nan':\n",
    "                verb_map[verb] = r['predicate']\n",
    "    return verb_map\n",
    "\n",
    "def encode_and_store(sentences, model, file_path):\n",
    "    # Encode sentences\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Create a dictionary with sentences as keys and their embeddings as values\n",
    "    embeddings_dict = {sentence: embedding.tolist() for sentence, embedding in zip(sentences, embeddings)}\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(embeddings_dict, file)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    # Load the embeddings from the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        embeddings_dict = json.load(file)\n",
    "\n",
    "    # Convert the embeddings from list to numpy array\n",
    "    embeddings_dict = {sentence: [float(value) for value in embedding] for sentence, embedding in embeddings_dict.items()}\n",
    "    return embeddings_dict\n",
    "\n",
    "# # Example sentences\n",
    "# sentences = list(verb_map.keys())\n",
    "\n",
    "# # Model initialization\n",
    "# model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# # File path to store and load embeddings\n",
    "\n",
    "# embd_path = 'C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/predicate_embeddings.json'\n",
    "\n",
    "# # Encode sentences, store in a file, and then load them\n",
    "# encode_and_store(sentences, model, embd_path)\n",
    "# loaded_embeddings = load_embeddings(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5b0c61-3d7f-46f3-8a44-77eddb71a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PredicateMapper_utilities\n",
    "\n",
    "class PredicateMapper:\n",
    "    def __init__(self, triples_path,verbs_path, embd_path):\n",
    "        self.predicate_embd = PredicateMapper_utilities.load_embeddings(embd_path)\n",
    "        self.verb_map = PredicateMapper_utilities.loadVerbMap(verbs_path)\n",
    "        self.input_triples = PredicateMapper_utilities.read_json_file(triples_path)\n",
    "        self.mapped_predicate = {}\n",
    "        \n",
    "    def similarity_mapping(self, predicate):\n",
    "       \n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "        embd_predicate = np.array(embd_predicate).reshape(1, -1)\n",
    "        \n",
    "        max_similarity = float('-inf')\n",
    "        max_element = None\n",
    "    \n",
    "        for k, v in self.predicate_embd.items():\n",
    "            embd_verb = np.array(v).reshape(1, -1)\n",
    "            similarity = cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "    \n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_element = {k: similarity}\n",
    "    \n",
    "        # print(max_element)\n",
    "        return max_element\n",
    "    \n",
    "\n",
    "    def get_last_pp(self, predicate):\n",
    "        ## return a tuple the predicate with out last pp and the pp\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def check_last_word_preposition(self, predicate):\n",
    "    # Liste des prépositions en anglais (à compléter si nécessaire)\n",
    "        prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"around\", \n",
    "                        \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"by\", \n",
    "                        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \n",
    "                        \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\", \"on\", \"onto\", \"out\", \"outside\", \"over\", \n",
    "                        \"past\", \"regarding\", \"round\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"under\", \n",
    "                        \"underneath\", \"until\", \"unto\", \"up\", \"upon\", \"with\", \"within\", \"without\"]\n",
    "    \n",
    "        words = predicate.split()\n",
    "        last_word = words[-1].lower()  # Convertir en minuscules pour la comparaison en anglais\n",
    "    \n",
    "        if last_word in prepositions:\n",
    "            # Le dernier mot est une préposition\n",
    "            sentence_without_preposition = ' '.join(words[:-1])\n",
    "            return (sentence_without_preposition, last_word)\n",
    "        else:\n",
    "            # Le dernier mot n'est pas une préposition\n",
    "            return (predicate,None)\n",
    "    \n",
    "    ## return the dic of mapped predicate\n",
    "        # threshold: the predicate mapping threshold\n",
    "        # option: keep or  delete for unmpapped predicate\n",
    "            ## keep: keep the as them (not mention them in the dict mapping)\n",
    "            ## delete: in the mapping dict add a message that indicate thist triple should be deleted (\"invalid triple\")\n",
    "    def predicate_mapping(self, threshold, option = \"keep\"):\n",
    "        for triple in tqdm(self.input_triples, desc=\"predicate mapping\"):\n",
    "            predicate = triple[\"predicate\"]\n",
    "            predicate_ = self.check_last_word_preposition(predicate)\n",
    "            if predicate_:\n",
    "                if predicate_[0] in self.verb_map.keys(): \n",
    "                    pp = \"\"\n",
    "                    if predicate_[1]:\n",
    "                        pp = \" \"+ predicate_[1]\n",
    "                    self.mapped_predicate[predicate] = self.verb_map[predicate_[0]] + pp\n",
    "                    # print(\"direct mapping:  \", predicate,\"-->\" ,self.verb_map[predicate_[0]] + pp )\n",
    "                else:\n",
    "                   \n",
    "                    max_sim_verb = self.similarity_mapping(predicate_[0])\n",
    "                    # print(max_sim_verb)\n",
    "                    if list(max_sim_verb.values())[0] >= threshold:\n",
    "                        pp = \"\"\n",
    "                        if predicate_[1]:\n",
    "                            pp = \" \"+ predicate_[1]\n",
    "                        self.mapped_predicate[predicate] = self.verb_map[list(max_sim_verb.keys())[0]] + pp\n",
    "                        # print(\"sim:\",max_sim_verb,\"\\n\")\n",
    "                        # print(\"similarity mapping:  \", predicate,\"-->\", self.verb_map[list(max_sim_verb.keys())[0]] + pp )\n",
    "                    else:\n",
    "                        ### keep or delete ! \n",
    "                        if option == \"delete\":\n",
    "                            self.mapped_predicate[predicate] = \"invalid triple\"\n",
    "                        if option == \"keep\":\n",
    "                            pass\n",
    "                        else:\n",
    "                            print(\"invalid option !\")\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c72e605-4393-419b-886c-139504fa3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/syntactic_cleaning/cleaned_triples.json\"\n",
    "verbs_map_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/CSKG_VerbNet_verb_map.csv\"\n",
    "embd_path = 'C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/predicate_embeddings.json'\n",
    "triples = PredicateMapper_utilities.read_json_file(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22a96293-fb42-4b6c-91b3-8d154f8a0628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc418f0d-1b57-471a-843d-b415ab1ca56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predicate mapping: 100%|██████████| 36/36 [01:13<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "pm = PredicateMapper(input_path,verbs_map_path,embd_path)\n",
    "pm.predicate_mapping(0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c79119-d8b3-4f2e-812f-e1cc939e6282",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpm\u001b[49m\u001b[38;5;241m.\u001b[39mmapped_predicate\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pm' is not defined"
     ]
    }
   ],
   "source": [
    "pm.mapped_predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15fb1e-4d15-4277-a556-5c30e1f48bba",
   "metadata": {},
   "source": [
    "#### similarity based mapping (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c196e17e-7457-4ce0-96d0-ecf1cf2550d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create': 0.8466176138921004}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PredicateMapper_utilities\n",
    "import numpy as np\n",
    "def similarity_mapping(predicate, predicate_embd):\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "    embd_predicate = np.array(embd_predicate).reshape(1, -1)\n",
    "\n",
    "    max_similarity = float('-inf')\n",
    "    max_element = None\n",
    "\n",
    "    for k, v in predicate_embd.items():\n",
    "        embd_verb = np.array(v).reshape(1, -1)\n",
    "        similarity = cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_element = {k: similarity}\n",
    "\n",
    "    # print(max_element)\n",
    "    return max_element\n",
    "# p ='are required to build'\n",
    "# p = 'have been repeated for'\n",
    "# p = 'do need do need to'\n",
    "# p = \"is creating\"\n",
    "# p = \"to dive deep\"\n",
    "# p = 'will use'\n",
    "p = 'is created'\n",
    "predicate_embd = PredicateMapper_utilities.load_embeddings(embd_path)\n",
    "similarity_mapping(p, predicate_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f209f6db-e225-4cd7-b9fd-4b441675d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_map = PredicateMapper_utilities.loadVerbMap(verbs_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d2d649-cca9-44d7-be50-eef973571522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produces'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_map['create']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "717e6d5b-ddfd-4655-bbb2-dad2b3634e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(ent):\n",
    "        validEntities = []\n",
    "\t\t# brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "        semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "        for e in ent:\t\t\n",
    "            valid = True\n",
    "            for synset in wn.synsets(e):\n",
    "                ic_value = semcor_ic['n'][synset.offset()]\n",
    "                if ic_value <= 4 and ic_value > 0:\n",
    "                    valid = False\n",
    "                    #print(e, 'NOT', ic_value)\n",
    "                    break\n",
    "                if valid:\n",
    "                    validEntities.append(e)\n",
    "        return validEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ff7c0e7e-327f-4091-b56c-f0a73e471dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'paper',\n",
       " 'paper',\n",
       " 'method',\n",
       " 'method',\n",
       " 'computer',\n",
       " 'computer',\n",
       " 'human',\n",
       " 'human',\n",
       " 'human',\n",
       " 'human']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent = [\"paper\", \"method\", \"computer\", \"human\"]\n",
    "validation(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efb78d-f826-4b16-a039-6b6b406e7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b25c9915-e97b-405b-b63c-22fbf7b98ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "302385c7-0988-4dd6-8cb8-9c050a9d27e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\admin-\n",
      "[nltk_data]     user\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2bbb4616-127e-4213-8871-242ef456445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wn.synsets('machine learning')\n",
    "valid = True\n",
    "for synset in synsets:\n",
    "        ic_value = semcor_ic['n'][synset.offset()]\n",
    "        # if ic_value <= 4 and ic_value > 0:\n",
    "        #         valid = False\n",
    "        #         #print(e, 'NOT', ic_value)\n",
    "        #         break\n",
    "        # if valid:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "402d0042-cc1d-46a4-8ed1-43a5c570f7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "866d82d5-5566-4c9c-be5f-4ecbb80876f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(self):\n",
    "\t\tbrown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\t\tsemcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\t\tfor e in self.inputEntities:\n",
    "\t\t\tif e in self.blacklist or len(e) <= 2 or e.isdigit() or e[0].isdigit() or len(nltk.word_tokenize(e)) >= 7:# # no blacklist, no 1-character entities, no only numbers, no entities that start with a number, no entities with more than 7 tokens\n",
    "\t\t\t\tcontinue\t\t\t\n",
    "  \n",
    "\t\t\tif e in self.csoTopics:\n",
    "\t\t\t\tself.validEntities.add(e)\n",
    "\t\t\telif e in self.magTopics:\n",
    "\t\t\t\tself.validEntities.add(e)\n",
    "\t\t\telse:\n",
    "\t\t\t\tvalid = True\n",
    "\t\t\t\tfor synset in wn.synsets(e):\n",
    "\t\t\t\t\tic_value = semcor_ic['n'][synset.offset()]\n",
    "\t\t\t\t\tif ic_value <= 4 and ic_value > 0:\n",
    "\t\t\t\t\t\tvalid = False\n",
    "\t\t\t\t\t\t#print(e, 'NOT', ic_value)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif valid:\n",
    "\t\t\t\t\tself.validEntities.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "54eb78f3-822c-493d-8e0d-3624bb9139a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "     ---------------------------------------- 0.0/250.0 kB ? eta -:--:--\n",
      "     ------ ------------------------------ 41.0/250.0 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 250.0/250.0 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7774f57a-7788-4abb-8db5-1d9cecdd148b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62eee513-0ef9-434c-ae02-dcad637bab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def is_passive(s,p,o):\n",
    "    # Unpack the triple into subject, predicate, and object\n",
    "    subject = s\n",
    "    predicate = p\n",
    "    obj = o\n",
    "    \n",
    "    # Define patterns for identifying passive voice\n",
    "    passive_patterns = [\n",
    "        re.compile(r\"\\b(?:by)\\b\", re.IGNORECASE),\n",
    "    ]\n",
    "    \n",
    "    # Check if any passive voice pattern is present in the predicate\n",
    "    is_passive = any(pattern.search(predicate) for pattern in passive_patterns)\n",
    "    \n",
    "    # Determine the voice based on the analysis\n",
    "    if is_passive:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def convert_to_active(s,p,o):\n",
    "  # Unpack the triple into subject, predicate, and object\n",
    "    subject = s\n",
    "    predicate = p\n",
    "    obj = o\n",
    "    \n",
    "    # subject, predicate, obj = passive_triple\n",
    "    predicate_without_by = re.sub(r\"\\b(?:by)\\b\", \"\", predicate, flags=re.IGNORECASE).strip()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_verb = lemmatizer.lemmatize(predicate_without_by, pos='v')\n",
    "    active_triple = (obj,lemmatized_verb, subject)\n",
    "    return active_triple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d10bf0d5-a868-42e1-8b9c-2b10e296e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79024fce-529f-4428-955e-20f0508891f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('attacker', 'exploit', 'vulnirability')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_active(triple_passive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7f385a-5fef-453d-9372-5816b48f9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detecter_patron(texte):\n",
    "    # Séparation du texte par \"-\"\n",
    "    parties = texte.split(\"-\")\n",
    "    \n",
    "    # Vérification si le texte a exactement deux parties et si les deux parties sont du texte\n",
    "    if len(parties) == 2 and all(partie.strip().isalpha() for partie in parties):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd0a3f3-ee7f-4d37-8786-7e5754330151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
