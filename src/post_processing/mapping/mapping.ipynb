{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5291f48b-b565-42a3-bc82-114201004060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import Entities_Mapper as em\n",
    "import json\n",
    "import PredicateMapper as pm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6910e8-08f4-4f05-a33f-3983bd3bfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping:\n",
    "    def __init__(self, input_path, verbs_map_path, embd_verbs_path, output_path):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.verbs_map_path = verbs_map_path\n",
    "        self.embd_verbs_path = embd_verbs_path\n",
    "        self.mapping_result = [] ## store the result\n",
    "\n",
    "\n",
    "\n",
    "    def is_passive(self, s , p , o ):\n",
    "        # Unpack the triple into subject, predicate, and object\n",
    "        subject = s\n",
    "        predicate = p\n",
    "        obj = o\n",
    "        \n",
    "        # Define patterns for identifying passive voice\n",
    "        passive_patterns = [\n",
    "            # re.compile(r\"\\b(?:am|is|are|was|were|been|being)\\b\", re.IGNORECASE),\n",
    "            re.compile(r\"\\b(?:-by)\\b\", re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        # Check if any passive voice pattern is present in the predicate\n",
    "        is_passive = any(pattern.search(predicate) for pattern in passive_patterns)\n",
    "        \n",
    "        # Determine the voice based on the analysis\n",
    "        if is_passive:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def convert_to_active(self,s,p,o):\n",
    "      # Unpack the triple into subject, predicate, and object\n",
    "        subject = s\n",
    "        predicate = p\n",
    "        obj = o\n",
    "        \n",
    "        # subject, predicate, obj = passive_triple\n",
    "        predicate_without_by = re.sub(r\"\\b(?:-by)\\b\", \"\", predicate, flags=re.IGNORECASE).strip()\n",
    "        \n",
    "        active_triple = (obj,predicate_without_by, subject)\n",
    "        return active_triple\n",
    "\n",
    "    def apply(self):\n",
    "        entities_mapper = em.EntitiesMapper(self.input_path, self.output_path )\n",
    "        entities_mapper.run()\n",
    "        predicate_mapper = pm.PredicateMapper(self.input_path, self.verbs_map_path,  self.embd_verbs_path)\n",
    "        # predicate_mapper.predicate_mapping(0.7)\n",
    "        for element in entities_mapper.input_triples:\n",
    "            subject = element[\"subject\"]\n",
    "            predicate = element[\"predicate\"]\n",
    "            object = element[\"object\"]\n",
    "            # if predicate in predicate_mapper.mapped_predicate.keys():\n",
    "            predicate = predicate_mapper.direct_mapping(predicate)\n",
    "            if subject in entities_mapper.label2cskg_entity.keys():\n",
    "                subject = entities_mapper.label2cskg_entity[subject]\n",
    "            if object in entities_mapper.label2cskg_entity.keys():\n",
    "                 object = entities_mapper.label2cskg_entity[object]\n",
    "\n",
    "            ## verify if active or passive and convert to active\n",
    "            if self.is_passive(subject, predicate, object):\n",
    "                subject, predicate, object = self.convert_to_active(subject, predicate, object)\n",
    "            ## lemma of predicates\n",
    "            # predicate = self.lemmatize_predicate(predicate)\n",
    "            \n",
    "            self.mapping_result.append(\n",
    "                {\n",
    "                        'sentence': element['sentence'],\n",
    "                        'subject': subject,\n",
    "                        'predicate': predicate,\n",
    "                        'object': object,\n",
    "                        'confidence': element['confidence'],\n",
    "                        'first_validation': element['first_validation']\n",
    "                    }\n",
    "                \n",
    "            )\n",
    "            for element in self.mapping_result:\n",
    "                element[\"subject\"] = self.supprimer_ponctuation(element[\"subject\"])\n",
    "                element[\"object\"] = self.supprimer_ponctuation(element[\"object\"])\n",
    "                element['first_validation'] = self.supprimer_ponctuation(element[\"first_validation\"])\n",
    "            self.handle_duplicates()\n",
    "            # self.handle_duplicates()\n",
    "\n",
    "    \n",
    "    \n",
    "    ### avec la semilarité des predicats !!!\n",
    "    def apply_sim_pred(self):\n",
    "        entities_mapper = em.EntitiesMapper(self.input_path, self.output_path )\n",
    "        entities_mapper.run()\n",
    "        predicate_mapper = pm.PredicateMapper(self.input_path, self.verbs_map_path,  self.embd_verbs_path)\n",
    "        predicate_mapper.similarity_predicate_mapping(0.7)\n",
    "        for element in entities_mapper.input_triples:\n",
    "            subject = element[\"subject\"]\n",
    "            predicate = element[\"predicate\"]\n",
    "            object = element[\"object\"]\n",
    "            if predicate in predicate_mapper.mapped_predicate.keys():\n",
    "                predicate = predicate_mapper.mapped_predicate[predicate]\n",
    "            if subject in entities_mapper.label2cskg_entity.keys():\n",
    "                subject = entities_mapper.label2cskg_entity[subject]\n",
    "            if object in entities_mapper.label2cskg_entity.keys():\n",
    "                 object = entities_mapper.label2cskg_entity[object]\n",
    "\n",
    "            ## verify if active or passive and convert to active\n",
    "            if self.is_passive(subject, predicate, object):\n",
    "                subject, predicate, object = self.convert_to_active(subject, predicate, object)\n",
    "            ## lemma of predicates\n",
    "            # predicate = self.lemmatize_predicate(predicate)\n",
    "            \n",
    "            self.mapping_result.append(\n",
    "                {\n",
    "                        'sentence': element['sentence'],\n",
    "                        'subject': subject,\n",
    "                        'predicate': predicate,\n",
    "                        'object': object,\n",
    "                        'confidence': element['confidence'],\n",
    "                        'first_validation': element[\"first_validation\"]\n",
    "                        \n",
    "                    }\n",
    "                \n",
    "            )\n",
    "        for element in self.mapping_result:\n",
    "            element[\"subject\"] = self.supprimer_ponctuation(element[\"subject\"])\n",
    "            element[\"object\"] = self.supprimer_ponctuation(element[\"object\"])\n",
    "            element['first_validation'] = self.supprimer_ponctuation(element[\"first_validation\"])\n",
    "        self.handle_duplicates()\n",
    "            \n",
    "        \n",
    "    def supprimer_ponctuation(self, phrase):\n",
    "        # Définition des caractères de ponctuation à supprimer\n",
    "        ponctuations = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
    "        # Filtrer les caractères pour ne garder que ceux qui ne sont pas des ponctuations\n",
    "        phrase_sans_ponctuation = ''.join(caractere for caractere in phrase if caractere not in ponctuations)\n",
    "        return phrase_sans_ponctuation \n",
    "        \n",
    "    def handle_duplicates(self):\n",
    "        # Create a dictionary to store unique sentences with their confidence\n",
    "        unique_entries = {}\n",
    "        for entry in self.mapping_result:\n",
    "            key = (entry['subject'], entry['predicate'], entry['object'])\n",
    "            if key in unique_entries:\n",
    "                # Vérifie si la phrase est déjà présente pour éviter la duplication\n",
    "                if entry['sentence'] not in unique_entries[key]['sentence']:\n",
    "                    unique_entries[key]['sentence'] += f\" | {entry['sentence']}\"\n",
    "                if entry['first_validation'] == 'True':\n",
    "                    unique_entries[key]['first_validation'] = 'True'\n",
    "            else:\n",
    "                unique_entries[key] = entry\n",
    "        self.mapping_result = list(unique_entries.values())\n",
    "        return list(unique_entries.values())\n",
    "       \n",
    "    def write_to_json(self):\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(self.mapping_result, jsonfile, ensure_ascii=False, indent=2)\n",
    "    def save_results(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e1c956-0f4f-4d34-a5b4-0a76fc8d62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get Entities and pairs\n",
      "\t>> Entities to be mapped: 412\n",
      "- \t >> Mapping with wikidata started\n",
      "\t >> Wikidata Processed 100 entities in 55.64 secs.\n",
      "\t >> Wikidata Processed 200 entities in 116.78 secs.\n",
      "\t >> Wikidata Processed 300 entities in 188.22 secs.\n",
      "\t >> Wikidata Processed 400 entities in 282.14 secs.\n",
      "> Mapped to Wikidata: 122\n",
      "- \t >> Mapping with dbpedia started\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/triplets_validator/Bench_merged_withoutGPT_v.json\"\n",
    "\n",
    "# ## Computer Science\n",
    "# input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/triplets_validator/Bench_merged_withGPT_v.json\"\n",
    "# output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/mapping/Bench_mapped_withtGP_v.json\"\n",
    "\n",
    "## Music\n",
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/triplets_validator//MusicBench_withGPT_v.json\"\n",
    "output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/mapping/Music_mapped_withGP_v.json\"\n",
    "\n",
    "verbs_map_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/CSKG_VerbNet_verb_map.csv\"\n",
    "embd_path = 'C:/Users/admin-user/Desktop/my_phd/implementations_KG/resources/predicate_embeddings.json'\n",
    "\n",
    "\n",
    "mapping = Mapping(input_path,verbs_map_path, embd_path ,output_path)\n",
    "mapping.apply()\n",
    "mapping.write_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1898ef04-32e6-4743-b4cb-c9da1c675686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping.mapping_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505ae06-98ac-4df8-935b-c2115a20dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_mapper = pm.PredicateMapper(input_path, verbs_map_path,  embd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725bbd4-a112-4cee-8ff9-29f1a5aee9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_mapper.direct_mapping(\"be use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c3887-ee89-4db7-a260-126e5829cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaaee260-3e94-45ae-90db-e48286204300",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "Constructing knowledge graphs (KGs) from textual data isessential for numerous applications, but presents significant challenges\n",
    "due to the complexity of natural language. Traditional approaches often\n",
    "rely on Open Information Extraction (OpenIE) for extracting domain-\n",
    "independent binary relations. However, they are prone to generating\n",
    "noisy data and incorrect triples. While Named Entity Recognition (NER)\n",
    "is commonly used to filter entities and enhance data quality, it can\n",
    "compromise the domain independence of these approaches and overlook\n",
    "crucial information. To address these limitations, we introduce a novel\n",
    "pipeline that aims to preserve the domain independence of KG construc-\n",
    "tion while reducing the prevalence of incorrect triples, thus offering a\n",
    "cost-effective solution without the need for domain-specific adaptations.\n",
    "The pipeline utilizes state-of-the-art OpenIE techniques combined with\n",
    "syntactic cleaning strategies focused on identifying noun phrases to mini-\n",
    "mize noise and isolate entities. Furthermore, it leverages Large Language\n",
    "Models (LLMs), specifically GPT-4, to verify the correctness and rele-\n",
    "vance of extracted triples in relation to the source text. the pipeline’s\n",
    "performance is evaluated using two gold standards in distinct domains\n",
    "(i.e. computer science and music), to assess its domain independence.\n",
    "Experimental results demonstrated its high recall compared to the re-\n",
    "call one of the state-of-the-art approaches for computer science KG con-\n",
    "struction, and its precision is notably enhanced through the incorpo-\n",
    "ration of LLMs. This highlights the potential of LLMs for validating\n",
    "extracted information. Moreover, experiments conducted on the music\n",
    "domain corpus showed that our approach maintains good performance\n",
    "across different fields, underscoring its versatility and effectiveness in\n",
    "domain-independent KG construction.\n",
    "\"\"\"\n",
    "s1 = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d886e9-4839-4d67-b666-56bb6e6e964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = \"\"\"\n",
    "Constructing knowledge graphs (KGs) from textual data is essential for numerous applications, but presents significant challenges due to the complexity of natural language. Traditional approaches often rely on Open Information Extraction (OpenIE) for extracting domain-independent binary relations. However, they are prone to generating noisy data and incorrect triples. While Named Entity Recognition (NER) is commonly used to filter entities and enhance data quality, it can compromise the domain independence of these approaches and overlook crucial information. To address these limitations, we introduce a novel pipeline that aims to preserve the domain independence of KG construction while reducing the prevalence of incorrect triples, thus offering a cost-effective solution without the need for domain-specific adaptations. \n",
    "The pipeline utilizes state-of-the-art OpenIE techniques combined with syntactic cleaning strategies focused on identifying noun phrases to minimize noise and isolate entities. Furthermore, it leverages Large Language Models (LLMs), specifically GPT-4, to verify the correctness and relevance of extracted triples in relation to the source text. \n",
    "the pipeline's performance is evaluated using two gold standards in distinct domains (i.e. computer science and music), to assess its domain independence. Experimental results demonstrated its high recall compared to the recall one of the state-of-the-art approaches for computer science KG construction, and its precision is notably enhanced through the incorporation of LLMs. This highlights the potential of LLMs for validating extracted information. Moreover, experiments conducted on the music domain corpus showed that our approach maintains good performance across different fields, underscoring its versatility and effectiveness in domain-independent KG construction.\n",
    "\n",
    "\"\"\"\n",
    "s2 = s2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c3234e-4994-4586-ad3b-7325cd5cfa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(s1,s2):\n",
    "    if i != j:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51b6b0a8-e030-4612-9173-461c91b1a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(data):\n",
    "        # Create a dictionary to store unique sentences with their confidence\n",
    "        unique_sentences = {}\n",
    "    \n",
    "        # Iterate through each element in the JSON\n",
    "        for item in data:\n",
    "            sentence_key = (item['subject'], item['predicate'], item['object'])\n",
    "            sentence_value = item['sentence']\n",
    "    \n",
    "            # Check if the sentence already exists in the dictionary\n",
    "            if sentence_key in unique_sentences:\n",
    "                # Check if the sentence is identical\n",
    "                if unique_sentences[sentence_key]['sentence'] != sentence_value:\n",
    "                    # Merge sentences by adding a semicolon\n",
    "                    unique_sentences[sentence_key]['sentence'] += \";\" + sentence_value\n",
    "            else:\n",
    "                unique_sentences[sentence_key] = {'sentence': sentence_value}\n",
    "    \n",
    "        # Convert the dictionary into a list of dictionaries for saving\n",
    "        unique_data = [{'subject': key[0], 'predicate': key[1], 'object': key[2], 'sentence': value['sentence'], 'first_validation': value['first_validation']} for key, value in unique_sentences.items()]\n",
    "        result = unique_data\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5a374c2-8b63-42be-b886-1ecefbd558f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../utilities')\n",
    "import utilities as u\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd0782e8-a01a-4b45-982c-1318f9e0b478",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'first_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mread_json_file(output_path)\n\u001b[1;32m----> 2\u001b[0m un \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m, in \u001b[0;36mhandle_duplicates\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     17\u001b[0m         unique_sentences[sentence_key] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: sentence_value}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert the dictionary into a list of dictionaries for saving\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m unique_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_validation\u001b[39m\u001b[38;5;124m'\u001b[39m: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_validation\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m unique_sentences\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     21\u001b[0m result \u001b[38;5;241m=\u001b[39m unique_data\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m         unique_sentences[sentence_key] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: sentence_value}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert the dictionary into a list of dictionaries for saving\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m unique_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m: key[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_validation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_validation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m} \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m unique_sentences\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     21\u001b[0m result \u001b[38;5;241m=\u001b[39m unique_data\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyError\u001b[0m: 'first_validation'"
     ]
    }
   ],
   "source": [
    "data = u.read_json_file(output_path)\n",
    "un = handle_duplicates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55ee093a-82ee-4c16-a0bc-5724bae8e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0727b55-b361-4fd0-8253-31834037bb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Cyber-physical attacks (CPAs) are classified as the major threatening of SGs security because Cyber-physical attacks (CPAs) may lead to severe consequences such as large blackout and destruction of infrastructures.',\n",
       "  'subject': 'large blackout',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'severe consequence',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Cyber-physical attacks (CPAs) are classified as the major threatening of SGs security because Cyber-physical attacks (CPAs) may lead to severe consequences such as large blackout and destruction of infrastructures.',\n",
       "  'subject': 'destruction',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'severe consequence',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware, software, and data sources.',\n",
       "  'subject': 'hardware,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware, software, and data sources.',\n",
       "  'subject': 'software,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware, software, and data sources.',\n",
       "  'subject': 'datum source',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware, software, and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'hardware,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware, software, and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'software,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware, software, and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'datum source',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'JSAV: the JavaScript AV development library goes beyond traditional AV library support for displaying standard data structures components, to provide functionality to simplify creation of AVs on many engagement levels including interactive exercises.',\n",
       "  'subject': 'interactive exercise',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'engagement level',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs.',\n",
       "  'subject': 'relational database,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs.',\n",
       "  'subject': 'spreadsheet,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs.',\n",
       "  'subject': 'xml,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs.',\n",
       "  'subject': 'json',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs.',\n",
       "  'subject': 'web api',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However, Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs rarely provide a semantic model to describe Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contents.',\n",
       "  'subject': 'relational database,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However, Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs rarely provide a semantic model to describe Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contents.',\n",
       "  'subject': 'spreadsheet,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However, Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs rarely provide a semantic model to describe Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contents.',\n",
       "  'subject': 'xml,',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However, Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs rarely provide a semantic model to describe Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contents.',\n",
       "  'subject': 'json',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However, Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs rarely provide a semantic model to describe Information sources such as relational databases, spreadsheets, XML, JSON, and Web APIs contents.',\n",
       "  'subject': 'web api',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'information source',\n",
       "  'confidence': '-',\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'A common way to learn is by studying written step-by-step tutorials such as worked examples .',\n",
       "  'subject': 'written step - by - step tutorial',\n",
       "  'predicate': 'be-learn',\n",
       "  'object': 'common way',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization .',\n",
       "  'subject': 'system',\n",
       "  'predicate': 'enable',\n",
       "  'object': 'people',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization .',\n",
       "  'subject': 'people',\n",
       "  'predicate': 'to-produce',\n",
       "  'object': 'visual coding tutorial',\n",
       "  'confidence': 0.03,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'system',\n",
       "  'predicate': 'enable-in',\n",
       "  'object': 'automatically - generated program',\n",
       "  'confidence': 0.76,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'people',\n",
       "  'predicate': 'to-produce',\n",
       "  'object': 'visual coding tutorial',\n",
       "  'confidence': 0.56,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'best annotation',\n",
       "  'predicate': 'produce-tutorial-vote-on',\n",
       "  'object': 'learner',\n",
       "  'confidence': 0.01,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': '4 expert judge',\n",
       "  'predicate': 'provide',\n",
       "  'object': '145 raw annotation',\n",
       "  'confidence': 0.96,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"code learner crowd 's annotation\",\n",
       "  'predicate': 'show',\n",
       "  'object': '4 expert judge',\n",
       "  'confidence': 0.25,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'experiment-with-show-to-be-include',\n",
       "  'object': '4 expert judge insight even expert',\n",
       "  'confidence': 0.09,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'to-be-include',\n",
       "  'object': 'insight even expert',\n",
       "  'confidence': 0.44,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': '101 learner',\n",
       "  'predicate': 'provide',\n",
       "  'object': '145 raw annotation',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': 'python',\n",
       "  'predicate': 'show',\n",
       "  'object': \"learner crowd 's annotation\",\n",
       "  'confidence': 0.85,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'show-to-be-include',\n",
       "  'object': 'insight even expert',\n",
       "  'confidence': 0.84,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'As the conventional power systems turn towards smart grids ( SGs ) on a fast pace , turn may create new and significant challenges to the existing electrical network security .',\n",
       "  'subject': 'conventional power system',\n",
       "  'predicate': 'turn',\n",
       "  'object': 'smart grid',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'include-emerge-with',\n",
       "  'object': 'feature of smart grid',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'to-be',\n",
       "  'object': 'critical issue',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Cyber-physical attacks ( CPAs ) are classified as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as large blackout and destruction of infrastructures .',\n",
       "  'subject': 'cyber - physical attack',\n",
       "  'predicate': 'guide-to',\n",
       "  'object': 'severe consequence',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.66,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'to-acquire',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.81,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start-to-acquire',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.72,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start-mitigate',\n",
       "  'object': 'type of attack',\n",
       "  'confidence': 0.07,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Thyristor-Controlled Braking Resistor ( TCBR ) can enable us to stabilize the target generator in a relatively short time .',\n",
       "  'subject': 'thyristor - controlled braking resistor',\n",
       "  'predicate': 'support-to-stabilize',\n",
       "  'object': 'target generator',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Background and objective : According to the World Health Organization ( WHO ) epilepsy affects approximately 45-50 million people .',\n",
       "  'subject': 'epilepsy',\n",
       "  'predicate': 'affect',\n",
       "  'object': '45 - 50 million people world',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Background and objective : According to the World Health Organization ( WHO ) epilepsy affects approximately 45-50 million people .',\n",
       "  'subject': 'epilepsy',\n",
       "  'predicate': 'affect',\n",
       "  'object': '45 - 50 million people world',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'neurological activity',\n",
       "  'predicate': 'acquire',\n",
       "  'object': 'neurological activity',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': '-True'},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'neurological activity',\n",
       "  'predicate': 'be-use-to-identify',\n",
       "  'object': 'epilepsy',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'neurological activity',\n",
       "  'predicate': 'to-identify',\n",
       "  'object': 'epilepsy',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error .',\n",
       "  'subject': 'visual inspection of eeg signal',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'time - consuming process',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error .',\n",
       "  'subject': 'visual inspection of eeg signal',\n",
       "  'predicate': 'guide-to',\n",
       "  'object': 'human error',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers .',\n",
       "  'subject': 'classification',\n",
       "  'predicate': 'be-execute-use',\n",
       "  'object': 'machine learning',\n",
       "  'confidence': 0.9,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The benchmark epilepsy EEG dataset provided by the University of Bonn is used in this study .',\n",
       "  'subject': 'university of bonn',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'benchmark epilepsy eeg dataset',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The classification performance is evaluated using 10 -fold cross validation .',\n",
       "  'subject': 'classification performance',\n",
       "  'predicate': 'use',\n",
       "  'object': '10 - fold cross validation',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'nearest neighbor',\n",
       "  'confidence': 0.97,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'vector machine',\n",
       "  'confidence': 0.96,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'decision tree',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'artificial neural network',\n",
       "  'confidence': 0.94,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Results : Neighbor Descriptive Pattern [ LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82 % and 99.80 % , respectively , for the classification between normal and epileptic EEG signals .',\n",
       "  'subject': 'neighbor descriptive pattern [',\n",
       "  'predicate': 'acquire',\n",
       "  'object': 'average classification accuracy of 99.82 % 99.80 %',\n",
       "  'confidence': 0.6,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Results : Neighbor Descriptive Pattern [ LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82 % and 99.80 % , respectively , for the classification between normal and epileptic EEG signals .',\n",
       "  'subject': 'ann classifier',\n",
       "  'predicate': 'acquire',\n",
       "  'object': 'average classification accuracy of 99.82 % 99.80 %',\n",
       "  'confidence': 0.61,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware , software , and data sources .',\n",
       "  'subject': 'computer network',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'asset',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'asset',\n",
       "  'predicate': 'include',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'be-use',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 0.97,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'violate',\n",
       "  'object': 'security policy',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'analyze',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'be-use',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'violate',\n",
       "  'object': 'security policy',\n",
       "  'confidence': 0.96,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'to-protect',\n",
       "  'object': 'computer network',\n",
       "  'confidence': 0.82,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'prioritize',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"So far , several security parameters are offered to analyze security risks from the network security administrator 's perspective .\",\n",
       "  'subject': 'security parameter',\n",
       "  'predicate': 'be-offer-to-analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 0.96,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"So far , several security parameters are offered to analyze security risks from the network security administrator 's perspective .\",\n",
       "  'subject': 'security parameter',\n",
       "  'predicate': 'to-analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Depending on the motivation of potential attackers , different attack path may be selected for network security compromise .',\n",
       "  'subject': 'attack path',\n",
       "  'predicate': 'be-acquire-for',\n",
       "  'object': 'network security compromise',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': \"So , attacker 's motivation is a key factor in predicting the attacker 's behavior .\",\n",
       "  'subject': \"attacker 's motivation\",\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'key factor',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"In this paper , the attacker 's motivation is considered in the process of security risk analysis , so network administrators are able to analyze security risks more accurately .\",\n",
       "  'subject': \"attacker 's motivation network administrator\",\n",
       "  'predicate': 'be-in',\n",
       "  'object': 'process of security risk analysis',\n",
       "  'confidence': 0.5,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': \"In this paper , the attacker 's motivation is considered in the process of security risk analysis , so network administrators are able to analyze security risks more accurately .\",\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'be-to-analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 0.4,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The proposed method is applied on a network and the results are compared with novel works in this area .',\n",
       "  'subject': 'proposed method',\n",
       "  'predicate': 'be-use-on',\n",
       "  'object': 'computer network',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'to-predict',\n",
       "  'object': 'behavior of attacker',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show',\n",
       "  'object': 'network administrator',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'to-use',\n",
       "  'object': 'countermeasure',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'will-be-to-use',\n",
       "  'object': 'countermeasure efficiently',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'data sharing',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'information exchange',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Semi-structured storage technology is easier to deploy and much more promising to promote in a wider range than all-structured methods .',\n",
       "  'subject': 'semi - structured storage technology',\n",
       "  'predicate': 'to-support-in',\n",
       "  'object': 'wider range',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggest',\n",
       "  'object': 'well current situation',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggest',\n",
       "  'object': 'combination of central data storage fit',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggest',\n",
       "  'object': 'combination of distributed data storage fit well current situation in china',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Data Structures and Algorithms are a central part of Computer Science .',\n",
       "  'subject': 'data structure',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'central part of computer science',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': 'Data Structures and Algorithms are a central part of Computer Science .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'central part of computer science',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Due to Data Structures and Algorithms abstract and dynamic nature , Data Structures and Algorithms are a difficult topic to learn for many students .',\n",
       "  'subject': 'data structure',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'difficult topic to learn for student',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Due to Data Structures and Algorithms abstract and dynamic nature , Data Structures and Algorithms are a difficult topic to learn for many students .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'difficult topic to learn for student',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .',\n",
       "  'subject': 'instructor',\n",
       "  'predicate': 'include-turn-to',\n",
       "  'object': 'algorithm visualization',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .',\n",
       "  'subject': 'instructor',\n",
       "  'predicate': 'include-turn-to',\n",
       "  'object': 'av system',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Until recently , most AV systems were Java-based systems .',\n",
       "  'subject': 'av system',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'java - based system',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'But , the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online .',\n",
       "  'subject': 'html5 content online',\n",
       "  'predicate': 'be-be-supplant',\n",
       "  'object': 'popularity of java',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'But , the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online .',\n",
       "  'subject': 'javascript content online',\n",
       "  'predicate': 'be-be-supplant',\n",
       "  'object': 'popularity of java',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'javascript av development library',\n",
       "  'predicate': 'based-on',\n",
       "  'object': 'content',\n",
       "  'confidence': 0.82,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'content',\n",
       "  'predicate': 'summarize',\n",
       "  'object': 'jsav',\n",
       "  'confidence': 0.72,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'content',\n",
       "  'predicate': 'support',\n",
       "  'object': 'three year of experience from using jsav',\n",
       "  'confidence': 0.35,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'javascript content',\n",
       "  'predicate': 'support',\n",
       "  'object': 'research result',\n",
       "  'confidence': 0.4,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'drop is because highly expressive axioms can not be detected from database schema alone , but instead require a combined analysis of the database schema and data .',\n",
       "  'subject': 'highly expressive axiom',\n",
       "  'predicate': 'require-combine',\n",
       "  'object': 'analysis of database schema',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'drop is because highly expressive axioms can not be detected from database schema alone , but instead require a combined analysis of the database schema and data .',\n",
       "  'subject': 'highly expressive axiom',\n",
       "  'predicate': 'require-combine',\n",
       "  'object': 'analysis of data',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Furthermore , we apply machine learning algorithms to help in ranking the resulting features based on user supplied relevance scores .',\n",
       "  'subject': 'feature',\n",
       "  'predicate': 'based-on',\n",
       "  'object': 'user supplied relevance score',\n",
       "  'confidence': 0.8,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Formal language theory plays , in computer science , a fundamental role that allows , among other things , the development of one of the cornerstones of information technology : programming languages .',\n",
       "  'subject': 'formal language theory',\n",
       "  'predicate': 'execute-in',\n",
       "  'object': 'computer science',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines .',\n",
       "  'subject': 'formal language theory',\n",
       "  'predicate': 'define',\n",
       "  'object': 'mandatory grammatical rule programmer',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines .',\n",
       "  'subject': 'mandatory grammatical rule programmer',\n",
       "  'predicate': 'to-follow-to-produce',\n",
       "  'object': 'tool',\n",
       "  'confidence': 0.84,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Despite Formal language theory significance , formal language theory is often taken for granted , even by software developers , who regularly follow the rules of their programming domain .',\n",
       "  'subject': 'software developer',\n",
       "  'predicate': 'follow',\n",
       "  'object': 'rule of programming domain',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'be-produce',\n",
       "  'object': 'developer programming language ,',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'will-to-dive',\n",
       "  'object': 'formal language theory',\n",
       "  'confidence': 0.91,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'to-dive',\n",
       "  'object': 'grammar',\n",
       "  'confidence': 0.94,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'be-describe',\n",
       "  'object': 'formal background of existing one',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'all of formal background',\n",
       "  'predicate': 'will-support-to-transform',\n",
       "  'object': 'theory',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'predefined',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 0.47,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm stream of text',\n",
       "  'predicate': 'acquire',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.21,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'acquire',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.26,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm stream of text',\n",
       "  'predicate': 'reject',\n",
       "  'object': 'stream of text comply',\n",
       "  'confidence': 0.27,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'reject',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.37,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'In the last two decades , we have seen an amazing development of image processing techniques targeted for medical applications .',\n",
       "  'subject': 'image processing technique',\n",
       "  'predicate': 'target-for',\n",
       "  'object': 'medical application',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'While in automated bright lesion detection in retinal fundus images , as segmentation was not used , shape-based object detection was the compute-intensive task identified .',\n",
       "  'subject': 'shape - based object detection',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'compute - intensive task',\n",
       "  'confidence': 0.01,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'accelerated method',\n",
       "  'predicate': 'execute-on',\n",
       "  'object': 'multi - gpu system',\n",
       "  'confidence': 0.97,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show-accelerate',\n",
       "  'object': 'average 265 time faster than original cpu version',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show-to-process-accelerate',\n",
       "  'object': '344 frame',\n",
       "  'confidence': 0.72,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'accelerated method',\n",
       "  'predicate': 'be-to',\n",
       "  'object': 'process 344 frame',\n",
       "  'confidence': 0.91,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Despite advances in equipment as well as methods , automatic face image processing for recognition or even just for the extraction of demographics , is still a challenging task in unrestricted scenarios .',\n",
       "  'subject': 'automatic face image processing',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'challenging task',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones .',\n",
       "  'subject': 'test',\n",
       "  'predicate': 'be-aim-at-include',\n",
       "  'object': 'extensive comparison of feature based approach',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones .',\n",
       "  'subject': 'test',\n",
       "  'predicate': 'at-include-out',\n",
       "  'object': 'extensive comparison of feature based approach with two score',\n",
       "  'confidence': 0.95,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'When using scores , different operators are applied in a completely separate way , so that each of different operators produces the corresponding scores .',\n",
       "  'subject': 'each of operator',\n",
       "  'predicate': 'produce',\n",
       "  'object': 'score',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Answers are then either fed to a SVM , or compared pairwise to exploit Likelihood Ratio .',\n",
       "  'subject': 'answer',\n",
       "  'predicate': 'feed-to',\n",
       "  'object': 'svm',\n",
       "  'confidence': 0.88,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'testbeds',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'ega database',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'ega database',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'balance',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'testbeds',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'gropus',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'include',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to-describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.91,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'json',\n",
       "  'predicate': 'to-describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.91,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'to-describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.71,\n",
       "  'first_validation': '-False'},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model of data source',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'implicit meaning of data',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model of data source',\n",
       "  'predicate': 'by-specify',\n",
       "  'object': 'concept',\n",
       "  'confidence': 0.71,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model data source',\n",
       "  'predicate': 'by-specify',\n",
       "  'object': 'relationship',\n",
       "  'confidence': 0.62,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Such models are the key ingredients to automatically publish the data into knowledge graphs .',\n",
       "  'subject': 'model',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'key ingredient to automatically publish data into knowledge graph',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Such models are the key ingredients to automatically publish the data into knowledge graphs .',\n",
       "  'subject': 'key ingredient',\n",
       "  'predicate': 'to-publish',\n",
       "  'object': 'data',\n",
       "  'confidence': 0.89,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Manually modeling the semantics of data sources requires significant effort and expertise , and although desirable , building Such models automatically is a challenging problem .',\n",
       "  'subject': 'desirable , building model',\n",
       "  'predicate': 'be-challenge',\n",
       "  'object': 'problem',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Most of the related work focuses on semantic annotation of the data fields ( source attributes ) .',\n",
       "  'subject': 'related work',\n",
       "  'predicate': 'focus-on',\n",
       "  'object': 'semantic annotation of data field',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'However , constructing a semantic model that explicitly describes the relationships between the attributes in addition to the attributes semantic types is critical .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'describe',\n",
       "  'object': 'relationship',\n",
       "  'confidence': 0.98,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'use',\n",
       "  'object': 'knowledge',\n",
       "  'confidence': 0.78,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'use',\n",
       "  'object': 'semantic model of previously modeled source',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'rich semantic model',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'semantics of source',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'domain ontology',\n",
       "  'predicate': 'define',\n",
       "  'object': 'concept',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'domain ontology',\n",
       "  'predicate': 'define',\n",
       "  'object': 'relationship',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Given some sample data from a new source , We leverage the knowledge in a domain ontology and the known semantic models to construct a weighted graph that represents the space of plausible semantic models for a new source .',\n",
       "  'subject': 'weighted graph',\n",
       "  'predicate': 'is-a',\n",
       "  'object': 'space of plausible semantic model for source',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'use',\n",
       "  'object': 'knowledge',\n",
       "  'confidence': 0.65,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'knowledge-from-domain-ontology-to-learn',\n",
       "  'object': 'rich semantic model',\n",
       "  'confidence': 0.04,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'use',\n",
       "  'object': 'semantic model of previously modeled source',\n",
       "  'confidence': 0.53,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'produce-learn',\n",
       "  'object': 'previously source rich semantic model',\n",
       "  'confidence': 0.02,\n",
       "  'first_validation': 'False'},\n",
       " {'sentence': 'Information technology has been contributing to various areas of knowledge ; in particular , the field of education stands out .',\n",
       "  'subject': 'information technology',\n",
       "  'predicate': 'contributes-to',\n",
       "  'object': 'various area of knowledge',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'In what concerns the teaching of computer programming , literature contains important efforts that aim to assist in the learning process .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'aim-to-support',\n",
       "  'object': 'learning process',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'In what concerns the teaching of computer programming , literature contains important efforts that aim to assist in the learning process .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'aim-to-support-in-learn',\n",
       "  'object': 'process',\n",
       "  'confidence': 0.99,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'include',\n",
       "  'object': 'difficulty',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'include-in-understand',\n",
       "  'object': 'problem logic',\n",
       "  'confidence': 0.63,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'include-in-abstract',\n",
       "  'object': 'problem logic',\n",
       "  'confidence': 0.62,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'An alternative that has contributed to the teaching-learning process is the use of Learning Objects ( LO ) , which contribute towards mediating and enhancing the teaching-learning process .',\n",
       "  'subject': 'alternative',\n",
       "  'predicate': 'be-use',\n",
       "  'object': 'learning object',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': \"One of the great difficulties of learning during the initial semesters of Engineering and Computer Science courses is related to the contents of computer programming , which increases the students ' failure level and also the dropout rate of such courses .\",\n",
       "  'subject': 'computer programming',\n",
       "  'predicate': 'affect',\n",
       "  'object': \"student ' failure\",\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'In order to decrease those rates , we have developed a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses , and the results were very positive .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'include-produce-learn-object-to-support-teach',\n",
       "  'object': 'concept',\n",
       "  'confidence': 0.11,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process .',\n",
       "  'subject': 'outcome data',\n",
       "  'predicate': 'show-use',\n",
       "  'object': 'learning object',\n",
       "  'confidence': 1.0,\n",
       "  'first_validation': 'True'},\n",
       " {'sentence': 'a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'to-produce',\n",
       "  'object': 'various learning object to help',\n",
       "  'confidence': 0.86,\n",
       "  'first_validation': 'False'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da1e1d19-963b-4808-bbb8-d35d44ab1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_ponctuation(phrase):\n",
    "    # Définition des caractères de ponctuation à supprimer\n",
    "    ponctuations = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
    "    \n",
    "    # Filtrer les caractères pour ne garder que ceux qui ne sont pas des ponctuations\n",
    "    phrase_sans_ponctuation = ''.join(caractere for caractere in phrase if caractere not in ponctuations)\n",
    "    \n",
    "    return phrase_sans_ponctuation\n",
    "def remove_duplicates_with_sentence_check(entries):\n",
    "    unique_entries = {}\n",
    "    for entry in entries:\n",
    "        key = (entry['subject'], entry['predicate'], entry['object'])\n",
    "        if key in unique_entries:\n",
    "            # Vérifie si la phrase est déjà présente pour éviter la duplication\n",
    "            if entry['sentence'] not in unique_entries[key]['sentence']:\n",
    "                unique_entries[key]['sentence'] += f\" | {entry['sentence']}\"\n",
    "            if entry['first_validation'] == 'True':\n",
    "                unique_entries[key]['first_validation'] = 'True'\n",
    "        else:\n",
    "            unique_entries[key] = entry\n",
    "    return list(unique_entries.values())\n",
    "\n",
    "result = remove_duplicates_with_sentence_check(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20d1069d-3ef9-4cfb-9f2a-fdd0f5ac9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76919efa-25af-4238-8b4c-cdca9327d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_ponctuation(phrase):\n",
    "    # Définition des caractères de ponctuation à supprimer\n",
    "    ponctuations = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
    "    \n",
    "    # Filtrer les caractères pour ne garder que ceux qui ne sont pas des ponctuations\n",
    "    phrase_sans_ponctuation = ''.join(caractere for caractere in phrase if caractere not in ponctuations)\n",
    "    \n",
    "    return phrase_sans_ponctuation\n",
    "\n",
    "\n",
    "for i in data:\n",
    "    i[\"subject\"] = supprimer_ponctuation(i[\"subject\"])\n",
    "    i[\"object\"] =supprimer_ponctuation(i[\"object\"])\n",
    "    i['first_validation'] = supprimer_ponctuation(i[\"first_validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e506ffdb-3d69-4c95-968d-676835afc8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relational database'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supprimer_ponctuation(\"relational database,\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
