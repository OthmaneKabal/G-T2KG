{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4bead7-680c-4ad9-8352-5718d5cda2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin-user\\anaconda3\\envs\\mapping-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PredicateMapper_utilities\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class PredicateMapper:\n",
    "    def __init__(self, triples_path,verbs_path, embd_path):\n",
    "        self.predicate_embd = PredicateMapper_utilities.load_embeddings(embd_path)\n",
    "        self.verb_map = PredicateMapper_utilities.loadVerbMap(verbs_path)\n",
    "        self.input_triples = PredicateMapper_utilities.read_json_file(triples_path)\n",
    "        self.mapped_predicate = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    def similarity_mapping(self, predicate):\n",
    "       \n",
    "        model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        embd_predicate = model.encode(predicate, convert_to_tensor=True)\n",
    "        embd_predicate = np.array(embd_predicate).reshape(1, -1)    \n",
    "        max_similarity = float('-inf')\n",
    "        max_element = None\n",
    "        for k, v in self.predicate_embd.items():\n",
    "            embd_verb = np.array(v).reshape(1, -1)\n",
    "            similarity = cosine_similarity(embd_predicate, embd_verb)[0, 0]\n",
    "    \n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                max_element = {k: similarity}\n",
    "    \n",
    "        # print(max_element)\n",
    "        return max_element\n",
    "    \n",
    "\n",
    "    def get_last_pp(self, predicate):\n",
    "        ## return a tuple the predicate with out last pp and the pp\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def check_last_word_preposition(self, predicate):\n",
    "    # Liste des prépositions en anglais (à compléter si nécessaire)\n",
    "        prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"around\", \n",
    "                        \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"by\", \n",
    "                        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \n",
    "                        \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\", \"on\", \"onto\", \"out\", \"outside\", \"over\", \n",
    "                        \"past\", \"regarding\", \"round\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"under\", \n",
    "                        \"underneath\", \"until\", \"unto\", \"up\", \"upon\", \"with\", \"within\", \"without\"]\n",
    "    \n",
    "        words = predicate.split()\n",
    "        last_word = words[-1].lower()  # Convertir en minuscules pour la comparaison en anglais\n",
    "    \n",
    "        if last_word in prepositions:\n",
    "            # Le dernier mot est une préposition\n",
    "            sentence_without_preposition = ' '.join(words[:-1])\n",
    "            return (sentence_without_preposition, last_word)\n",
    "        else:\n",
    "            # Le dernier mot n'est pas une préposition\n",
    "            return (predicate,None)\n",
    "\n",
    "\n",
    "    def lemmatize_predicate(self,p):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_verb = lemmatizer.lemmatize(p, pos='v')\n",
    "            return lemmatized_verb\n",
    "    \n",
    "    ## return the dic of mapped predicate\n",
    "        # threshold: the predicate mapping threshold\n",
    "        # option: keep or  delete for unmpapped predicate\n",
    "            ## keep: keep the as them (not mention them in the dict mapping)\n",
    "            ## delete: in the mapping dict add a message that indicate thist triple should be deleted (\"invalid triple\")\n",
    "    def predicate_mapping(self, threshold, option = \"keep\"):\n",
    "        for triple in tqdm(self.input_triples, desc=\"predicate mapping\"):\n",
    "            predicate =  self.lemmatize_predicate(triple[\"predicate\"])\n",
    "            predicate_ = self.check_last_word_preposition(predicate)\n",
    "            if predicate_:\n",
    "                if predicate_[0] in self.verb_map.keys(): \n",
    "                    pp = \"\"\n",
    "                    if predicate_[1]:\n",
    "                        pp = \" \"+ predicate_[1]\n",
    "                    self.mapped_predicate[predicate] = self.lemmatize_predicate(self.verb_map[predicate_[0]]) + pp\n",
    "                    # print(\"direct mapping:  \", predicate,\"-->\" ,self.verb_map[predicate_[0]] + pp )\n",
    "                else:\n",
    "                   \n",
    "                    max_sim_verb = self.similarity_mapping(predicate_[0])\n",
    "                    # print(max_sim_verb)\n",
    "                    if list(max_sim_verb.values())[0] >= threshold:\n",
    "                        pp = \"\"\n",
    "                        if predicate_[1]:\n",
    "                            pp = \" \"+ predicate_[1]\n",
    "                        self.mapped_predicate[predicate] =  self.lemmatize_predicate(self.verb_map[list(max_sim_verb.keys())[0]]) + pp\n",
    "                        # print(\"sim:\",max_sim_verb,\"\\n\")\n",
    "                        # print(\"similarity mapping:  \", predicate,\"-->\", self.verb_map[list(max_sim_verb.keys())[0]] + pp )\n",
    "                    else:\n",
    "                        ### keep or delete ! \n",
    "                        if option == \"delete\":\n",
    "                            self.mapped_predicate[predicate] = \"invalid triple\"\n",
    "                        if option == \"keep\":\n",
    "                            pass\n",
    "                        else:\n",
    "                            print(\"invalid option !\")\n",
    "                            break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6932289a-16b6-4218-bcdb-0ab2e05586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_predicate(p):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_verb = lemmatizer.lemmatize(p, pos='v')\n",
    "            return lemmatized_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171d242c-59d7-46c3-943a-cb47943d2333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65ae9e-4779-43c6-bb01-ceaf44ac6fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
