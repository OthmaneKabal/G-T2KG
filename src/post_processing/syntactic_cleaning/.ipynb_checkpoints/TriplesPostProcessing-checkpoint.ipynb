{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ae782b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate post_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5353c1-3046-46f8-9890-2a245c6666fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin-user\\anaconda3\\envs\\post_processing\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "from nltk import Tree\n",
    "import re\n",
    "import stanza\n",
    "import spacy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import pos_tag\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import logging\n",
    "# logger = logging.getLogger('stanza')\n",
    "# logger.setLevel(logging.ERROR)\n",
    "# Définir le niveau de journalisation pour ignorer les messages INFO et WARNING\n",
    "# logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80370e2a-c27f-4148-ad1a-821c95a2ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data -----> utilities\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents as a Python dictionary.\n",
    "\n",
    "    :param file_path: The path to the JSON file.\n",
    "    :type file_path: str\n",
    "    :return: A dictionary representing the JSON data.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "\n",
    "## for read the general stopwords by domain ;) \n",
    "def get_general_words(file_path):\n",
    "    \"\"\"\n",
    "    Read lines from a text file and return them as a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list containing the lines from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        strip_lines = [line.strip() for line in lines]\n",
    "        return strip_lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e96457b-4ede-4178-8369-6dcfda251edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###post_traitment utilities\n",
    "class ConstituentNode:\n",
    "    def __init__(self, label, children=None):\n",
    "        self.label = label\n",
    "        self.children = children or []\n",
    "\n",
    "\n",
    "def first_most_deepNP(node):\n",
    "  ### return le NP le plus profond\n",
    "    if node is not None:\n",
    "        if node.label == \"NP\":\n",
    "            # Retrieve the subtree with the root \"NP\"\n",
    "            result = tree_to_string(node)\n",
    "\n",
    "            # Initialize the variable to store the recursively found NP child subtree\n",
    "            np_child_subtree = None\n",
    "\n",
    "            # Check if the NP node has a direct child that is also an NP node\n",
    "            for child in node.children:\n",
    "                if child.label == \"NP\":\n",
    "                    # Retrieve the subtree of the NP child recursively\n",
    "                    np_child_subtree = first_most_deepNP(child)\n",
    "\n",
    "            # Return the NP child subtree if it exists, otherwise return the current subtree\n",
    "            return np_child_subtree if np_child_subtree is not None else result\n",
    "\n",
    "        # If the node is not \"NP\", continue the traversal\n",
    "        for child in node.children:\n",
    "            result = first_most_deepNP(child)\n",
    "            if result is not None:\n",
    "                return result\n",
    "\n",
    "\n",
    "\n",
    "def tree_to_string(node, level=0):\n",
    "  # return a tree as a string \n",
    "\n",
    "    if not node.children:  # Check if the node is a leaf\n",
    "        return \" \" * level + f\"{node.label}\"\n",
    "\n",
    "    result = \" \" * level + f\"({node.label}\"\n",
    "    for child in node.children:\n",
    "        result += tree_to_string(child, level + 1)\n",
    "    result += \" \" * level + \")\"\n",
    "    return result\n",
    "\n",
    "def build_tree_from_string(s):\n",
    "    if s:\n",
    "      return Tree.fromstring(s)\n",
    "    else:\n",
    "      return\n",
    "\n",
    "def get_text_from_tree(tree):\n",
    "  if tree:\n",
    "    leaves = tree.leaves()\n",
    "    return \" \".join(leaves)\n",
    "##############################################################################################################################################\n",
    "\n",
    "## verify if the subject and the object do not exceed cerntain lenght\n",
    "def valid_lenght(string, max_nb_words):\n",
    "    words = string.split()\n",
    "    return len(words) <= max_nb_words\n",
    "\n",
    "## !!!!!!\n",
    "def lemmatize_and_lowercase(sentence,nlp):\n",
    "    # Analysez la phrase avec spaCy\n",
    "    doc = nlp(sentence)\n",
    "    # Lemmatisez chaque mot et convertissez-le en minuscules\n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
    "    # Rejoignez les mots lemmatisés pour former la phrase résultante\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def lemmatize_Nouns_and_lowercase(sentence, nlp):\n",
    "    # Analysez la phrase avec spaCy\n",
    "    doc = nlp(sentence)\n",
    "    # Lemmatisez chaque mot qui est un nom (Noun) et convertissez-le en minuscules\n",
    "    lemmatized_tokens = [token.lemma_.lower() if token.pos_ == 'NOUN' else token.text.lower() for token in doc]\n",
    "    # Rejoignez les mots lemmatisés pour former la phrase résultante\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    print(lemmatized_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "## delete adj  & adv for predicate\n",
    "def delete_adj_adv(s,nlp):\n",
    "    doc = nlp(s)\n",
    "    cleaned_s = ' '.join(token.text for token in doc if token.pos_ not in ['ADJ', 'ADV'])\n",
    "    # predicate_stpW_removal(cleaned_predicate)\n",
    "    return cleaned_s\n",
    "\n",
    "## delete first \"to\" (predicate)\n",
    "def delete_first_TO(s):\n",
    "    if not s:\n",
    "        return\n",
    "    tokens = s.split()\n",
    "    if tokens[0] == \"to\":\n",
    "        return (\" \".join(tokens[1:])).strip()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to WordNet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_predicate(predicate):\n",
    "    tokens = word_tokenize(predicate)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for word, tag in tagged_tokens:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            # If no WordNet POS tag is found, lemmatize as a noun by default.\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def lemmatize_onlyNouns_and_lowercase_stanza(sentence,nlp_stanza):\n",
    "  \n",
    "    doc = nlp_stanza(sentence)\n",
    "    Nouns_lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatiser chaque mot qui est un nom (Noun) et convertissez-le en minuscules\n",
    "    lemmatized_tokens = [Nouns_lemmatizer.lemmatize(word.text.lower(),\"n\") if word.pos == 'NOUN' else word.text.lower() for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    # Rejoignez les mots lemmatisés pour former la phrase résultante\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "# # #### leamm_wordnet\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "#     if treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     return None\n",
    "\n",
    "# def lemmatize_onlyNouns_and_lowercase_wordNet(sentence):\n",
    "#     tokens = word_tokenize(sentence)\n",
    "#     tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "#     Nouns_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_Nouns = [Nouns_lemmatizer.lemmatize(word.lower(), pos=get_wordnet_pos(pos_tag)) if get_wordnet_pos(pos_tag) else word.lower() for word, pos_tag in tagged_tokens]\n",
    "    \n",
    "#     lemmatized_sentence = ' '.join(lemmatized_Nouns)\n",
    "    \n",
    "#     return lemmatized_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_negation(text):\n",
    "    # Detects the presence of negation in a given English text.\n",
    "\n",
    "    # Args:\n",
    "    # - text (str): The input text in English.\n",
    "\n",
    "    # Returns:\n",
    "    # - bool: True if negation is detected, False otherwise.\n",
    "\n",
    "    negation_words = [\"not\", \"no\", \"never\", \"none\", \"nobody\", \"nowhere\", \"nothing\", \"neither\",\"nor\", \"hardly\", \"scarcely\", \"barely\",\n",
    "                      \"doesn't\", \"isn't\", \"wasn't\", \"hasn't\", \"can't\",\n",
    "                      \"won't\", \"couldn't\", \"wouldn't\", \"shouldn't\", \"didn't\", \"doesn't\", \"won't\", \"can't\", \"isn't\", \"haven't\", \"aren't\"]\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive comparison\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check if any negation word is present in the text\n",
    "    for word in negation_words:\n",
    "        if word in text_lower:\n",
    "            return True\n",
    "    # If no negation word is found, return False\n",
    "        return False\n",
    "\n",
    "\n",
    "## return the first word of a string (used for predicate rectification)\n",
    "def get_first_word(string):\n",
    "    words = string.split()\n",
    "    if words:\n",
    "        return words[0]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "## pour extraire les noms qui suivent directement la \"Head\"\n",
    "def extract_noun_follow_head_direct(input_sentence,nlp_spacy):\n",
    "    doc = nlp_spacy(input_sentence)\n",
    "    parts = []\n",
    "    # Ajouter les noms successifs au début de la phrase\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            parts.append(token.text)\n",
    "        else:\n",
    "            break\n",
    "    result = \" \".join(parts)\n",
    "    return result\n",
    "\n",
    "## Extrare jusqu'a le root  +  les nons qui suivent directement le Root\n",
    "def only_head_Noun(sentence, nlp_spacy):\n",
    "    doc = nlp_spacy(sentence)\n",
    "    # Recherchez le nœud racine de l'arbre de dépendances\n",
    "    root = next(token for token in doc if token.head == token)\n",
    "\n",
    "    # Extraire la partie principale de la phrase jusqu'au nœud racine\n",
    "    main_part_1 = sentence[:root.idx + len(root.text)]\n",
    "    main_part_2 = sentence[root.idx + len(root.text):].strip()\n",
    "    print(main_part_2)\n",
    "\n",
    "    main_part_2 = extract_noun_follow_head_direct(main_part_2,nlp)\n",
    "    main_part = main_part_1 +\" \"+ main_part_2\n",
    "    return main_part\n",
    "\n",
    "\n",
    "\n",
    "def is_verb_with_stanza(word,nlp_stanza):\n",
    "    doc = nlp_stanza(word)\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos == 'VERB':\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Fonction  pour extraire tous les NPs\n",
    "def extract_all_nps(tree):\n",
    "    nps = []  # Liste pour stocker tous les groupes nominaux trouvés\n",
    "\n",
    "    def recurse(node):\n",
    "        if node.label == 'NP':  # Si le nœud est un groupe nominal\n",
    "            nps.append(node)  # Ajoutez ce nœud à la liste des NPs\n",
    "        for child in node.children:  # Parcourir les enfants du nœud récursivement\n",
    "            recurse(child)\n",
    "\n",
    "    recurse(tree)  # Commencer la recherche récursive\n",
    "    return nps\n",
    "\n",
    "##options:[\"SBAR\",(\"PP\"or \"PP-of\")]\n",
    "## SBAR: supprimer SBAR; PP: supprimer: PP, PP-of: supprimer PP sauf ceux  qui contiennent \"of\"\n",
    "def get_NP_Head(node, word, options = [\"SBAR\",\"PP-of\", \"VP\",\"S\"],brother = None, nlp_stanza = None):\n",
    "    current_np = None\n",
    "    current_brother = None\n",
    "    # if not nlp_stanza:\n",
    "    #     nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency,depparse,lemma')\n",
    "    if node is not None:\n",
    "        # if node.label == \"NN\" and len(node.leaf_labels()) == 1 and word in node.leaf_labels():\n",
    "        #   return node, None\n",
    "        # if node.label == \"NNP\" and word in node.leaf_labels():\n",
    "          # return node, None\n",
    "        if is_verb_with_stanza(word, nlp_stanza):\n",
    "              nps = extract_all_nps(node)\n",
    "              NP = None\n",
    "              for np in nps:\n",
    "                  if word not in np.leaf_labels() and node.leaf_labels().index(word) > node.leaf_labels().index(np.leaf_labels()[-1]):\n",
    "                      NP = np\n",
    "              return NP,None\n",
    "        if node.label == \"NP\" and word in node.leaf_labels():\n",
    "            if is_verb_with_stanza(word, nlp_stanza):\n",
    "              nps = extract_all_nps(node)\n",
    "              NP = None\n",
    "              for np in nps:\n",
    "                  if word not in np.leaf_labels() and node.leaf_labels().index(word) > node.leaf_labels().index(np.leaf_labels()[-1]):\n",
    "                      NP = np\n",
    "              return NP,None\n",
    "            # current_np = tree_to_string(node)\n",
    "            current_np = node\n",
    "            if brother:\n",
    "              current_brother =  brother\n",
    "              # current_brother =  tree_to_string(brother)\n",
    "              for option in options:\n",
    "                if option ==\"PP-of\" and brother.label == \"PP\":\n",
    "                  if brother.leaf_labels()[0] != \"of\":\n",
    "                    current_brother = None\n",
    "                else:\n",
    "                  if brother.label == option:\n",
    "                    current_brother = None\n",
    "\n",
    "        for num_cild,child in enumerate(node.children):\n",
    "            if num_cild >= len(node.children)-1:\n",
    "                brother = None\n",
    "            else:\n",
    "                brother = node.children[num_cild+1]\n",
    "\n",
    "            result,bro = get_NP_Head(child,word,options,brother, nlp_stanza)\n",
    "            if result is not None:\n",
    "                current_np = result\n",
    "                current_brother = bro\n",
    "        return current_np, current_brother\n",
    "        \n",
    "## stanford\n",
    "def get_root_word_stanford(sentence, nlp_stanford):\n",
    "    doc = nlp_stanford(sentence)\n",
    "    return next((word.text for sent in doc.sentences for word in sent.words if word.deprel == \"root\"), None)\n",
    "### spacy\n",
    "def get_root_word(sentence,nlp_spacy):\n",
    "    doc = nlp_spacy(sentence)\n",
    "    # Recherchez le nœud racine de l'arbre de dépendances\n",
    "    root = next(token for token in doc if token.head == token)\n",
    "    return root.text\n",
    "\n",
    "def is_word_count_less_than(sentence, l):\n",
    "    \"\"\"\n",
    "    Check if the number of words in a sentence is less than a given length.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The sentence to check.\n",
    "    l (int): The length to compare the word count against.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the number of words is less than l, False otherwise.\n",
    "    \"\"\"\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Count the words and compare with l\n",
    "    return len(words) < l\n",
    "#-------------------------------clean by syntactic rules -----------------------------\n",
    "# This function encapsulates all treatment for extracting NP if it exists (option)\n",
    "## sentence: la phrase (sujet ou l'objet ) pour laquelle on extrait le NP\n",
    "## NP method, choisir la methode ou le critere pour extraire le NP il y'a trois methodes possible:\n",
    "    ### NP+head:\n",
    "          ####options:[\"SBAR\",(\"PP\"or \"PP-of\")]\n",
    "              ## SBAR: supprimer SBAR; PP: supprimer: PP, PP-of: supprimer PP sauf ceux  qui contiennent \"of\"\n",
    "    ### only_head:\n",
    "    ###  first_most_DeepNP:\n",
    "def Extract_NP(sentence, nlp_stanza, nlp_spacy, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"]):\n",
    "    doc = nlp_stanza(sentence)\n",
    "    constituency_tree = doc.sentences[0].constituency\n",
    "    final_result = None\n",
    "    if NP_method == \"first_most_DeepNP\":\n",
    "      extracted_Np = first_most_deepNP(constituency_tree)\n",
    "      tree_obj = build_tree_from_string(extracted_Np)\n",
    "      final_result = get_text_from_tree(tree_obj)\n",
    "    elif NP_method ==\"only_head\":\n",
    "      final_result = only_head_Noun(sentence, nlp_spacy)\n",
    "    elif NP_method ==\"NP+Head\":\n",
    "      #root_word = get_root_word(sentence, nlp_spacy)\n",
    "      root_word = get_root_word_stanford(sentence,nlp_stanza)\n",
    "      NP_head, brother_tree = get_NP_Head(constituency_tree,root_word,option)\n",
    "        \n",
    "      if NP_head:\n",
    "        NP_head =  tree_to_string(NP_head)\n",
    "        NP_tree_obj = build_tree_from_string(NP_head)\n",
    "        extracted_Np = get_text_from_tree(NP_tree_obj)\n",
    "        extracted_brother =\"\"\n",
    "        if brother_tree:\n",
    "          # brother_tree, br = get_NP_Head(brother_tree.children[1],\"attackers\",[\"SBAR\",\"PP-of\"])\n",
    "          brother_tree = tree_to_string(brother_tree)\n",
    "          \n",
    "          brother_tree_obj = build_tree_from_string(brother_tree)\n",
    "          extracted_brother = get_text_from_tree(brother_tree_obj)\n",
    "          brother_tree, br = get_NP_Head(brother_tree.children[1],get_root_word_stanford(extracted_brother,nlp_stanza),[\"SBAR\",\"PP-of\"])\n",
    "          if brother_tree:\n",
    "              brother_tree = tree_to_string(brother_tree)\n",
    "              brother_tree_obj = build_tree_from_string(brother_tree)\n",
    "              extracted_brother = get_text_from_tree(brother_tree_obj)\n",
    "        if extracted_brother:\n",
    "            final_result = extracted_Np + \" of \" + extracted_brother\n",
    "        else:\n",
    "            final_result = extracted_Np \n",
    "    return final_result\n",
    "\n",
    "##------------------------------------------------------\n",
    "### verifie si une phrase donnée commance par \"ADP\" si oui on supprime ce dernier\n",
    "## exemple: in the brain and Electroencephalogram => the brain and Electroencephalogram\n",
    "def firstWord_is_ADP(sentence, nlp_spacy):\n",
    "   first_word = get_first_word(sentence)\n",
    "   doc = nlp_spacy(first_word)\n",
    "   if doc[0].pos_ == 'ADP':\n",
    "     mots = sentence.split()\n",
    "     return  ' '.join(mots[1:])\n",
    "   else:\n",
    "      return sentence\n",
    "\n",
    "\n",
    "# ### verify if the sentence contains NP or NN or NNP\n",
    "def is_contains_NP(sentence, nlp_stenza):\n",
    "  doc = nlp_stenza(sentence)\n",
    "  tree = doc.sentences[0].constituency\n",
    "  Counter = tree.get_constituent_counts(tree.children)\n",
    "  # print(Counter)\n",
    "  if 'NP' in Counter or 'NN' in tree.get_root_labels(tree.children) or 'NNP' in tree.get_root_labels(tree.children):\n",
    "      return True\n",
    "  else:\n",
    "      return False\n",
    "\n",
    "# def is_contains_NP(sentence, nlp_stenza):\n",
    "#   doc = nlp_stenza(sentence)\n",
    "#   tree = doc.sentences[0].constituency\n",
    "#   Counter = tree.get_constituent_counts(tree.children)\n",
    "#   return 'NP' or 'NNP' or \"NN\" in Counter\n",
    "\n",
    "\n",
    "def is_general(triple, general_words_path):\n",
    "    s,p,o = triple\n",
    "    ## \n",
    "    general_words = get_general_words(general_words_path)\n",
    "    if s in general_words or o in general_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def remove_adj_stopwords(sentence):\n",
    "    adj_stopwords = [\n",
    "        'able', 'available', 'brief', 'certain',\"this\",\n",
    "        'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "        'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "        'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "        'many', 'more', 'most', 'much', 'my', 'necessary',\n",
    "        'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "        'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "        'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure',\n",
    "        'a',\"an\", 'the', \"treeb\", \"rbek\", \"en\", \"that\", \"those\", 'some',\"these\",\n",
    "        \"often\", \"approximately\", \"and\", \"their\", \"we\", \"our\",\"us\", \"effective\", \"can\",\"may\",\n",
    "        \"should\",\"need\", \"either\", \"then\", 'good', 'accurately', 'corresponding',\"explicitly\", \"along\", \"major\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in adj_stopwords]\n",
    "    result_sentence = ' '.join(filtered_words)\n",
    "    return result_sentence\n",
    "\n",
    "def duplicated_triples(triples):\n",
    "    pass\n",
    "\n",
    "\n",
    "def not_empty_triple(triple):\n",
    "    if not triple:\n",
    "        return False\n",
    "    s,p,o = triple\n",
    "    return s and p and o\n",
    "\n",
    "\n",
    "def remove_abbreviations(sentence):\n",
    "    # Using a regular expression to remove anything within parentheses\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    # Also remove any prefix ending with an open parenthesis at the end of the string\n",
    "    sentence = re.sub(r'\\s*\\([^)]*$', '', sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "\n",
    "general_terms = [ 'method',\n",
    "                     'paper',\n",
    "                     'approach',\n",
    "                     'novel work',\n",
    "                     'two main step',\n",
    "                     'introduction',\n",
    "                     'paper introduction',\n",
    "                     'aim',\n",
    "                     'study',\n",
    "                     'result',\n",
    "                     'experiment',\n",
    "                     'thing',\n",
    "                     'two score',\n",
    "                 \n",
    "                \n",
    "                    \n",
    "                 \n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a66466-17e9-41f0-a21a-4cbab1377e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_NP(sentence, nlp_stanza, nlp_spacy, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"]):\n",
    "    doc = nlp_stanza(sentence)\n",
    "    constituency_tree = doc.sentences[0].constituency\n",
    "    final_result = None\n",
    "    if NP_method == \"first_most_DeepNP\":\n",
    "      extracted_Np = first_most_deepNP(constituency_tree)\n",
    "      tree_obj = build_tree_from_string(extracted_Np)\n",
    "      final_result = get_text_from_tree(tree_obj)\n",
    "    elif NP_method ==\"only_head\":\n",
    "      final_result = only_head_Noun(sentence, nlp_spacy)\n",
    "    elif NP_method ==\"NP+Head\":\n",
    "      #root_word = get_root_word(sentence, nlp_spacy)\n",
    "      root_word = get_root_word_stanford(sentence,nlp_stanza)\n",
    "      if  constituency_tree.get_root_labels(constituency_tree.children)[0] == \"NN\" and len(constituency_tree.leaf_labels()) == 1:\n",
    "          NP_head, brother_tree = constituency_tree, None\n",
    "          \n",
    "      elif constituency_tree.get_root_labels(constituency_tree.children)[0] == \"NNP\" and len(constituency_tree.leaf_labels()) == 1:\n",
    "          NP_head, brother_tree = constituency_tree, None\n",
    "      else:\n",
    "          NP_head, brother_tree = get_NP_Head(constituency_tree,root_word,option, None,nlp_stanza)\n",
    "        \n",
    "      if NP_head:\n",
    "        NP_head =  tree_to_string(NP_head)\n",
    "        \n",
    "        NP_tree_obj = build_tree_from_string(NP_head)\n",
    "        extracted_Np = get_text_from_tree(NP_tree_obj)\n",
    "        extracted_brother =\"\"\n",
    "        if brother_tree:\n",
    "          brother_tree = tree_to_string(brother_tree)\n",
    "          brother_tree_obj = build_tree_from_string(brother_tree)\n",
    "          extracted_brother = get_text_from_tree(brother_tree_obj)\n",
    "        if extracted_brother:\n",
    "            final_result = extracted_Np + \" \" + extracted_brother\n",
    "        else:\n",
    "            final_result = extracted_Np \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee7a6eb2-8ec1-4f9d-b882-d72d13418580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriplesPostProcessing:\n",
    "    def __init__(self,data_path, output_path):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.input_triples = self.get_triples() ##input triples before processing\n",
    "        self.cleaned_triples = [] ## output triples\n",
    "        self.nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "        self.nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency,depparse,lemma')\n",
    "    def get_triples(self):\n",
    "        data = read_json_file(input_data)\n",
    "        return data\n",
    "\n",
    "    def json_to_tuple(self, json_element):\n",
    "        return tuple([json_element['subject'],\n",
    "                     json_element['predicate'],\n",
    "                     json_element['object']])\n",
    "    \n",
    "    ## predicate rectification\n",
    "    def predicate_rectifier(self,t):\n",
    "        sujet, predicate, objet = t\n",
    "        #### to lower\n",
    "        sujet = sujet.lower()\n",
    "        predicate = predicate.lower()\n",
    "        objet = objet.lower()\n",
    "        # Liste des prépositions à vérifier\n",
    "        prepositions = ['by', 'at', 'in', 'to', 'for', 'of', 'on','with']\n",
    "    \n",
    "        # Vérifier si l'objet commence par l'une des prépositions\n",
    "        for preposition in prepositions:\n",
    "            if get_first_word(objet) == preposition:\n",
    "                # Ajouter l'objet à la fin du prédicat\n",
    "                predicate += ' ' + preposition\n",
    "                # Supprimer la préposition de l'objet\n",
    "                objet = objet[len(preposition):].strip()\n",
    "                #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        fw = get_first_word(objet)\n",
    "        if not fw:\n",
    "            return\n",
    "        doc = self.nlp_spacy(fw)\n",
    "        if  doc[0].pos_ =='VERB':\n",
    "            predicate += ' ' + fw\n",
    "            # Supprimer la préposition de l'objet\n",
    "            objet = objet[len(fw):].strip()\n",
    "            # break  # Sortir de la boucle après la première correspondance\n",
    "    \n",
    "        # Retourner le tuple modifié\n",
    "        return sujet, predicate, objet\n",
    "\n",
    "\n",
    "    def triple_cleaning(self, triple, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"] ):\n",
    "        if not_empty_triple(triple):\n",
    "          s = remove_adj_stopwords(triple[0])\n",
    "          o = remove_adj_stopwords(triple[2])\n",
    "          if not s or not o:\n",
    "              return\n",
    "          triple = (s,triple[1],o)\n",
    "          temp = self.predicate_rectifier(triple)\n",
    "          if temp:\n",
    "              s, p, o = temp\n",
    "              ## delete stop words using defined list\n",
    "              p = remove_adj_stopwords(p)\n",
    "              ## delete all adjectives and adverbs\n",
    "              p = delete_adj_adv(p,self.nlp_spacy)\n",
    "              p = lemmatize_predicate(p)\n",
    "              # p = delete_first_TO(p)\n",
    "              ## if is_passive(p):\n",
    "              ## inverse_triple()\n",
    "          else:\n",
    "              return\n",
    "          if not p:\n",
    "              return\n",
    "          if detect_negation(p):\n",
    "            return\n",
    "          s = remove_abbreviations(s)\n",
    "          o = remove_abbreviations(o)\n",
    "          s = firstWord_is_ADP(s, self.nlp_spacy)\n",
    "          if not s or not o:\n",
    "              return\n",
    "          if is_contains_NP(s, self.nlp_stanza ) and is_contains_NP(o, self.nlp_stanza) :\n",
    "            s = lemmatize_onlyNouns_and_lowercase_stanza(s,self.nlp_stanza )\n",
    "            o = lemmatize_onlyNouns_and_lowercase_stanza(o,self.nlp_stanza )\n",
    "            # s = lemmatize_onlyNouns_and_lowercase_wordNet(s)\n",
    "            # o = lemmatize_onlyNouns_and_lowercase_wordNet(o)\n",
    "              \n",
    "\n",
    "              \n",
    "            NP_subject = Extract_NP(s, self.nlp_stanza, self.nlp_spacy, NP_method , option )\n",
    "            NP_object = Extract_NP(o, self.nlp_stanza, self.nlp_spacy, NP_method , option )\n",
    "    \n",
    "            if NP_subject in general_terms or NP_object in general_terms:\n",
    "                return\n",
    "            if NP_subject and NP_object:\n",
    "                ## verify_max_len()\n",
    "                if is_word_count_less_than(NP_subject,6) and is_word_count_less_than(NP_subject,6): \n",
    "                    return NP_subject, p, NP_object\n",
    "                else:\n",
    "                    return\n",
    "            else:\n",
    "                return \n",
    "            # else:\n",
    "            #     return\n",
    "          else:\n",
    "            return\n",
    "        else:\n",
    "           return\n",
    "\n",
    "    def clean_triples(self):\n",
    "        for triple in tqdm(self.input_triples, desc=\"Cleaning triples\", unit=\"triple\"):\n",
    "            t = self.json_to_tuple(triple)\n",
    "            cleaned_t = self.triple_cleaning(t)\n",
    "            if cleaned_t:\n",
    "                self.cleaned_triples.append(\n",
    "                    {\n",
    "                        'sentence': triple['sentence'],\n",
    "                        'subject': cleaned_t[0],\n",
    "                        'predicate': cleaned_t[1],\n",
    "                        'object': cleaned_t[2],\n",
    "                        'confidence': triple['confidence']\n",
    "                        \n",
    "                    }\n",
    "                \n",
    "                )\n",
    "    def write_to_json(self):\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(self.cleaned_triples, jsonfile, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    def run(self):\n",
    "        self.clean_triples()\n",
    "        # print(\"number of triples before cleaning\": len(self.clean_triples))\n",
    "        self.write_to_json()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f901d0b-0f95-4d6a-aea5-19c822ba0a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 12:16:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 5.48MB/s]                    \n",
      "2024-02-15 12:16:36 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-02-15 12:16:37 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2024-02-15 12:16:37 INFO: Using device: cpu\n",
      "2024-02-15 12:16:37 INFO: Loading: tokenize\n",
      "2024-02-15 12:16:37 INFO: Loading: mwt\n",
      "2024-02-15 12:16:37 INFO: Loading: pos\n",
      "2024-02-15 12:16:37 INFO: Loading: lemma\n",
      "2024-02-15 12:16:37 INFO: Loading: constituency\n",
      "2024-02-15 12:16:38 INFO: Loading: depparse\n",
      "2024-02-15 12:16:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "input_data = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/informations_extraction/OIE_outputs/oie_Bench_triples.json\"\n",
    "\n",
    "output_data = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/src/post_processing/Bench_cleaned_triples_vf.json\"\n",
    "\n",
    "tpp = TriplesPostProcessing(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f064f3-4516-4e89-81d1-38e38991df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning triples: 100%|██████████| 395/395 [56:28<00:00,  8.58s/triple]  \n"
     ]
    }
   ],
   "source": [
    "tpp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e7f98ae-10fe-4f47-8c10-5b5fc1fb4e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpp.input_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ea5cc0-1dd3-434e-accb-a8748f6241bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'A common way to learn is by studying written step-by-step tutorials such as worked examples .',\n",
       "  'subject': 'common way',\n",
       "  'predicate': 'be by study',\n",
       "  'object': 'written step - by - step tutorial',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization .',\n",
       "  'subject': 'system',\n",
       "  'predicate': 'enables',\n",
       "  'object': 'people',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'We created a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization .',\n",
       "  'subject': 'people',\n",
       "  'predicate': 'to create',\n",
       "  'object': 'visual coding tutorial',\n",
       "  'confidence': 0.03},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'system',\n",
       "  'predicate': 'enables in',\n",
       "  'object': 'automatically - generated program',\n",
       "  'confidence': 0.76},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'people',\n",
       "  'predicate': 'to create',\n",
       "  'object': 'visual coding tutorial',\n",
       "  'confidence': 0.56},\n",
       " {'sentence': 'Using a system called Codepourri that enables people to easily create visual coding tutorials by annotating steps in an automatically-generated program visualization , We developed a novel crowdsourcing workflow where learners who are visiting an educational website ( www.pythontutor.com ) collectively create a tutorial by annotating execution steps in a piece of code and then voting on the best annotations .',\n",
       "  'subject': 'learner',\n",
       "  'predicate': 'create tutorial by vote on',\n",
       "  'object': 'best annotation',\n",
       "  'confidence': 0.01},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': '4 expert judge',\n",
       "  'predicate': 'add',\n",
       "  'object': '145 raw annotation',\n",
       "  'confidence': 0.96},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"code learner crowd 's annotation\",\n",
       "  'predicate': 'show',\n",
       "  'object': '4 expert judge',\n",
       "  'confidence': 0.25},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': '4 expert judge',\n",
       "  'predicate': 'add',\n",
       "  'object': '145 raw annotation',\n",
       "  'confidence': 0.89},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'experiment with show to be contain',\n",
       "  'object': '4 expert judge insight even expert',\n",
       "  'confidence': 0.09},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'to be contain',\n",
       "  'object': 'insight even expert',\n",
       "  'confidence': 0.44},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': '101 learner',\n",
       "  'predicate': 'add',\n",
       "  'object': '145 raw annotation',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': 'python',\n",
       "  'predicate': 'show',\n",
       "  'object': \"learner crowd 's annotation\",\n",
       "  'confidence': 0.85},\n",
       " {'sentence': \"We experiments with 4 expert judges and 101 learners adding 145 raw annotations to two pieces of textbook Python code show the learner crowd 's annotations to be accurate , informative , and containing some insights that even experts missed .\",\n",
       "  'subject': \"learner crowd 's annotation\",\n",
       "  'predicate': 'show to be contain',\n",
       "  'object': 'insight even expert',\n",
       "  'confidence': 0.84},\n",
       " {'sentence': 'As the conventional power systems turn towards smart grids ( SGs ) on a fast pace , turn may create new and significant challenges to the existing electrical network security .',\n",
       "  'subject': 'conventional power system',\n",
       "  'predicate': 'turn',\n",
       "  'object': 'smart grid',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'have emerge with',\n",
       "  'object': 'feature of smart grid',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'to be',\n",
       "  'object': 'critical issue',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'to be',\n",
       "  'object': 'critical issue',\n",
       "  'confidence': 0.97},\n",
       " {'sentence': 'Along with many important features of smart grids ( SGs cyber security has emerged to be a critical issue due to the interconnection of several loads , generators , and renewable resources through the communication network .',\n",
       "  'subject': 'sg cyber security',\n",
       "  'predicate': 'to be',\n",
       "  'object': 'critical issue',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Cyber-physical attacks ( CPAs ) are classified as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as large blackout and destruction of infrastructures .',\n",
       "  'subject': 'cyber - physical attack',\n",
       "  'predicate': 'lead to',\n",
       "  'object': 'severe consequence',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Cyber-physical attacks ( CPAs ) are classified as the major threatening of SGs security because Cyber-physical attacks ( CPAs ) may lead to severe consequences such as large blackout and destruction of infrastructures .',\n",
       "  'subject': 'cyber - physical attack',\n",
       "  'predicate': 'lead to',\n",
       "  'object': 'severe consequence',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.66},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'to attract',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.81},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start to attract',\n",
       "  'object': 'attention',\n",
       "  'confidence': 0.72},\n",
       " {'sentence': 'Cyber switching attacks ( CSAs ) ( as a part CPAs ) start to attract the attention due to Cyber switching attacks ( CSAs ) ( as a part CPAs ) severity and speed in destabilizing smart grids ( SGs , we present in this paper Thyristor-Controlled Braking Resistor ( TCBR ) as a solution to mitigate this type of attack .',\n",
       "  'subject': 'cyber switching attack',\n",
       "  'predicate': 'start mitigate',\n",
       "  'object': 'type of attack',\n",
       "  'confidence': 0.07},\n",
       " {'sentence': 'Thyristor-Controlled Braking Resistor ( TCBR ) can enable us to stabilize the target generator in a relatively short time .',\n",
       "  'subject': 'thyristor - controlled braking resistor',\n",
       "  'predicate': 'enable to stabilize',\n",
       "  'object': 'target generator',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Background and objective : According to the World Health Organization ( WHO ) epilepsy affects approximately 45-50 million people .',\n",
       "  'subject': 'epilepsy',\n",
       "  'predicate': 'affect',\n",
       "  'object': '45 - 50 million people',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Background and objective : According to the World Health Organization ( WHO ) epilepsy affects approximately 45-50 million people .',\n",
       "  'subject': 'epilepsy',\n",
       "  'predicate': 'affect',\n",
       "  'object': '45 - 50 million people world',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'electroencephalogram',\n",
       "  'predicate': 'record',\n",
       "  'object': 'neurological activity',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'electroencephalogram',\n",
       "  'predicate': 'be use to identify',\n",
       "  'object': 'epilepsy',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Electroencephalogram ( EEG ) records the neurological activity in the brain and Electroencephalogram ( EEG ) is used to identify epilepsy .',\n",
       "  'subject': 'electroencephalogram',\n",
       "  'predicate': 'to identify',\n",
       "  'object': 'epilepsy',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error .',\n",
       "  'subject': 'visual inspection of eeg signal',\n",
       "  'predicate': 'be',\n",
       "  'object': 'time - consuming process',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Visual inspection of EEG signals is a time-consuming process and Visual inspection of EEG signals may lead to human error .',\n",
       "  'subject': 'visual inspection of eeg signal',\n",
       "  'predicate': 'lead to',\n",
       "  'object': 'human error',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The classification between epileptic seizure and non -seizure signals is performed using different machine learning classifiers .',\n",
       "  'subject': 'classification',\n",
       "  'predicate': 'be perform use',\n",
       "  'object': 'machine learning',\n",
       "  'confidence': 0.9},\n",
       " {'sentence': 'The benchmark epilepsy EEG dataset provided by the University of Bonn is used in this study .',\n",
       "  'subject': 'benchmark epilepsy eeg dataset',\n",
       "  'predicate': 'provide by',\n",
       "  'object': 'university of bonn',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'The classification performance is evaluated using 10 -fold cross validation .',\n",
       "  'subject': 'classification performance',\n",
       "  'predicate': 'use',\n",
       "  'object': '10 - fold cross validation',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'be',\n",
       "  'object': 'nearest neighbor',\n",
       "  'confidence': 0.97},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'be',\n",
       "  'object': 'vector machine',\n",
       "  'confidence': 0.96},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'be',\n",
       "  'object': 'decision tree',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The classifiers used are the Nearest Neighbor ( NN ) , Support Vector Machine ( SVM ) , Decision Tree ( DT ) and Artificial Neural Network ( ANN ) .',\n",
       "  'subject': 'classifier',\n",
       "  'predicate': 'be',\n",
       "  'object': 'artificial neural network',\n",
       "  'confidence': 0.94},\n",
       " {'sentence': 'Results : Neighbor Descriptive Pattern [ LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82 % and 99.80 % , respectively , for the classification between normal and epileptic EEG signals .',\n",
       "  'subject': 'neighbor descriptive pattern [',\n",
       "  'predicate': 'achieve',\n",
       "  'object': 'average classification accuracy of 99.82 % 99.80 %',\n",
       "  'confidence': 0.6},\n",
       " {'sentence': 'Results : Neighbor Descriptive Pattern [ LNDP and 1D-LGPfeature extraction techniques with ANN classifier achieved the average classification accuracy of 99.82 % and 99.80 % , respectively , for the classification between normal and epileptic EEG signals .',\n",
       "  'subject': 'ann classifier',\n",
       "  'predicate': 'achieve',\n",
       "  'object': 'average classification accuracy of 99.82 % 99.80 %',\n",
       "  'confidence': 0.61},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware , software , and data sources .',\n",
       "  'subject': 'computer network',\n",
       "  'predicate': 'consist of',\n",
       "  'object': 'asset',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware , software , and data sources .',\n",
       "  'subject': 'computer network',\n",
       "  'predicate': 'consist of',\n",
       "  'object': 'asset',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Computer networks consist of several assets such as hardware , software , and data sources .',\n",
       "  'subject': 'computer network',\n",
       "  'predicate': 'consist of',\n",
       "  'object': 'asset',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'asset',\n",
       "  'predicate': 'have',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'vulnerability',\n",
       "  'predicate': 'be exploit by',\n",
       "  'object': 'attacker',\n",
       "  'confidence': 0.97},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'violate',\n",
       "  'object': 'security policy',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'asset',\n",
       "  'predicate': 'have',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'several assets such as hardware , software , and data sources have often some vulnerabilities which can be exploited by attackers that violate security policies in Computer networks .',\n",
       "  'subject': 'asset',\n",
       "  'predicate': 'have',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'analyze',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'vulnerability',\n",
       "  'predicate': 'be exploit by',\n",
       "  'object': 'attacker',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'violate',\n",
       "  'object': 'security policy',\n",
       "  'confidence': 0.7},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'to protect',\n",
       "  'object': 'network',\n",
       "  'confidence': 0.82},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'prioritize',\n",
       "  'object': 'vulnerability',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Considering the limited budget , the network administrator should analyze and prioritize some vulnerabilities which can be exploited by attackers that violate security policies in the network to be able to efficiently protect a network by mitigating the most risky ones .',\n",
       "  'subject': 'attacker',\n",
       "  'predicate': 'violate',\n",
       "  'object': 'security policy',\n",
       "  'confidence': 0.96},\n",
       " {'sentence': \"So far , several security parameters are offered to analyze security risks from the network security administrator 's perspective .\",\n",
       "  'subject': 'security parameter',\n",
       "  'predicate': 'be offer to analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 0.96},\n",
       " {'sentence': \"So far , several security parameters are offered to analyze security risks from the network security administrator 's perspective .\",\n",
       "  'subject': 'security parameter',\n",
       "  'predicate': 'to analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Depending on the motivation of potential attackers , different attack path may be selected for network security compromise .',\n",
       "  'subject': 'attack path',\n",
       "  'predicate': 'be select for',\n",
       "  'object': 'network security compromise',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': \"So , attacker 's motivation is a key factor in predicting the attacker 's behavior .\",\n",
       "  'subject': \"attacker 's motivation\",\n",
       "  'predicate': 'be',\n",
       "  'object': 'key factor',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': \"In this paper , the attacker 's motivation is considered in the process of security risk analysis , so network administrators are able to analyze security risks more accurately .\",\n",
       "  'subject': \"attacker 's motivation network administrator\",\n",
       "  'predicate': 'be in',\n",
       "  'object': 'process of security risk analysis',\n",
       "  'confidence': 0.5},\n",
       " {'sentence': \"In this paper , the attacker 's motivation is considered in the process of security risk analysis , so network administrators are able to analyze security risks more accurately .\",\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'be to analyze',\n",
       "  'object': 'security risk',\n",
       "  'confidence': 0.4},\n",
       " {'sentence': 'The proposed method is applied on a network and the results are compared with novel works in this area .',\n",
       "  'subject': 'proposed method',\n",
       "  'predicate': 'be apply on',\n",
       "  'object': 'network',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'to predict',\n",
       "  'object': 'behavior of attacker',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show',\n",
       "  'object': 'network administrator',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'to apply',\n",
       "  'object': 'countermeasure',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show',\n",
       "  'object': 'network administrator',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The experimental results show that network administrator will be able to precisely predict the behavior of attackers and apply countermeasures more efficiently .',\n",
       "  'subject': 'network administrator',\n",
       "  'predicate': 'will be to apply',\n",
       "  'object': 'countermeasure efficiently',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'data sharing',\n",
       "  'predicate': 'be',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'data sharing',\n",
       "  'predicate': 'be',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'information exchange',\n",
       "  'predicate': 'be',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients .',\n",
       "  'subject': 'information exchange',\n",
       "  'predicate': 'be',\n",
       "  'object': 'requirement',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Semi-structured storage technology is easier to deploy and much more promising to promote in a wider range than all-structured methods .',\n",
       "  'subject': 'semi - structured storage technology',\n",
       "  'predicate': 'to promote in',\n",
       "  'object': 'wider range',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggests',\n",
       "  'object': 'well current situation',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggests',\n",
       "  'object': 'combination of central data storage fit',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'This analysis suggests that semi-structural data storage technology and The combination of central and distributed data storage are efficient and fit well the current situation in China .',\n",
       "  'subject': 'analysis',\n",
       "  'predicate': 'suggests',\n",
       "  'object': 'combination of distributed data storage fit well current situation in china',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Data Structures and Algorithms are a central part of Computer Science .',\n",
       "  'subject': 'data structure',\n",
       "  'predicate': 'be',\n",
       "  'object': 'central part of computer science',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Data Structures and Algorithms are a central part of Computer Science .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'be',\n",
       "  'object': 'central part of computer science',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Due to Data Structures and Algorithms abstract and dynamic nature , Data Structures and Algorithms are a difficult topic to learn for many students .',\n",
       "  'subject': 'data structure',\n",
       "  'predicate': 'be',\n",
       "  'object': 'difficult topic to learn for student',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Due to Data Structures and Algorithms abstract and dynamic nature , Data Structures and Algorithms are a difficult topic to learn for many students .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'be',\n",
       "  'object': 'difficult topic to learn for student',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .',\n",
       "  'subject': 'instructor',\n",
       "  'predicate': 'have turn to',\n",
       "  'object': 'algorithm visualization',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .',\n",
       "  'subject': 'instructor',\n",
       "  'predicate': 'have turn to',\n",
       "  'object': 'av system',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Until recently , most AV systems were Java-based systems .',\n",
       "  'subject': 'av system',\n",
       "  'predicate': 'be',\n",
       "  'object': 'java - based system',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'But , the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online .',\n",
       "  'subject': 'popularity of java',\n",
       "  'predicate': 'be be supplant by',\n",
       "  'object': 'html5 content online',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'But , the popularity of Java has declined and is being supplanted by HTML5 and JavaScript content online .',\n",
       "  'subject': 'popularity of java',\n",
       "  'predicate': 'be be supplant by',\n",
       "  'object': 'javascript content online',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'javascript av development library',\n",
       "  'predicate': 'to build',\n",
       "  'object': 'content',\n",
       "  'confidence': 0.84},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'content',\n",
       "  'predicate': 'summarize',\n",
       "  'object': 'jsav',\n",
       "  'confidence': 0.67},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'content',\n",
       "  'predicate': 'support',\n",
       "  'object': 'three year of experience from using jsav',\n",
       "  'confidence': 0.29},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'javascript av development library',\n",
       "  'predicate': 'to build',\n",
       "  'object': 'content',\n",
       "  'confidence': 0.79},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'content',\n",
       "  'predicate': 'summarize',\n",
       "  'object': 'jsav',\n",
       "  'confidence': 0.77},\n",
       " {'sentence': 'we describe the growing body of content created with JSAV : the JavaScript AV development library and summarize we three years of experience and research results from using JSAV : the JavaScript AV development library to build content that supports CS education .',\n",
       "  'subject': 'javascript content',\n",
       "  'predicate': 'support',\n",
       "  'object': 'research result',\n",
       "  'confidence': 0.34},\n",
       " {'sentence': 'Formal language theory plays , in computer science , a fundamental role that allows , among other things , the development of one of the cornerstones of information technology : programming languages .',\n",
       "  'subject': 'formal language theory',\n",
       "  'predicate': 'play in',\n",
       "  'object': 'computer science',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines .',\n",
       "  'subject': 'formal language theory',\n",
       "  'predicate': 'define',\n",
       "  'object': 'mandatory grammatical rule programmer',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'Formal language theory define the mandatory grammatical rules that programmers need to follow to create the tools that enable humans to interact with machines .',\n",
       "  'subject': 'mandatory grammatical rule programmer',\n",
       "  'predicate': 'to follow to create',\n",
       "  'object': 'tool',\n",
       "  'confidence': 0.71},\n",
       " {'sentence': 'Despite Formal language theory significance , formal language theory is often taken for granted , even by software developers , who regularly follow the rules of their programming domain .',\n",
       "  'subject': 'software developer',\n",
       "  'predicate': 'follow',\n",
       "  'object': 'rule of programming domain',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'be create',\n",
       "  'object': 'developer programming language ,',\n",
       "  'confidence': 0.99},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'will to dive',\n",
       "  'object': 'formal language',\n",
       "  'confidence': 0.91},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'to dive',\n",
       "  'object': 'grammar',\n",
       "  'confidence': 0.94},\n",
       " {'sentence': 'Unless the developer is creating the developer own programming language , data structure , or describing the formal background of an existing one , the developer will not need to dive deep into formal languages , grammar or automaton theory .',\n",
       "  'subject': 'developer',\n",
       "  'predicate': 'be describe',\n",
       "  'object': 'formal background of existing one',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'all of formal background',\n",
       "  'predicate': 'will allow to transform',\n",
       "  'object': 'theory',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm stream of text',\n",
       "  'predicate': 'accept',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.22},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'accept',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.19},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'reject',\n",
       "  'object': 'stream of text comply',\n",
       "  'confidence': 0.22},\n",
       " {'sentence': 'All of this formal background will allow us to transform theory into bits , by developing an algorithm that will analyze streams of texts , accepting or rejecting streams of texts as streams of texts comply or not with the predefined rules .',\n",
       "  'subject': 'algorithm',\n",
       "  'predicate': 'reject',\n",
       "  'object': 'stream of text',\n",
       "  'confidence': 0.3},\n",
       " {'sentence': 'In the last two decades , we have seen an amazing development of image processing techniques targeted for medical applications .',\n",
       "  'subject': 'image processing technique',\n",
       "  'predicate': 'target for',\n",
       "  'object': 'medical application',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'While in automated bright lesion detection in retinal fundus images , as segmentation was not used , shape-based object detection was the compute-intensive task identified .',\n",
       "  'subject': 'shape - based object detection',\n",
       "  'predicate': 'be',\n",
       "  'object': 'compute - intensive task',\n",
       "  'confidence': 0.05},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'accelerated method',\n",
       "  'predicate': 'run on',\n",
       "  'object': 'multi - gpu system',\n",
       "  'confidence': 0.97},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show accelerate',\n",
       "  'object': 'average 265 time faster than original cpu version',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'experimental result',\n",
       "  'predicate': 'show to process accelerate',\n",
       "  'object': '344 frame',\n",
       "  'confidence': 0.7},\n",
       " {'sentence': 'Experimental results show that the accelerated method running on multi-GPU systems for blood detection in WCE images is on average 265 times faster than the original CPU version and is able to process 344 frames per second .',\n",
       "  'subject': 'accelerated method',\n",
       "  'predicate': 'be to',\n",
       "  'object': 'process 344 frame',\n",
       "  'confidence': 0.86},\n",
       " {'sentence': 'Despite advances in equipment as well as methods , automatic face image processing for recognition or even just for the extraction of demographics , is still a challenging task in unrestricted scenarios .',\n",
       "  'subject': 'method automatic face image processing',\n",
       "  'predicate': 'be',\n",
       "  'object': 'challenging task',\n",
       "  'confidence': 0.04},\n",
       " {'sentence': 'Despite advances in equipment as well as methods , automatic face image processing for recognition or even just for the extraction of demographics , is still a challenging task in unrestricted scenarios .',\n",
       "  'subject': 'automatic face image processing',\n",
       "  'predicate': 'be',\n",
       "  'object': 'challenging task',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones .',\n",
       "  'subject': 'test',\n",
       "  'predicate': 'be aim at carry',\n",
       "  'object': 'extensive comparison of feature based approach',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Our tests are aimed at carrying out an extensive comparison of a feature based approach with two score based ones .',\n",
       "  'subject': 'test',\n",
       "  'predicate': 'at carry out',\n",
       "  'object': 'extensive comparison of feature based approach with two score',\n",
       "  'confidence': 0.03},\n",
       " {'sentence': 'When using scores , different operators are applied in a completely separate way , so that each of different operators produces the corresponding scores .',\n",
       "  'subject': 'each of operator',\n",
       "  'predicate': 'produce',\n",
       "  'object': 'score',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Answers are then either fed to a SVM , or compared pairwise to exploit Likelihood Ratio .',\n",
       "  'subject': 'answer',\n",
       "  'predicate': 'fed to',\n",
       "  'object': 'svm',\n",
       "  'confidence': 0.92},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'testbeds',\n",
       "  'predicate': 'be',\n",
       "  'object': 'ega database',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'ega database',\n",
       "  'predicate': 'present',\n",
       "  'object': 'balance',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'The testbeds used for experiments are EGA database , which presents a good balance with respect to demographic features of stored face images , and GROPUS , an increasingly popular benchmark for massive experiments .',\n",
       "  'subject': 'testbeds',\n",
       "  'predicate': 'be',\n",
       "  'object': 'gropus',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'contain',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'contain',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'contain',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'contain',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contain a tremendous amount of structured data that can be leveraged to build and augment knowledge graphs .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'contain',\n",
       "  'object': 'tremendous amount of structured data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'database',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.79},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.85},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.85},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.89},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.92},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.89},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.71},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.66},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'json semantic model',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.77},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'to describe',\n",
       "  'object': 'information source',\n",
       "  'confidence': 0.63},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs rarely provide a semantic model to describe Information sources such as relational databases , spreadsheets , XML , JSON , and Web APIs contents .',\n",
       "  'subject': 'information source',\n",
       "  'predicate': 'provide',\n",
       "  'object': 'semantic model',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model of data source',\n",
       "  'predicate': 'represent',\n",
       "  'object': 'implicit meaning of data',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model of data source',\n",
       "  'predicate': 'by specify',\n",
       "  'object': 'concept',\n",
       "  'confidence': 0.62},\n",
       " {'sentence': 'Semantic models of data sources represent the implicit meaning of the data by specifying the concepts and the relationships within the data .',\n",
       "  'subject': 'semantic model data source',\n",
       "  'predicate': 'by specify',\n",
       "  'object': 'relationship',\n",
       "  'confidence': 0.56},\n",
       " {'sentence': 'Such models are the key ingredients to automatically publish the data into knowledge graphs .',\n",
       "  'subject': 'model',\n",
       "  'predicate': 'be',\n",
       "  'object': 'key ingredient to automatically publish data into knowledge graph',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Such models are the key ingredients to automatically publish the data into knowledge graphs .',\n",
       "  'subject': 'key ingredient',\n",
       "  'predicate': 'to publish',\n",
       "  'object': 'data',\n",
       "  'confidence': 0.84},\n",
       " {'sentence': 'Manually modeling the semantics of data sources requires significant effort and expertise , and although desirable , building Such models automatically is a challenging problem .',\n",
       "  'subject': 'desirable , building model',\n",
       "  'predicate': 'be challenge',\n",
       "  'object': 'problem',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Most of the related work focuses on semantic annotation of the data fields ( source attributes ) .',\n",
       "  'subject': 'related work',\n",
       "  'predicate': 'focus on',\n",
       "  'object': 'semantic annotation of data field',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'However , constructing a semantic model that explicitly describes the relationships between the attributes in addition to the attributes semantic types is critical .',\n",
       "  'subject': 'semantic model',\n",
       "  'predicate': 'describes',\n",
       "  'object': 'relationship',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'exploit',\n",
       "  'object': 'knowledge',\n",
       "  'confidence': 0.69},\n",
       " {'sentence': 'We present a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'exploit',\n",
       "  'object': 'semantic model of previously modeled source',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'rich semantic model',\n",
       "  'predicate': 'represent',\n",
       "  'object': 'semantics of source',\n",
       "  'confidence': 0.93},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'concept',\n",
       "  'predicate': 'define by',\n",
       "  'object': 'domain ontology',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'rich semantic model',\n",
       "  'predicate': 'represent',\n",
       "  'object': 'semantics of source',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'a rich semantic model for a new source represents the semantics of a new source in terms of the concepts and relationships defined by a domain ontology .',\n",
       "  'subject': 'relationship',\n",
       "  'predicate': 'define by',\n",
       "  'object': 'domain ontology',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Given some sample data from a new source , We leverage the knowledge in a domain ontology and the known semantic models to construct a weighted graph that represents the space of plausible semantic models for a new source .',\n",
       "  'subject': 'weighted graph',\n",
       "  'predicate': 'represent',\n",
       "  'object': 'space of plausible semantic model for source',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'exploit',\n",
       "  'object': 'knowledge',\n",
       "  'confidence': 0.7},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'knowledge from domain ontology to learn',\n",
       "  'object': 'rich semantic model',\n",
       "  'confidence': 0.09},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'exploit',\n",
       "  'object': 'semantic model of previously modeled source',\n",
       "  'confidence': 0.61},\n",
       " {'sentence': 'We evaluation shows that a novel approach that exploits the knowledge from a domain ontology and the semantic models of previously modeled sources to automatically learn a rich semantic model for a new source generates expressive semantic models for data sources and services with minimal user input .',\n",
       "  'subject': 'novel approach',\n",
       "  'predicate': 'model learn',\n",
       "  'object': 'previously source rich semantic model',\n",
       "  'confidence': 0.05},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services make it possible to automatically integrate the data across sources and provide rich support for source discovery and service composition .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'expressive semantic models for data sources and services also make it possible to automatically publish semantic data into knowledge graphs .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'expressive semantic models for data sources and services also make it possible to automatically publish semantic data into knowledge graphs .',\n",
       "  'subject': 'expressive semantic model',\n",
       "  'predicate': 'make',\n",
       "  'object': 'it',\n",
       "  'confidence': 0.98},\n",
       " {'sentence': 'Information technology has been contributing to various areas of knowledge ; in particular , the field of education stands out .',\n",
       "  'subject': 'information technology',\n",
       "  'predicate': 'have be contribute to',\n",
       "  'object': 'various area of knowledge',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'In what concerns the teaching of computer programming , literature contains important efforts that aim to assist in the learning process .',\n",
       "  'subject': 'effort',\n",
       "  'predicate': 'aim to assist',\n",
       "  'object': 'learning process',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'In what concerns the teaching of computer programming , literature contains important efforts that aim to assist in the learning process .',\n",
       "  'subject': 'effort',\n",
       "  'predicate': 'aim to assist in learn',\n",
       "  'object': 'process',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'have',\n",
       "  'object': 'difficulty',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'have in understand',\n",
       "  'object': 'problem logic',\n",
       "  'confidence': 0.65},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'have',\n",
       "  'object': 'difficulty',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'Teaching algorithms and programming concepts for first year students has always been a great challenge for universities , new Computer Science students usually have difficulties in understanding and abstracting the problem logics .',\n",
       "  'subject': 'computer science student',\n",
       "  'predicate': 'have in abstract',\n",
       "  'object': 'problem logic',\n",
       "  'confidence': 0.64},\n",
       " {'sentence': 'An alternative that has contributed to the teaching-learning process is the use of Learning Objects ( LO ) , which contribute towards mediating and enhancing the teaching-learning process .',\n",
       "  'subject': 'alternative',\n",
       "  'predicate': 'be use',\n",
       "  'object': 'learning object',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'An alternative that has contributed to the teaching-learning process is the use of Learning Objects ( LO ) , which contribute towards mediating and enhancing the teaching-learning process .',\n",
       "  'subject': 'alternative',\n",
       "  'predicate': 'be use',\n",
       "  'object': 'learning object',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': \"One of the great difficulties of learning during the initial semesters of Engineering and Computer Science courses is related to the contents of computer programming , which increases the students ' failure level and also the dropout rate of such courses .\",\n",
       "  'subject': 'computer programming',\n",
       "  'predicate': 'increase',\n",
       "  'object': \"student ' failure\",\n",
       "  'confidence': 1.0},\n",
       " {'sentence': \"One of the great difficulties of learning during the initial semesters of Engineering and Computer Science courses is related to the contents of computer programming , which increases the students ' failure level and also the dropout rate of such courses .\",\n",
       "  'subject': 'content of computer programming',\n",
       "  'predicate': 'increase',\n",
       "  'object': \"student ' failure\",\n",
       "  'confidence': 0.76},\n",
       " {'sentence': 'In order to decrease those rates , we have developed a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses , and the results were very positive .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'have develop learn object to help teach',\n",
       "  'object': 'concept',\n",
       "  'confidence': 0.03},\n",
       " {'sentence': 'a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process .',\n",
       "  'subject': 'outcome data',\n",
       "  'predicate': 'show use',\n",
       "  'object': 'learning object',\n",
       "  'confidence': 1.0},\n",
       " {'sentence': 'a project to create various learning objects to help teach concepts that are considered difficult to understand by students of Science courses was conducted in 2013 and 2014 and outcome data showed that the use of learning objects contributes significantly to the teaching-learning process .',\n",
       "  'subject': 'project',\n",
       "  'predicate': 'to create',\n",
       "  'object': 'various learning object to help',\n",
       "  'confidence': 0.74}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpp.cleaned_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb91db20-80c8-4542-a681-f02ce2c9d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpp.cleaned_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d37d504-402a-463a-bf00-e93153148045",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (\"shape-based object detection\",\"was\",\"the compute-intensive task identified While in automated bright lesion detection in retinal fundus images , as segmentation was not used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fc52e14f-4afb-4af9-bdc9-720d4e57990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tpp.triple_cleaning(t, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "29a8ac28-6ecd-438a-afc3-5b8586c603ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('shape - based object detection', 'be', 'compute - intensive task')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4253d4b8-e007-4bec-ac7f-bb6cb3f9f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriplesPostProcessing:\n",
    "    def __init__(self,data_path, output_path):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        # self.input_triples = self.get_triples() ##input triples before processing\n",
    "        self.cleaned_triples = [] ## output triples\n",
    "        self.nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "        self.nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency,depparse,lemma')\n",
    "    def get_triples(self):\n",
    "        data = read_json_file(input_data)\n",
    "        return data\n",
    "\n",
    "    def json_to_tuple(self, json_element):\n",
    "        return tuple([json_element['subject'],\n",
    "                     json_element['predicate'],\n",
    "                     json_element['object']])\n",
    "    \n",
    "    ## predicate rectification\n",
    "    def predicate_rectifier(self,t):\n",
    "        sujet, predicate, objet = t\n",
    "        #### to lower\n",
    "        sujet = sujet.lower()\n",
    "        predicate = predicate.lower()\n",
    "        objet = objet.lower()\n",
    "        # Liste des prépositions à vérifier\n",
    "        prepositions = ['by', 'at', 'in', 'to', 'for', 'of', 'on','with']\n",
    "    \n",
    "        # Vérifier si l'objet commence par l'une des prépositions\n",
    "        for preposition in prepositions:\n",
    "            if get_first_word(objet) == preposition:\n",
    "                # Ajouter l'objet à la fin du prédicat\n",
    "                predicate += ' ' + preposition\n",
    "                # Supprimer la préposition de l'objet\n",
    "                objet = objet[len(preposition):].strip()\n",
    "                #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        fw = get_first_word(objet)\n",
    "        if not fw:\n",
    "            return\n",
    "        doc = self.nlp_spacy(fw)\n",
    "        if  doc[0].pos_ =='VERB':\n",
    "            predicate += ' ' + fw\n",
    "            # Supprimer la préposition de l'objet\n",
    "            objet = objet[len(fw):].strip()\n",
    "            # break  # Sortir de la boucle après la première correspondance\n",
    "    \n",
    "        # Retourner le tuple modifié\n",
    "        return sujet, predicate, objet\n",
    "\n",
    "\n",
    "    def triple_cleaning(self, triple, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"] ):\n",
    "        if not_empty_triple(triple):\n",
    "          s = remove_adj_stopwords(triple[0])\n",
    "          o = remove_adj_stopwords(triple[2])\n",
    "          if not s or not o:\n",
    "              return\n",
    "          triple = (s,triple[1],o)\n",
    "          temp = self.predicate_rectifier(triple)\n",
    "          if temp:\n",
    "              s, p, o = temp\n",
    "              ## delete stop words using defined list\n",
    "              p = remove_adj_stopwords(p)\n",
    "              ## delete all adjectives and adverbs\n",
    "              p = delete_adj_adv(p,self.nlp_spacy)\n",
    "              print(p)\n",
    "              p = lemmatize_predicate(p)\n",
    "              print(p)\n",
    "              # p = delete_first_TO(p)\n",
    "              ## if is_passive(p):\n",
    "              ## inverse_triple()\n",
    "          else:\n",
    "              return\n",
    "          if not p:\n",
    "              return\n",
    "          if detect_negation(p):\n",
    "            return\n",
    "          s = remove_abbreviations(s)\n",
    "          o = remove_abbreviations(o)\n",
    "          s = firstWord_is_ADP(s, self.nlp_spacy)\n",
    "          if not s or not o:\n",
    "              return\n",
    "          if is_contains_NP(s, self.nlp_stanza ) and is_contains_NP(o, self.nlp_stanza) :\n",
    "            s = lemmatize_onlyNouns_and_lowercase_stanza(s,self.nlp_stanza )\n",
    "            o = lemmatize_onlyNouns_and_lowercase_stanza(o,self.nlp_stanza )\n",
    "            # s = lemmatize_onlyNouns_and_lowercase_wordNet(s)\n",
    "            # o = lemmatize_onlyNouns_and_lowercase_wordNet(o)\n",
    "              \n",
    "\n",
    "              \n",
    "            NP_subject = Extract_NP(s, self.nlp_stanza, self.nlp_spacy, NP_method , option )\n",
    "            NP_object = Extract_NP(o, self.nlp_stanza, self.nlp_spacy, NP_method , option )\n",
    "    \n",
    "            if NP_subject in general_terms or NP_object in general_terms:\n",
    "                return\n",
    "            if NP_subject and NP_object:\n",
    "                ## verify_max_len()\n",
    "                if is_word_count_less_than(NP_subject,6) and is_word_count_less_than(NP_subject,6): \n",
    "                    return NP_subject, p, NP_object\n",
    "                else:\n",
    "                    return\n",
    "            else:\n",
    "                return \n",
    "            # else:\n",
    "            #     return\n",
    "          else:\n",
    "            return\n",
    "        else:\n",
    "           return\n",
    "\n",
    "    def clean_triples(self):\n",
    "        for triple in tqdm(self.input_triples, desc=\"Cleaning triples\", unit=\"triple\"):\n",
    "            t = self.json_to_tuple(triple)\n",
    "            cleaned_t = self.triple_cleaning(t)\n",
    "            if cleaned_t:\n",
    "                self.cleaned_triples.append(\n",
    "                    {\n",
    "                        'sentence': triple['sentence'],\n",
    "                        'subject': cleaned_t[0],\n",
    "                        'predicate': cleaned_t[1],\n",
    "                        'object': cleaned_t[2],\n",
    "                        'confidence': triple['confidence']\n",
    "                        \n",
    "                    }\n",
    "                \n",
    "                )\n",
    "    def write_to_json(self):\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(self.cleaned_triples, jsonfile, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    def run(self):\n",
    "        self.clean_triples()\n",
    "        # print(\"number of triples before cleaning\": len(self.clean_triples))\n",
    "        self.write_to_json()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "370404e7-2052-4b49-a2e9-05da9cf4927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = {\n",
    "    \"sentence\": \"Answers are then either fed to a SVM , or compared pairwise to exploit Likelihood Ratio .\",\n",
    "    \"subject\": \"Answers\",\n",
    "    \"predicate\": \"fed\",\n",
    "    \"object\": \"to a SVM\",\n",
    "    \"confidence\": 0.95\n",
    "  }\n",
    "e_2 = {\n",
    "    \"sentence\": \"Data Structures and Algorithms are a central part of Computer Science .\",\n",
    "    \"subject\": \"Data Structures\",\n",
    "    \"predicate\": \"are\",\n",
    "    \"object\": \"a central part of Computer Science\",\n",
    "    \"confidence\": 1.0\n",
    "  }\n",
    "e_3 = {\n",
    "     \"sentence\": \"To alleviate these learning difficulties , instructors have turned to algorithm visualizations ( AV ) and AV systems .\",\n",
    "    \"subject\": \"instructors\",\n",
    "    \"predicate\": \"have turned\",\n",
    "    \"object\": \"to AV systems\",\n",
    "}\n",
    "s = e_3[\"subject\"]\n",
    "p = e_3[\"predicate\"]\n",
    "o = e_3[\"object\"]\n",
    "triple = (s,p,o)\n",
    "# triple = (\"different operators\",\"are applied\",\"When using scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28887a80-f03b-4ef4-b3d6-cfc9cae2c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 11:57:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 5.15MB/s]                    \n",
      "2024-02-15 11:57:50 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-02-15 11:57:51 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "======================================\n",
      "\n",
      "2024-02-15 11:57:51 INFO: Using device: cpu\n",
      "2024-02-15 11:57:51 INFO: Loading: tokenize\n",
      "2024-02-15 11:57:51 INFO: Loading: mwt\n",
      "2024-02-15 11:57:51 INFO: Loading: pos\n",
      "2024-02-15 11:57:51 INFO: Loading: lemma\n",
      "2024-02-15 11:57:52 INFO: Loading: constituency\n",
      "2024-02-15 11:57:52 INFO: Loading: depparse\n",
      "2024-02-15 11:57:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tp = TriplesPostProcessing(\"i\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eadfe65-58ca-408a-bc13-c8331f5aff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have turned to\n",
      "have turn to\n"
     ]
    }
   ],
   "source": [
    "pr= tp.triple_cleaning(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df3b3366-bb5e-4675-973a-2c31cc51a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('instructor', 'have turn to', 'av system')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "422a3e51-6fa2-40dc-aefe-a10cb4196f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_NP(sentence, nlp_stanza, nlp_spacy, NP_method = \"NP+Head\", option = [\"SBAR\",\"PP-of\", \"VP\",\"S\"]):\n",
    "    doc = nlp_stanza(sentence)\n",
    "    constituency_tree = doc.sentences[0].constituency\n",
    "    final_result = None\n",
    "    if NP_method == \"first_most_DeepNP\":\n",
    "      extracted_Np = first_most_deepNP(constituency_tree)\n",
    "      tree_obj = build_tree_from_string(extracted_Np)\n",
    "      final_result = get_text_from_tree(tree_obj)\n",
    "    elif NP_method ==\"only_head\":\n",
    "      final_result = only_head_Noun(sentence, nlp_spacy)\n",
    "    elif NP_method ==\"NP+Head\":\n",
    "      #root_word = get_root_word(sentence, nlp_spacy)\n",
    "      root_word = get_root_word_stanford(sentence,nlp_stanza)\n",
    "      if  constituency_tree.get_root_labels(constituency_tree.children)[0] == \"NN\" and len(constituency_tree.leaf_labels()) == 1:\n",
    "          NP_head, brother_tree = constituency_tree, None\n",
    "          \n",
    "      elif constituency_tree.get_root_labels(constituency_tree.children)[0] == \"NNP\" and len(constituency_tree.leaf_labels()) == 1:\n",
    "          NP_head, brother_tree = constituency_tree, None\n",
    "      else:\n",
    "          NP_head, brother_tree = get_NP_Head(constituency_tree,root_word,option,None, nlp_stanza)\n",
    "        \n",
    "      if NP_head:\n",
    "        NP_head =  tree_to_string(NP_head)\n",
    "        \n",
    "        NP_tree_obj = build_tree_from_string(NP_head)\n",
    "        extracted_Np = get_text_from_tree(NP_tree_obj)\n",
    "        extracted_brother =\"\"\n",
    "        if brother_tree:\n",
    "          brother_tree = tree_to_string(brother_tree)\n",
    "          brother_tree_obj = build_tree_from_string(brother_tree)\n",
    "          extracted_brother = get_text_from_tree(brother_tree_obj)\n",
    "        if extracted_brother:\n",
    "            final_result = extracted_Np + \" \" + extracted_brother\n",
    "        else:\n",
    "            final_result = extracted_Np \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f519fd5-0ad7-49e2-bfc8-b6ce3360d4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computer network'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Extract_NP(\"Computer network\",tp.nlp_stanza,tp.nlp_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ea5592d8-9c36-4748-b759-b1f0ddc57bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (NN answer))\n",
      "answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    def get_NP_Head(node, word, options = [\"SBAR\",\"PP-of\", \"VP\",\"S\"],brother = None, nlp_stanza = None):\n",
    "    current_np = None\n",
    "    current_brother = None\n",
    "    # if not nlp_stanza:\n",
    "    #     nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency,depparse,lemma')\n",
    "    if node is not None:\n",
    "        # if node.label == \"NN\" and len(node.leaf_labels()) == 1 and word in node.leaf_labels():\n",
    "        #   return node, None\n",
    "        # if node.label == \"NNP\" and word in node.leaf_labels():\n",
    "          # return node, None\n",
    "        if is_verb_with_stanza(word, nlp_stanza):\n",
    "              nps = extract_all_nps(node)\n",
    "              NP = None\n",
    "              for np in nps:\n",
    "                  if word not in np.leaf_labels() and node.leaf_labels().index(word) > node.leaf_labels().index(np.leaf_labels()[-1]):\n",
    "                      NP = np\n",
    "              return NP,None\n",
    "        if node.label == \"NP\" and word in node.leaf_labels():\n",
    "            if is_verb_with_stanza(word, nlp_stanza):\n",
    "              nps = extract_all_nps(node)\n",
    "              NP = None\n",
    "              for np in nps:\n",
    "                  if word not in np.leaf_labels() and node.leaf_labels().index(word) > node.leaf_labels().index(np.leaf_labels()[-1]):\n",
    "                      NP = np\n",
    "              return NP,None\n",
    "            # current_np = tree_to_string(node)\n",
    "            current_np = node\n",
    "            if brother:\n",
    "              current_brother =  brother\n",
    "              # current_brother =  tree_to_string(brother)\n",
    "              for option in options:\n",
    "                if option ==\"PP-of\" and brother.label == \"PP\":\n",
    "                  if brother.leaf_labels()[0] != \"of\":\n",
    "                    current_brother = None\n",
    "                else:\n",
    "                  if brother.label == option:\n",
    "                    current_brother = None\n",
    "\n",
    "        for num_cild,child in enumerate(node.children):\n",
    "            if num_cild >= len(node.children)-1:\n",
    "                brother = None\n",
    "            else:\n",
    "                brother = node.children[num_cild+1]\n",
    "\n",
    "            result,bro = get_NP_Head(child,word,options,brother, nlp_stanza)\n",
    "            if result is not None:\n",
    "                current_np = result\n",
    "                current_brother = bro\n",
    "        return current_np, current_brother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cc0c3416-124b-4460-9005-ea3ddb1d647d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a536c5db-4610-41f9-aa19-706fe27ab7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
