{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9d86c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "## read data\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents as a Python dictionary.\n",
    "\n",
    "    :param file_path: The path to the JSON file.\n",
    "    :type file_path: str\n",
    "    :return: A dictionary representing the JSON data.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "\n",
    "class OpenIEDataPreparer:\n",
    "    ## data path: after coreferences resolution (json file)\n",
    "    def __init__(self, data_path, output_path, corpus):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.data = {}\n",
    "        self.result = []\n",
    "        self.sentences = []\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        ## paragraph -> sentences\n",
    "    def split_paragraph(self,paragraphe):\n",
    "        sentences = nltk.sent_tokenize(paragraphe)\n",
    "        return sentences\n",
    "        \n",
    "    \n",
    "    ## computer sc \n",
    "    def prepare_setences(self):\n",
    "        data = read_json_file(self.data_path)\n",
    "        if self.corpus == \"Computer_science\":\n",
    "            abstracts = [paragraph[\"abstract\"] for paragraph in data.values()]\n",
    "    #         with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            for abstract in abstracts:\n",
    "                sentences = self.split_paragraph(abstract)\n",
    "                for sentence in tqdm(sentences, desc=\"Processing Sentences\", unit=\"sentence\", leave=False):\n",
    "    #                     output_file.write(sentence.strip() +'\\n')\n",
    "                    self.sentences.append(sentence.strip())\n",
    "        if self.corpus == \"Music\":\n",
    "            paragraphs = [paragraph[\"paragraph\"] for paragraph in data.values()]\n",
    "            for paragraph in paragraphs:\n",
    "                sentences = self.split_paragraph(paragraph)\n",
    "                for sentence in tqdm(sentences, desc=\"Processing Sentences\", unit=\"sentence\", leave=False):\n",
    "    #               output_file.write(sentence.strip() +'\\n')\n",
    "                    self.sentences.append(sentence.strip())\n",
    "\n",
    "        else:\n",
    "            print(\"invalid corpus name !\")\n",
    "            return\n",
    "\n",
    "    def is_valid_sentence(self, sentence):\n",
    "        return not sentence.startswith(('(C)', '(c)')) \\\n",
    "               and len(sentence) >= 20 \\\n",
    "               and not sentence.startswith(':') \\\n",
    "               and any(c.isalpha() for c in sentence) \\\n",
    "               and not sentence.startswith('Copyright')\n",
    "    \n",
    "    def clean_sentences(self):\n",
    "    # Apply the filter and strip each sentence\n",
    "        filtered_sentences = [sentence.strip() for sentence in self.sentences if self.is_valid_sentence(sentence)]\n",
    "        self.result = filtered_sentences\n",
    "#         Open the output file in write mode and write the filtered lines\n",
    "        with open(self.output_path, \"w\",encoding='utf-8' ) as file:\n",
    "            file.write('\\n'.join(filtered_sentences))\n",
    "    \n",
    "    def run(self):\n",
    "        self.prepare_setences()\n",
    "        self.clean_sentences()\n",
    "        \n",
    "\n",
    "\n",
    "# ## split sentences\n",
    "# def separer_en_phrases(paragraphe):\n",
    "#     phrases = nltk.sent_tokenize(paragraphe)\n",
    "#     return phrases\n",
    "\n",
    "# def prepare_setences(corpus_path, output_path):\n",
    "#     data = read_json_file(corpus_path)\n",
    "#     abstracts = [paragraph[\"abstract\"] for paragraph in data.values()]\n",
    "#     with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "#         for abstract in abstracts:\n",
    "#             sentences = nltk.sent_tokenize(abstract)\n",
    "#             for sentence in sentences:\n",
    "#                 output_file.write(sentence.strip() +'\\n')\n",
    "\n",
    "# def is_valid_line(line):\n",
    "#     return not line.startswith(('(C)', '(c)')) \\\n",
    "#            and len(line) >= 20 \\\n",
    "#            and not line.startswith(':') \\\n",
    "#            and any(c.isalpha() for c in line) \\\n",
    "#            and not line.startswith('Copyright')\n",
    "\n",
    "\n",
    "# def clean_text_file(input_file, output_file):\n",
    "#     # Open the input file in read mode\n",
    "#     with open(input_file, \"r\") as file:\n",
    "#         # Read all lines from the file\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     # Apply the filter and strip each line\n",
    "#     filtered_lines = [line.strip() for line in lines if is_valid_line(line)]\n",
    "#     # Open the output file in write mode and write the filtered lines\n",
    "#     with open(output_file, \"w\") as file:\n",
    "#         file.write('\\n'.join(filtered_lines))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35bb9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Computer Science\n",
    "# input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/ziwei_cs_data/CS_bench_coref.json\"\n",
    "# output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/ziwei_cs_data/CS_bench_oie.txt\"\n",
    "# corpus = \"Computer_science\"\n",
    "### Music\n",
    "corpus = \"Music\"\n",
    "input_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/Music_Bench/Bench_music_coref.json\"\n",
    "output_path = \"C:/Users/admin-user/Desktop/my_phd/implementations_KG/data/Music_Bench/Bench_music_oie.txt\"\n",
    "\n",
    "odp = OpenIEDataPreparer(input_path, output_path, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d76be57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "odp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45422781-2956-4cf2-afe9-63faf4889984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1919â€“20), people mainly listened to music at live Classical music concerts or musical theatre shows, which were too expensive for many lower-income people; on early phonograph players (a technology invented in 1877 which was not mass-marketed until the mid-1890s); or by individuals performing music or singing songs on an amateur basis at home, using sheet music, which required the ability to sing, play, and read music.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odp.sentences[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
